{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc61b84",
   "metadata": {},
   "source": [
    "# CS421 Project\n",
    "---\n",
    "\n",
    "Group: Empirical Risk Minimisers  \n",
    "Members:\n",
    "- Lai Wan Xuan Joanne (joanne.lai.2021)\n",
    "- Ryan Miguel Moralde Sia (ryansia.2022)\n",
    "- Dhruv Benegal (benegalda.2022)\n",
    "- Benedict Lee Zi Le (benedictlee.2022)\n",
    "\n",
    "In this notebook, we train our models on the four batches of data provided, and produce predictions on the fifth and final batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ade660",
   "metadata": {},
   "source": [
    "### 1. Background & Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce15d29",
   "metadata": {},
   "source": [
    "In this project, you will be working with data extracted from famous recommender systems type datasets: you are provided with a large set of interactions between users (persons) and items (movies). Whenever a user \"interacts\" with an item, it watches the movie and gives a \"rating\". There are 5 possible ratings expressed as a \"number of stars\": 1,2,3,4, or 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fab5e",
   "metadata": {},
   "source": [
    "In this exercise, we will **not** be performing the recommendation task per se. Instead, you will try to identify the amount of noise/corruption which was injected in each user. Indeed, for each of the users you have been given, an anomaly/noise generation procedure was applied to corrupt the sample. The noise generation procedure depends on two variables: the noise level $p\\in [0,1]$ and the noise type $X\\in\\{0,1,2\\}$.  Each user has been randomly assigned a noise level $p$ and anomaly/noise type $X$, and subsequently been corrupted with the associated noise generation procedure. \n",
    "\n",
    "You have two tasks: first, you must predict the noise level $p$ associated to each test user. This is a **supervised regression task**. Second, you must try to identify the noise generation type for each user. This is a classification task with three classes, with the possibility of including more classes later depending on class performance. This task will be semi-supervised: only a very small number of labels is provided. You will therefore need to combine supervised and unsupervised approaches for this component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c248f",
   "metadata": {},
   "source": [
    "### 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df689c",
   "metadata": {},
   "source": [
    "You are provided with three frames: the first one (\"X\") contains the interactions provided to you, and the second one (\"yy\") contains the continuous for the users. The third data frame \"yy_cat\" contains the anomaly/noise type for 15 users. The idea is to use these users to disambiguate the category types, but the task will mostly be unsupervised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc60513",
   "metadata": {},
   "source": [
    "As you can see, the three columns in \"X\" correspond to the user ID, the item ID and the rating (encoded into numerical form). Thus, each row of \"X\" contains a single interaction. For instance, if the row \"$142, 152, 5$\" is present, this means that the user with ID $142$ has given the movie $152$ a positive rating of $5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc9801",
   "metadata": {},
   "source": [
    "The dataframe \"yy\" has two columns. In the first column we have the user IDs, whilst the second column contains the continuous label. A label of $0.01$ indicates a very low anomaly level, whilst a label of $0.99$ indicates a very high amount of noise/corruption. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ef3e7",
   "metadata": {},
   "source": [
    "### 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ae8f7",
   "metadata": {},
   "source": [
    "Your task is to be able to regress the noise level $p$ for each new user, and predict the anomaly type $X$. The first (regression) task will be easier due to the larger amount of supervision, and will form the main basis of the evaluation. The second task will be more importance to showcase each team's creativity and differentiate between top performers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e059a16",
   "metadata": {},
   "source": [
    "THE **EVALUATION METRICs** are:  \n",
    "\n",
    "1. The Mean Absolute Error (MAE) for the regression task. \n",
    "2. The accuracy for the classiciation task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42d14f",
   "metadata": {},
   "source": [
    "Every few weeks, we will evaluate the performance of each team (on a *test set with unseen labels* that I will provide) in terms of both metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35f65c",
   "metadata": {},
   "source": [
    "The difficulty implied by **the generation procedure of the anomalies MAY CHANGE as the project evolves: depending on how well the teams are doing, I may generate easier or harder anomaly classes, which would change the number of labels in the classification task**. However, the regression task will still be the same (with a different distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065547d",
   "metadata": {},
   "source": [
    "### 4. Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac837a",
   "metadata": {},
   "source": [
    "Together with this file, you are provided with a first batch of examples \"`first_batch_regression_labelled.npz`\" which are labelled in terms of noise level. You are also provided with the test samples to rank by the next round (without labels) in the file \"`second_batch_regression_unlabelled.npz`\".\n",
    "\n",
    "The **first round** will take place after recess (week 9): you must hand in your scores for the second batch before the **Wednesday at NOON (15th of October)**. We will then look at the results together on the Friday.  \n",
    "\n",
    "We will check everyone's performance in this way every week (once on  week 10, once on week 11 and once on week 12). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527121a4",
   "metadata": {},
   "source": [
    "To summarise, the project deliverables are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab22a",
   "metadata": {},
   "source": [
    "- Before every checkpoint's deadline, you need to submit **a `.csv` file** containing a dataframe of size $\\text{number of test batch users} \\times 3$.\n",
    "    - The first column should be the user IDs of the test batch.\n",
    "    - The second column should contain the estimated noise level $p$ for each sample.\n",
    "    - The final column should contain the estimated class (it should be a natural number in \\{0,1,2\\}).\n",
    "- The order of rows should correspond to the user IDs. For example, if the test batch contains users 1100-2200, scores for user 1100 should be the first row (row 0), scores for user 1101 should be the second row (row 1), and so on.\n",
    "- On Week 12-13 (schedule to be decided), you need to present your work in class. The presentation duration is **10 minutes** with 5 minutes of QA.\n",
    "- On Week 12, you need to submit your **Jupyter Notebook** (with comments in Markdown) and the **slides** for your presentation. \n",
    "- On week 13 you need to submit your **final report**. The final report should be 2-3 pages long (consisting of problem statement, literature review, and motivation of algorithm design) with unlimited references/appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3015c",
   "metadata": {},
   "source": [
    "Whilst performance (expressed in terms of MAE and accuracy) at **each of the check points** (weeks 9 to 12 inclusive) is an **important component** of your **final grade**, the **final report** and the detail of the various methods you will have tried will **also** be very **important**. Ideally, to get perfect marks (A+), you should try at least **two supervised methods** and **two unsupervised methods**, as well as be ranked the **best team** in terms of performance. \n",
    "\n",
    "\n",
    "In addition, I will be especially interested in your **reasoning**. Especially high marks will be awarded to any team that is able to **qualitatively describe** the difference between the two anomaly types. You are also encouraged to compute statistics related to each class and describe what is different about them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab69c97",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b6c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import (\n",
    "    kurtosis,\n",
    "    skew,\n",
    "    entropy\n",
    ")\n",
    "from sklearn.decomposition import (\n",
    "    PCA,\n",
    "    TruncatedSVD\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    SGDRegressor\n",
    "    )\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeRegressor\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    BaggingRegressor\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5b0da-1a70-409b-8117-4e831ff7da13",
   "metadata": {},
   "source": [
    "## Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285ff9b",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "first_batch  = np.load(\"data/Week1/first_batch_regression_labelled.npz\")\n",
    "second_batch  = np.load(\"data/Week2/second_batch_regression_labelled.npz\")\n",
    "third_batch  = np.load(\"data/Week3/third_batch_regression_labelled.npz\")\n",
    "fourth_batch  = np.load(\"data/Week4/fourth_batch_regression_labelled.npz\")\n",
    "\n",
    "# Extract X, y, y_cat for each batch\n",
    "X1, y1, y1_cat = first_batch[\"X\"], first_batch[\"yy\"], first_batch[\"yy_cat\"]\n",
    "X2, y2, y2_cat = second_batch[\"X\"], second_batch[\"yy\"], second_batch[\"yy_cat\"]\n",
    "X3, y3, y3_cat = third_batch[\"X\"], third_batch[\"yy\"], third_batch[\"yy_cat\"]\n",
    "X4, y4, y4_cat = fourth_batch[\"X\"], fourth_batch[\"yy\"], fourth_batch[\"yy_cat\"]\n",
    "\n",
    "# Convert to DataFrames and rename columns for each batch\n",
    "X1     = pd.DataFrame(X1, columns=[\"user\", \"item\", \"rating\"])\n",
    "y1     = pd.DataFrame(y1, columns=[\"user\", \"label\"])\n",
    "y1_cat = pd.DataFrame(y1_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "X2     = pd.DataFrame(X2, columns=[\"user\", \"item\", \"rating\"])\n",
    "y2     = pd.DataFrame(y2, columns=[\"user\", \"label\"])\n",
    "y2_cat = pd.DataFrame(y2_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "X3     = pd.DataFrame(X3, columns=[\"user\", \"item\", \"rating\"])\n",
    "y3     = pd.DataFrame(y3, columns=[\"user\", \"label\"])\n",
    "y3_cat = pd.DataFrame(y3_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "X4     = pd.DataFrame(X4, columns=[\"user\", \"item\", \"rating\"])\n",
    "y4     = pd.DataFrame(y4, columns=[\"user\", \"label\"])\n",
    "y4_cat = pd.DataFrame(y4_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "\n",
    "# Combine the data \n",
    "X = pd.concat([X1, X2, X3, X4], ignore_index=True)\n",
    "y = pd.concat([y1, y2, y3, y4], ignore_index=True)\n",
    "y_cat = pd.concat([y1_cat, y2_cat, y3_cat, y4_cat], ignore_index=True)\n",
    "\n",
    "# Parse to correct types\n",
    "y     = y.astype({\"user\": int, \"label\": float})\n",
    "y_cat = y_cat.astype({\"user\": int, \"label\": float, \"anomtype\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf10da70-7e1f-416c-9ce2-f7cf4c78d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX    = np.load(\"data/Week4/fifth_batch_regression_unlabelled.npz\")['X']\n",
    "XX    = pd.DataFrame(XX, columns=[\"user\", \"item\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b3347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144737</th>\n",
       "      <td>3599</td>\n",
       "      <td>396</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144738</th>\n",
       "      <td>3599</td>\n",
       "      <td>183</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144739</th>\n",
       "      <td>3599</td>\n",
       "      <td>877</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144740</th>\n",
       "      <td>3599</td>\n",
       "      <td>961</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144741</th>\n",
       "      <td>3599</td>\n",
       "      <td>416</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1144742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user  item  rating\n",
       "0           0    94       2\n",
       "1           0    90       1\n",
       "2           0    97       2\n",
       "3           0   100       4\n",
       "4           0   101       2\n",
       "...       ...   ...     ...\n",
       "1144737  3599   396       4\n",
       "1144738  3599   183       3\n",
       "1144739  3599   877       3\n",
       "1144740  3599   961       5\n",
       "1144741  3599   416       4\n",
       "\n",
       "[1144742 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X contains interactions provided\n",
    "# has 1144742 rows\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b805f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---To check if number of users in X corresponds to number of rows in y---\n",
      "Number of unique users in X: 3600\n",
      "Number of rows in y: 3600\n"
     ]
    }
   ],
   "source": [
    "# y contains the noise level p\n",
    "# Has 3600 rows (900 users x 4 weeks) corresponding to users\n",
    "\n",
    "y\n",
    "\n",
    "print(\"---To check if number of users in X corresponds to number of rows in y---\")\n",
    "print(f\"Number of unique users in X: {X['user'].nunique()}\")\n",
    "print(f\"Number of rows in y: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8bed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>561</td>\n",
       "      <td>0.383316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>0.925028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>205</td>\n",
       "      <td>0.380860</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424</td>\n",
       "      <td>0.255181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>284</td>\n",
       "      <td>0.055162</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>667</td>\n",
       "      <td>0.558745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>730</td>\n",
       "      <td>0.311928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>469</td>\n",
       "      <td>0.233492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>199</td>\n",
       "      <td>0.165112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>699</td>\n",
       "      <td>0.261752</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>231</td>\n",
       "      <td>0.951103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>0.558222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>786</td>\n",
       "      <td>0.549116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>849</td>\n",
       "      <td>0.301816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>459</td>\n",
       "      <td>0.739300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1057</td>\n",
       "      <td>0.057549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1151</td>\n",
       "      <td>0.625202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1640</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1561</td>\n",
       "      <td>0.245883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1066</td>\n",
       "      <td>0.339369</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1455</td>\n",
       "      <td>0.979873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1450</td>\n",
       "      <td>0.426661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1722</td>\n",
       "      <td>0.150973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1045</td>\n",
       "      <td>0.638008</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1289</td>\n",
       "      <td>0.529647</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1279</td>\n",
       "      <td>0.099430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1617</td>\n",
       "      <td>0.903824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1563</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1682</td>\n",
       "      <td>0.587148</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1307</td>\n",
       "      <td>0.507218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2163</td>\n",
       "      <td>0.269215</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2120</td>\n",
       "      <td>0.671698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1865</td>\n",
       "      <td>0.873609</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2648</td>\n",
       "      <td>0.074802</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2548</td>\n",
       "      <td>0.698647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2458</td>\n",
       "      <td>0.441319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2093</td>\n",
       "      <td>0.805789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2328</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1903</td>\n",
       "      <td>0.520802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2595</td>\n",
       "      <td>0.440071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1818</td>\n",
       "      <td>0.580426</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2543</td>\n",
       "      <td>0.779798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1822</td>\n",
       "      <td>0.746696</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2225</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2390</td>\n",
       "      <td>0.567205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3499</td>\n",
       "      <td>0.590148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2782</td>\n",
       "      <td>0.976002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3153</td>\n",
       "      <td>0.449470</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3123</td>\n",
       "      <td>0.677369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2800</td>\n",
       "      <td>0.998705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3401</td>\n",
       "      <td>0.779507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3292</td>\n",
       "      <td>0.790033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2807</td>\n",
       "      <td>0.143894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2778</td>\n",
       "      <td>0.210702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2938</td>\n",
       "      <td>0.802037</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2980</td>\n",
       "      <td>0.779076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3576</td>\n",
       "      <td>0.403759</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3465</td>\n",
       "      <td>0.954013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2767</td>\n",
       "      <td>0.491294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3082</td>\n",
       "      <td>0.972140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user     label  anomtype\n",
       "0    561  0.383316         1\n",
       "1    202  0.925028         2\n",
       "2    205  0.380860         2\n",
       "3    424  0.255181         1\n",
       "4    284  0.055162         2\n",
       "5    667  0.558745         0\n",
       "6    730  0.311928         1\n",
       "7    469  0.233492         2\n",
       "8    199  0.165112         1\n",
       "9    699  0.261752         2\n",
       "10   231  0.951103         0\n",
       "11    26  0.558222         0\n",
       "12   786  0.549116         0\n",
       "13   849  0.301816         1\n",
       "14   459  0.739300         0\n",
       "15  1057  0.057549         0\n",
       "16  1151  0.625202         1\n",
       "17  1640  0.203252         0\n",
       "18  1561  0.245883         1\n",
       "19  1066  0.339369         2\n",
       "20  1455  0.979873         1\n",
       "21  1450  0.426661         0\n",
       "22  1722  0.150973         1\n",
       "23  1045  0.638008         2\n",
       "24  1289  0.529647         2\n",
       "25  1279  0.099430         0\n",
       "26  1617  0.903824         0\n",
       "27  1563  0.008819         2\n",
       "28  1682  0.587148         2\n",
       "29  1307  0.507218         1\n",
       "30  2163  0.269215         2\n",
       "31  2120  0.671698         0\n",
       "32  1865  0.873609         2\n",
       "33  2648  0.074802         2\n",
       "34  2548  0.698647         1\n",
       "35  2458  0.441319         1\n",
       "36  2093  0.805789         0\n",
       "37  2328  0.288185         1\n",
       "38  1903  0.520802         1\n",
       "39  2595  0.440071         0\n",
       "40  1818  0.580426         2\n",
       "41  2543  0.779798         0\n",
       "42  1822  0.746696         2\n",
       "43  2225  0.072081         1\n",
       "44  2390  0.567205         0\n",
       "45  3499  0.590148         0\n",
       "46  2782  0.976002         2\n",
       "47  3153  0.449470         2\n",
       "48  3123  0.677369         1\n",
       "49  2800  0.998705         1\n",
       "50  3401  0.779507         1\n",
       "51  3292  0.790033         1\n",
       "52  2807  0.143894         0\n",
       "53  2778  0.210702         0\n",
       "54  2938  0.802037         2\n",
       "55  2980  0.779076         0\n",
       "56  3576  0.403759         2\n",
       "57  3465  0.954013         2\n",
       "58  2767  0.491294         1\n",
       "59  3082  0.972140         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_cat contains the anomaly/noise type, which is in {0, 1, 2}\n",
    "# Only has 60 rows (15 users x 4 weeks)\n",
    "\n",
    "y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14ccea9-e508-4106-90eb-2ff8ac8dcd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>849</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3600</td>\n",
       "      <td>722</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3600</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3600</td>\n",
       "      <td>982</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600</td>\n",
       "      <td>749</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284949</th>\n",
       "      <td>4499</td>\n",
       "      <td>757</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284950</th>\n",
       "      <td>4499</td>\n",
       "      <td>752</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284951</th>\n",
       "      <td>4499</td>\n",
       "      <td>751</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284952</th>\n",
       "      <td>4499</td>\n",
       "      <td>778</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284953</th>\n",
       "      <td>4499</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284954 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  rating\n",
       "0       3600   849       5\n",
       "1       3600   722       5\n",
       "2       3600   462       4\n",
       "3       3600   982       4\n",
       "4       3600   749       4\n",
       "...      ...   ...     ...\n",
       "284949  4499   757       4\n",
       "284950  4499   752       4\n",
       "284951  4499   751       4\n",
       "284952  4499   778       4\n",
       "284953  4499     3       3\n",
       "\n",
       "[284954 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XX contains the second batch of data that we predict anomaly and noise on\n",
    "\n",
    "XX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6251994",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "For further explanation on how we decided on these features, refer to file `EDA.ipynb`.  \n",
    "*TODO: Add graphical explanations on why we chose these features in `EDA.ipynb`*   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7e3261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_components</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>cumulative_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.358422</td>\n",
       "      <td>0.358422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.436087</td>\n",
       "      <td>0.436087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.565867</td>\n",
       "      <td>0.565867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>0.670431</td>\n",
       "      <td>0.670431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>0.755216</td>\n",
       "      <td>0.755216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>0.824184</td>\n",
       "      <td>0.824184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>600</td>\n",
       "      <td>0.879620</td>\n",
       "      <td>0.879620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>650</td>\n",
       "      <td>0.902986</td>\n",
       "      <td>0.902986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>700</td>\n",
       "      <td>0.923665</td>\n",
       "      <td>0.923665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_components  explained_variance  cumulative_variance\n",
       "0            50            0.358422             0.358422\n",
       "1           100            0.436087             0.436087\n",
       "2           200            0.565867             0.565867\n",
       "3           300            0.670431             0.670431\n",
       "4           400            0.755216             0.755216\n",
       "5           500            0.824184             0.824184\n",
       "6           600            0.879620             0.879620\n",
       "7           650            0.902986             0.902986\n",
       "8           700            0.923665             0.923665"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended number of components: 600\n"
     ]
    }
   ],
   "source": [
    "# SVD Component Analysis - Find optimal number of components\n",
    "def analyze_svd_components(df_ratings, max_components=700):\n",
    "    \"\"\"\n",
    "    Analyze SVD components to find optimal dimensionality\n",
    "    Returns explained variance ratios for different component counts\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    component_range = [50, 100, 200, 300, 400, 500, 600, 650, 700]\n",
    "    \n",
    "    for n_comp in component_range:\n",
    "        if n_comp > min(df_ratings.shape):\n",
    "            continue\n",
    "            \n",
    "        svd = TruncatedSVD(n_components=n_comp, random_state=67)\n",
    "        svd.fit(df_ratings.values)\n",
    "        \n",
    "        explained_var = svd.explained_variance_ratio_.sum()\n",
    "        results.append({\n",
    "            'n_components': n_comp,\n",
    "            'explained_variance': explained_var,\n",
    "            'cumulative_variance': explained_var\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['n_components'], results_df['explained_variance'], 'o-', linewidth=2)\n",
    "    plt.axhline(y=0.85, color='r', linestyle='--', label='85% threshold')\n",
    "    plt.axhline(y=0.90, color='g', linestyle='--', label='90% threshold')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('SVD Component Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Test SVD optimization\n",
    "df_X_no_dupes = X.drop_duplicates(subset=[\"user\", \"item\"], keep=\"last\")\n",
    "df_ratings_test = df_X_no_dupes.pivot(index='user', columns='item', values='rating').fillna(-1)\n",
    "all_items = range(0, 1000)\n",
    "df_ratings_test = df_ratings_test.reindex(columns=all_items, fill_value=-1)\n",
    "\n",
    "svd_results = analyze_svd_components(df_ratings_test)\n",
    "display(svd_results)\n",
    "\n",
    "# Find optimal components (targeting ~85-90% variance explained)\n",
    "optimal_components = svd_results[svd_results['explained_variance'] >= 0.85]['n_components'].min()\n",
    "print(f\"\\nRecommended number of components: {optimal_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24361a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df_X, df_y=None, n_svd_components=300):\n",
    "    \"\"\"\n",
    "    Optimized feature engineering with configurable SVD components\n",
    "    Default reduced from 650 to 300 based on variance analysis\n",
    "    \"\"\"\n",
    "    df_X_no_dupes = df_X.drop_duplicates(subset=[\"user\", \"item\"], keep=\"last\")\n",
    "    df_ratings = df_X_no_dupes.pivot(index='user', columns='item', values='rating').fillna(-1)\n",
    "    all_items = range(0, 1000)\n",
    "    df_ratings = df_ratings.reindex(columns=all_items, fill_value=-1)\n",
    "\n",
    "    # Optimized SVD with fewer components\n",
    "    svd = TruncatedSVD(n_components=n_svd_components, random_state=67)\n",
    "    X_svd = svd.fit_transform(df_ratings.values)\n",
    "    svd_cols = [f\"svd_{i+1}\" for i in range(X_svd.shape[1])]\n",
    "    df_svd = pd.DataFrame(X_svd, columns=svd_cols, index=df_ratings.index)\n",
    "\n",
    "    # Basic user features\n",
    "    df_user_features = df_X.groupby(\"user\").agg(\n",
    "        mean_rating=(\"rating\", \"mean\"),\n",
    "        median_rating=(\"rating\", \"median\"),\n",
    "        std_rating=(\"rating\", \"std\"),\n",
    "        min_rating=(\"rating\", \"min\"),\n",
    "        max_rating=(\"rating\", \"max\"),\n",
    "        count_dislike=(\"rating\", lambda x: ((x == 0) | (x == 1) | (x == 2)).sum()),\n",
    "        count_neutral=(\"rating\", lambda x: (x == 3).sum()),\n",
    "        count_like=(\"rating\", lambda x: ((x == 4) | (x == 5)).sum()),\n",
    "        total_interactions=(\"rating\", \"count\"),\n",
    "    )\n",
    "\n",
    "    # Remove redundant rating_var (same as std_rating^2)\n",
    "    df_user_features[\"normalized_std\"] = (\n",
    "        df_user_features[\"std_rating\"] / (df_user_features[\"mean_rating\"] + 1e-5)\n",
    "    )\n",
    "\n",
    "    # Ratio features\n",
    "    df_user_features[\"like_ratio\"] = (\n",
    "        df_user_features[\"count_like\"] / df_user_features[\"total_interactions\"]\n",
    "    )\n",
    "    df_user_features[\"dislike_ratio\"] = (\n",
    "        df_user_features[\"count_dislike\"] / df_user_features[\"total_interactions\"]\n",
    "    )\n",
    "    df_user_features[\"neutral_ratio\"] = (\n",
    "        df_user_features[\"count_neutral\"] / df_user_features[\"total_interactions\"]\n",
    "    )\n",
    "\n",
    "    # num_items = df_X[\"item\"].nunique()\n",
    "    # df_user_features[\"interaction_ratio\"] = df_user_features[\"total_interactions\"] / num_items\n",
    "\n",
    "    # Weighted scores\n",
    "    df_user_features[\"weighted_score\"] = (\n",
    "        df_user_features[\"count_like\"] * 1.5 - df_user_features[\"count_dislike\"] * 1.5\n",
    "    )\n",
    "\n",
    "    # Distribution features\n",
    "    df_user_features[\"rating_kurtosis\"] = df_X.groupby(\"user\")[\"rating\"].apply(kurtosis)\n",
    "    df_user_features[\"rating_skewness\"] = df_X.groupby(\"user\")[\"rating\"].apply(skew)\n",
    "\n",
    "    # User bias (relative to item mean)\n",
    "    item_means = df_X.groupby(\"item\")[\"rating\"].mean().rename(\"item_mean\")\n",
    "    df_tmp = df_X.merge(item_means, on=\"item\", how=\"left\")\n",
    "    user_bias = df_tmp.groupby(\"user\").apply(\n",
    "        lambda df: (df[\"rating\"] - df[\"item_mean\"]).mean()\n",
    "    ).rename(\"user_bias\")\n",
    "    df_user_features = df_user_features.merge(user_bias, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Outliers\n",
    "    item_stats = df_X.groupby(\"item\")[\"rating\"].agg([\"mean\", \"std\"]).rename(\n",
    "        columns={\"mean\": \"item_mean\", \"std\": \"item_std\"}\n",
    "    )\n",
    "    df_tmp = df_X.merge(item_stats, on=\"item\", how=\"left\")\n",
    "    df_tmp[\"abs_dev\"] = abs(df_tmp[\"rating\"] - df_tmp[\"item_mean\"])\n",
    "    outlier_frac = (df_tmp[\"abs_dev\"] > 1.5 * df_tmp[\"item_std\"]).groupby(\n",
    "        df_tmp[\"user\"]\n",
    "    ).mean().rename(\"outlier_frac\")\n",
    "    df_user_features = df_user_features.merge(outlier_frac, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Mean item alignment\n",
    "    df_tmp[\"z_score_item\"] = (df_tmp[\"rating\"] - df_tmp[\"item_mean\"]) / (\n",
    "        df_tmp[\"item_std\"] + 1e-6\n",
    "    )\n",
    "    mean_item_alignment = df_tmp.groupby(\"user\")[\"z_score_item\"].mean().rename(\n",
    "        \"mean_item_alignment\"\n",
    "    )\n",
    "    df_user_features = df_user_features.merge(mean_item_alignment, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Entropy and extreme behaviour\n",
    "    user_entropy = df_X.groupby(\"user\")[\"rating\"].apply(\n",
    "        lambda x: entropy(x.value_counts(normalize=True))\n",
    "    ).rename(\"rating_entropy\")\n",
    "    extreme_ratio = df_X.groupby(\"user\").apply(\n",
    "        lambda df: ((df[\"rating\"] == 1) | (df[\"rating\"] == 5)).mean()\n",
    "    ).rename(\"extreme_ratio\")\n",
    "    df_user_features = df_user_features.merge(user_entropy, on=\"user\", how=\"left\")\n",
    "    df_user_features = df_user_features.merge(extreme_ratio, on=\"user\", how=\"left\")\n",
    "\n",
    "    # User mean rank\n",
    "    df_user_features[\"user_mean_rank\"] = df_user_features[\"mean_rating\"].rank(pct=True)\n",
    "\n",
    "    # Item popularity features\n",
    "    item_popularity = df_X.groupby(\"item\")[\"user\"].nunique().rename(\"item_popularity\")\n",
    "    item_rating_count = df_X.groupby(\"item\").size().rename(\"item_rating_count\")\n",
    "    df_tmp = df_X.merge(item_popularity, on=\"item\", how=\"left\")\n",
    "    df_tmp = df_tmp.merge(item_rating_count, on=\"item\", how=\"left\")\n",
    "    \n",
    "    user_avg_item_popularity = df_tmp.groupby(\"user\")[\"item_popularity\"].mean().rename(\n",
    "        \"avg_item_popularity\"\n",
    "    )\n",
    "    user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n",
    "        lambda x: (x[\"item_popularity\"] < x[\"item_popularity\"].quantile(0.25)).mean()\n",
    "    ).rename(\"rare_item_ratio\")\n",
    "    \n",
    "    df_user_features = df_user_features.merge(user_avg_item_popularity, on=\"user\", how=\"left\")\n",
    "    df_user_features = df_user_features.merge(user_rare_item_ratio, on=\"user\", how=\"left\")\n",
    "    \n",
    "    # Rating consistency features\n",
    "    user_rating_changes = df_X.sort_values([\"user\", \"item\"]).groupby(\"user\")[\"rating\"].apply(\n",
    "        lambda x: (x.diff().abs().mean()) if len(x) > 1 else 0\n",
    "    ).rename(\"rating_volatility\")\n",
    "    \n",
    "    df_user_features = df_user_features.merge(user_rating_changes, on=\"user\", how=\"left\")\n",
    "    \n",
    "    # Concentration features (Herfindahl index)\n",
    "    rating_concentration = df_X.groupby(\"user\")[\"rating\"].apply(\n",
    "        lambda x: (x.value_counts(normalize=True) ** 2).sum()\n",
    "    ).rename(\"rating_concentration\")\n",
    "    \n",
    "    df_user_features = df_user_features.merge(rating_concentration, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Fill NaN values\n",
    "    df_user_features = df_user_features.fillna(0)\n",
    "\n",
    "    final_df = pd.merge(df_svd.reset_index(), df_user_features, on='user')\n",
    "    \n",
    "    if df_y is not None:\n",
    "        df_merged = pd.merge(final_df.reset_index(), df_y, on=\"user\", how=\"inner\")\n",
    "        return df_merged.drop(columns=[\"index\"]).set_index(\"user\")\n",
    "    else:\n",
    "        return final_df.set_index(\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c14227",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43bd5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print MAE for regression task\n",
    "def evaluate_linear_predictions(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# To print accuracy for classification task\n",
    "def evaluate_classification_accuracy(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed7e2b",
   "metadata": {},
   "source": [
    "## Regression Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f862e",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d26bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_1</th>\n",
       "      <th>svd_2</th>\n",
       "      <th>svd_3</th>\n",
       "      <th>svd_4</th>\n",
       "      <th>svd_5</th>\n",
       "      <th>svd_6</th>\n",
       "      <th>svd_7</th>\n",
       "      <th>svd_8</th>\n",
       "      <th>svd_9</th>\n",
       "      <th>svd_10</th>\n",
       "      <th>...</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.860749</td>\n",
       "      <td>-6.068257</td>\n",
       "      <td>-9.349063</td>\n",
       "      <td>-11.332684</td>\n",
       "      <td>3.569385</td>\n",
       "      <td>18.300837</td>\n",
       "      <td>1.165931</td>\n",
       "      <td>4.843467</td>\n",
       "      <td>5.262733</td>\n",
       "      <td>-3.331607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>-0.676871</td>\n",
       "      <td>1.407269</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>1372.925743</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>1.273632</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>0.962817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.009061</td>\n",
       "      <td>4.579251</td>\n",
       "      <td>4.523304</td>\n",
       "      <td>-10.496718</td>\n",
       "      <td>-10.419450</td>\n",
       "      <td>-9.257935</td>\n",
       "      <td>1.974163</td>\n",
       "      <td>-2.492988</td>\n",
       "      <td>-0.088541</td>\n",
       "      <td>1.393418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>-0.400218</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>0.083582</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>1358.641791</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.880240</td>\n",
       "      <td>0.354564</td>\n",
       "      <td>0.031248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.236045</td>\n",
       "      <td>3.335757</td>\n",
       "      <td>19.619654</td>\n",
       "      <td>-9.828392</td>\n",
       "      <td>11.517358</td>\n",
       "      <td>-6.641359</td>\n",
       "      <td>-2.178109</td>\n",
       "      <td>2.165167</td>\n",
       "      <td>-1.300472</td>\n",
       "      <td>6.411729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041985</td>\n",
       "      <td>0.606235</td>\n",
       "      <td>0.866574</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.777639</td>\n",
       "      <td>1373.125954</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.536398</td>\n",
       "      <td>0.507750</td>\n",
       "      <td>0.068668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.282289</td>\n",
       "      <td>7.201780</td>\n",
       "      <td>-3.125457</td>\n",
       "      <td>2.699728</td>\n",
       "      <td>-8.397937</td>\n",
       "      <td>7.214575</td>\n",
       "      <td>11.027766</td>\n",
       "      <td>-7.014426</td>\n",
       "      <td>-0.113976</td>\n",
       "      <td>-1.846197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.511695</td>\n",
       "      <td>1.374852</td>\n",
       "      <td>0.301325</td>\n",
       "      <td>0.732639</td>\n",
       "      <td>1223.599338</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>1.016611</td>\n",
       "      <td>0.300776</td>\n",
       "      <td>0.349012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35.818199</td>\n",
       "      <td>37.524453</td>\n",
       "      <td>0.308825</td>\n",
       "      <td>-7.912539</td>\n",
       "      <td>-0.218294</td>\n",
       "      <td>-0.704625</td>\n",
       "      <td>-0.070853</td>\n",
       "      <td>-0.875664</td>\n",
       "      <td>-0.148923</td>\n",
       "      <td>0.212953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437630</td>\n",
       "      <td>0.854248</td>\n",
       "      <td>0.467836</td>\n",
       "      <td>0.970278</td>\n",
       "      <td>867.903509</td>\n",
       "      <td>0.251462</td>\n",
       "      <td>0.671554</td>\n",
       "      <td>0.463784</td>\n",
       "      <td>0.917704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          svd_1      svd_2      svd_3      svd_4      svd_5      svd_6  \\\n",
       "user                                                                     \n",
       "0     14.860749  -6.068257  -9.349063 -11.332684   3.569385  18.300837   \n",
       "1     33.009061   4.579251   4.523304 -10.496718 -10.419450  -9.257935   \n",
       "2     36.236045   3.335757  19.619654  -9.828392  11.517358  -6.641359   \n",
       "3     23.282289   7.201780  -3.125457   2.699728  -8.397937   7.214575   \n",
       "4    -35.818199  37.524453   0.308825  -7.912539  -0.218294  -0.704625   \n",
       "\n",
       "          svd_7     svd_8     svd_9    svd_10  ...  outlier_frac  \\\n",
       "user                                           ...                 \n",
       "0      1.165931  4.843467  5.262733 -3.331607  ...      0.272277   \n",
       "1      1.974163 -2.492988 -0.088541  1.393418  ...      0.038806   \n",
       "2     -2.178109  2.165167 -1.300472  6.411729  ...      0.041985   \n",
       "3     11.027766 -7.014426 -0.113976 -1.846197  ...      0.211921   \n",
       "4     -0.070853 -0.875664 -0.148923  0.212953  ...      0.000000   \n",
       "\n",
       "      mean_item_alignment  rating_entropy  extreme_ratio  user_mean_rank  \\\n",
       "user                                                                       \n",
       "0               -0.676871        1.407269       0.282178        0.023889   \n",
       "1               -0.400218        1.212858       0.083582        0.132500   \n",
       "2                0.606235        0.866574       0.118321        0.777639   \n",
       "3                0.511695        1.374852       0.301325        0.732639   \n",
       "4                0.437630        0.854248       0.467836        0.970278   \n",
       "\n",
       "      avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                            \n",
       "0             1372.925743         0.252475           1.273632   \n",
       "1             1358.641791         0.250746           0.880240   \n",
       "2             1373.125954         0.251908           0.536398   \n",
       "3             1223.599338         0.251656           1.016611   \n",
       "4              867.903509         0.251462           0.671554   \n",
       "\n",
       "      rating_concentration     label  \n",
       "user                                  \n",
       "0                 0.249240  0.962817  \n",
       "1                 0.354564  0.031248  \n",
       "2                 0.507750  0.068668  \n",
       "3                 0.300776  0.349012  \n",
       "4                 0.463784  0.917704  \n",
       "\n",
       "[5 rows x 327 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the initial dataframe for linear regression, using X and y\n",
    "df_reg = engineer_features(X, y)\n",
    "df_reg.columns = df_reg.columns.astype(str)\n",
    "\n",
    "display(df_reg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007957e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (regression):\n",
      "(2880, 326) (2880,) (720, 326) (720,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare and split dataset into train and validation set\n",
    "\n",
    "# Step 1: Extract features and labels\n",
    "X_lr = df_reg.drop(columns=[\"label\"]).values # Features\n",
    "y_lr = df_reg[\"label\"].values # Labels\n",
    "\n",
    "# Step 2: Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_lr, y_lr, test_size = 0.2, random_state=67\n",
    ")\n",
    "\n",
    "print(\"Shapes (regression):\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cae6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise features (for regression)\n",
    "\n",
    "scaler_reg = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler_reg.transform(X_train)\n",
    "X_val_std = scaler_reg.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed99a0",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68aa9a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: mean\n",
      "Features selected: 86 / 326\n",
      "Validation MAE: 0.0607\n",
      "Training MAE: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: median\n",
      "Features selected: 163 / 326\n",
      "Validation MAE: 0.0631\n",
      "Training MAE: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: 0.75*mean\n",
      "Features selected: 174 / 326\n",
      "Validation MAE: 0.0619\n",
      "Training MAE: 0.0212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>n_features</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>86</td>\n",
       "      <td>0.060657</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.038260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>163</td>\n",
       "      <td>0.063082</td>\n",
       "      <td>0.021701</td>\n",
       "      <td>0.041381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75*mean</td>\n",
       "      <td>174</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>0.040641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  n_features   val_mae  train_mae  overfitting\n",
       "0       mean          86  0.060657   0.022397     0.038260\n",
       "1     median         163  0.063082   0.021701     0.041381\n",
       "2  0.75*mean         174  0.061883   0.021242     0.040641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best threshold: mean\n",
      "\n",
      "Selected 86 features:\n",
      "['svd_1', 'svd_2', 'svd_3', 'svd_4', 'svd_5', 'svd_6', 'svd_7', 'svd_8', 'svd_9', 'svd_10', 'svd_11', 'svd_12', 'svd_13', 'svd_14', 'svd_15', 'svd_16', 'svd_17', 'svd_18', 'svd_19', 'svd_20'] ...\n"
     ]
    }
   ],
   "source": [
    "### Feature Selection using LGBM Importance\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Train LGBM to get feature importances\n",
    "lgbm_selector = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    learning_rate=0.05,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': df_reg.drop(columns=[\"label\"]).columns,\n",
    "    'importance': lgbm_selector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 30 features\n",
    "plt.figure(figsize=(10, 12))\n",
    "top_features = feature_importance.head(30)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 30 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select features using different thresholds\n",
    "thresholds = ['mean', 'median', '0.75*mean']\n",
    "results_comparison = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    selector = SelectFromModel(lgbm_selector, threshold=threshold, prefit=True)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Train model on selected features\n",
    "    lgbm_test = LGBMRegressor(\n",
    "        random_state=67,\n",
    "        max_depth=6,\n",
    "        num_leaves=64,\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.01,\n",
    "        reg_lambda=0.1,\n",
    "        reg_alpha=1.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbose=-1,\n",
    "        objective='mae',\n",
    "        metric='mae',\n",
    "        min_child_samples=20,\n",
    "        min_child_weight=0.001\n",
    "    )\n",
    "    \n",
    "    lgbm_test.fit(X_train_selected, y_train)\n",
    "    y_pred_val = lgbm_test.predict(X_val_selected)\n",
    "    y_pred_train = lgbm_test.predict(X_train_selected)\n",
    "    \n",
    "    mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'threshold': threshold,\n",
    "        'n_features': X_train_selected.shape[1],\n",
    "        'val_mae': mae_val,\n",
    "        'train_mae': mae_train,\n",
    "        'overfitting': mae_val - mae_train\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"Features selected: {X_train_selected.shape[1]} / {X_train.shape[1]}\")\n",
    "    print(f\"Validation MAE: {mae_val:.4f}\")\n",
    "    print(f\"Training MAE: {mae_train:.4f}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(results_comparison)\n",
    "display(comparison_df)\n",
    "\n",
    "# Choose best threshold\n",
    "best_threshold = comparison_df.loc[comparison_df['val_mae'].idxmin(), 'threshold']\n",
    "print(f\"\\nBest threshold: {best_threshold}\")\n",
    "\n",
    "# Create final selector\n",
    "selector_final = SelectFromModel(lgbm_selector, threshold=best_threshold, prefit=True)\n",
    "selected_features_mask = selector_final.get_support()\n",
    "selected_feature_names = df_reg.drop(columns=[\"label\"]).columns[selected_features_mask].tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_feature_names)} features:\")\n",
    "print(selected_feature_names[:20], \"...\")  # Show first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595e9b4",
   "metadata": {},
   "source": [
    "### Linear-based models (Linear Regression, Ridge, Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3f1adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.1072\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0910\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val_std)\n",
    "y_test = lr.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf20d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.1071\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0911\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train_std, y_train)\n",
    "y_pred = ridge.predict(X_val_std)\n",
    "y_test = ridge.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702ebd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.2079\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.2194\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_std, y_train)\n",
    "y_pred = lasso.predict(X_val_std)\n",
    "y_test = lasso.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3960ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.1182\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.1059\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (SGD) Regression\n",
    "\n",
    "sgd = SGDRegressor(random_state=67)\n",
    "sgd.fit(X_train_std, y_train)\n",
    "y_pred = sgd.predict(X_val_std)\n",
    "y_test = sgd.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01f96b",
   "metadata": {},
   "source": [
    "It appears that all linear-based models do not perform very well, which is expected as the data is not likely to follow a linear distribution.  \n",
    "We thus attempt using other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a099e",
   "metadata": {},
   "source": [
    "### Tree-based models (DecisionTree, XGBRegressor, LGBMRegressor, CatBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b885e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0935\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state=67)\n",
    "dtr.fit(X_train, y_train)\n",
    "y_pred = dtr.predict(X_val)\n",
    "y_test = dtr.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "257671c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0665\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0013\n"
     ]
    }
   ],
   "source": [
    "# XGB Regressor \n",
    "# XGB is an optimised software library that uses gradient boosted decision trees\n",
    "# We first try it without tweaking parameters\n",
    "\n",
    "xgbr = XGBRegressor(random_state=67)\n",
    "xgbr.fit(X_train, y_train)\n",
    "y_pred = xgbr.predict(X_val)\n",
    "y_test = xgbr.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecbfacd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0619\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Regressor\n",
    "# LightGBM is another well-known gradient boosting framework for efficient regression\n",
    "# We first try it without tweaking parameters\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=67)\n",
    "\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_pred = lgbm.predict(X_val)\n",
    "y_test = lgbm.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "039a612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0664\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0150\n"
     ]
    }
   ],
   "source": [
    "# CatBoostRegressor\n",
    "# CatBoost is the last of the well-known gradient boosting frameworks, known for its \n",
    "# high accuracy and ability to handle categorical features directly\n",
    "# We first try it without tweaking parameters\n",
    "\n",
    "cbr = CatBoostRegressor(random_state=67, verbose=0)\n",
    "\n",
    "cbr.fit(X_train_std, y_train)\n",
    "y_pred = cbr.predict(X_val_std)\n",
    "y_test = cbr.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b96d4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0623\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.6460\n"
     ]
    }
   ],
   "source": [
    "# LGBMRegressor appears to have the best performance without overfitting that much\n",
    "# We try to tune LGBMRegressor even more to deliver the best performance\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=0.001\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "target_transformer = QuantileTransformer(output_distribution='normal', random_state=67)\n",
    "y_train_transformed = target_transformer.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "lgbm.fit(X_train, y_train_transformed)\n",
    "y_pred_transformed = lgbm.predict(X_val)\n",
    "y_pred = target_transformer.inverse_transform(y_pred_transformed.reshape(-1, 1)).ravel()\n",
    "y_test = lgbm.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c1e8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code used to find LGBMRegressor parameters\n",
    "\n",
    "# Define ranges to sweep\n",
    "# depths = [3, 4, 5, 6, 7, 8]\n",
    "# lambdas = [0.0, 0.1, 0.5, 1.0]\n",
    "# learning_rates = [0.005, 0.008, 0.01]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Sweep over hyperparameters\n",
    "# for depth in depths:\n",
    "#     for lam in lambdas:\n",
    "#         for lr in learning_rates:\n",
    "#             model = LGBMRegressor(\n",
    "#                 random_state=67,\n",
    "#                 max_depth=depth,\n",
    "#                 num_leaves=2**depth,       # roughly matched to depth\n",
    "#                 n_estimators=3500,\n",
    "#                 learning_rate=lr,\n",
    "#                 reg_lambda=lam,\n",
    "#                 reg_alpha=0.1,\n",
    "#                 subsample=0.9,\n",
    "#                 colsample_bytree=0.9,\n",
    "#                 verbose=-1,\n",
    "#             )\n",
    "\n",
    "#             model.fit(X_train_std, y_train)\n",
    "#             y_train_pred = model.predict(X_train_std)\n",
    "#             y_val_pred = model.predict(X_val_std)\n",
    "\n",
    "#             train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "#             val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "#             results.append((depth, lam, lr, train_mae, val_mae))\n",
    "\n",
    "# # Convert to numpy for easy slicing\n",
    "# results = np.array(results, dtype=object)\n",
    "\n",
    "# # Pick the lowest validation MAE combo\n",
    "# best_idx = np.argmin(results[:, 4].astype(float))\n",
    "# best = results[best_idx]\n",
    "# print(f\"\\nBest config: depth={best[0]}, lambda={best[1]}, lr={best[2]} \"\n",
    "#       f\"→ train MAE={best[3]:.5f}, val MAE={best[4]:.5f}\")\n",
    "\n",
    "# # Plot curves by depth for the best lambda/lr combination\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# for lam in lambdas:\n",
    "#     subset = results[results[:, 1] == lam]\n",
    "#     plt.plot(subset[:, 0], subset[:, 3].astype(float), 'o--', label=f\"Train λ={lam}\")\n",
    "#     plt.plot(subset[:, 0], subset[:, 4].astype(float), 'o-', label=f\"Val λ={lam}\")\n",
    "\n",
    "# plt.xlabel(\"max_depth\")\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.title(\"Train vs Validation MAE across model complexity\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f616e2b",
   "metadata": {},
   "source": [
    "We find that Tree-based models perform much better than Linear-based models, which is understandable as the data likely does not follow a linear distribution.  \n",
    "In particular, the LGBM Regressor gave an exceptionally good performance out of the 3 most well-known gradient boosting frameworks.  \n",
    "This is likely because LGBM grows leaf-wise: it chooses the single leaf with the highest loss reduction to split next. Since our engineered features capture strong but localised signals, LightGBM's leaf-wise search can find them efficiently.  \n",
    "Based off this, we further fine-tuned LGBMRegressor to deliver the best performance we could.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8978c",
   "metadata": {},
   "source": [
    "### Ensemble-based models (RandomForest, BaggingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8185f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0672\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0267\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "rfr = RandomForestRegressor(random_state=67)\n",
    "rfr.fit(X_train_std, y_train)\n",
    "y_pred = rfr.predict(X_val_std)\n",
    "y_test = rfr.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae4f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0654\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0320\n"
     ]
    }
   ],
   "source": [
    "# Since Random Forest Regressor delivered great performance, we tried tuning it further\n",
    "# The performance did not increase much despite our best efforts\n",
    "\n",
    "rfr2 = RandomForestRegressor(\n",
    "    random_state=67,\n",
    "    n_estimators=1200,\n",
    "    n_jobs=-1,\n",
    "    max_depth=17,\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=10,\n",
    "    max_features=0.7,\n",
    "    )\n",
    "\n",
    "rfr2.fit(X_train_std, y_train)\n",
    "y_pred = rfr2.predict(X_val_std)\n",
    "y_test = rfr2.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d06328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0710\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0306\n"
     ]
    }
   ],
   "source": [
    "# Bagging Regressor\n",
    "\n",
    "br = BaggingRegressor(random_state=67)\n",
    "br.fit(X_train_std, y_train)\n",
    "y_pred = br.predict(X_val_std)\n",
    "y_test = br.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cb786",
   "metadata": {},
   "source": [
    "Ensemble-based methods also provide good performance out-of-the-box, but we find that Tree-based regressors provide the best performance out of all the supervised learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d35a6",
   "metadata": {},
   "source": [
    "### Neural Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c95a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0850\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0425\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer perceptron (MLP) Regressor with 4 layers\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.1,\n",
    "    learning_rate_init=0.003,\n",
    "    max_iter=5000,\n",
    "    random_state=67,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_std, y_train)\n",
    "y_pred = mlp.predict(X_val_std)\n",
    "y_test = mlp.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ad59f",
   "metadata": {},
   "source": [
    "After some fine tuning, the MLP Regressor is able to get decent performance on the validation dataset, but still not as good as the other types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe1d66",
   "metadata": {},
   "source": [
    "### Unsupervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "933c6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual result:\n",
      "MAE: 0.1426\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0489\n"
     ]
    }
   ],
   "source": [
    "# Trying out PCA + LGBMRegressor\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_val_pca = pca.transform(X_val_std)\n",
    "\n",
    "lgbm2 = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae'\n",
    ")\n",
    "\n",
    "lgbm2.fit(X_train_pca, y_train)\n",
    "y_pred = lgbm2.predict(X_val_pca)\n",
    "y_test = lgbm2.predict(X_train_pca)\n",
    "\n",
    "print(\"Actual result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b8862",
   "metadata": {},
   "source": [
    "Reducing the dimensionality of the data does not seem to work very well overall, even when we vary the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2177a12",
   "metadata": {},
   "source": [
    "### Stacking uncorrelated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62c4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.60365792 0.31032818 0.29620446 0.28511552]\n",
      " [0.60365792 1.         0.46708105 0.44877844 0.45135834]\n",
      " [0.31032818 0.46708105 1.         0.98698914 0.93715822]\n",
      " [0.29620446 0.44877844 0.98698914 1.         0.93479879]\n",
      " [0.28511552 0.45135834 0.93715822 0.93479879 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# We check the correlations between the predictions of the best models\n",
    "# If the models are too closely related, there is no point in stacking them\n",
    "\n",
    "pred_xgb = xgbr.predict(X_val_std)\n",
    "pred_lgbm = lgbm.predict(X_val_std)\n",
    "pred_cbr = cbr.predict(X_val_std)\n",
    "pred_rfr = rfr2.predict(X_val_std)\n",
    "pred_mlp = mlp.predict(X_val_std)\n",
    "\n",
    "corr_matrix = np.corrcoef([pred_xgb, pred_lgbm, pred_cbr, pred_rfr, pred_mlp])\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d504776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual result:\n",
      "MAE: 0.0624\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0264\n"
     ]
    }
   ],
   "source": [
    "# We try stacking models that are not strongly correlated (corr < 0.95)\n",
    "# This can potentially improve our performance, as the MLP can capture smooth nonlinear patterns missed by tree models\n",
    "# The stacking regressor can combine these predictions to reduce both bias and variance, improving overall MAE\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"lgbm\", LGBMRegressor(\n",
    "            random_state=67,\n",
    "            max_depth=6,\n",
    "            num_leaves=64,\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.01,\n",
    "            reg_lambda=0.1,\n",
    "            reg_alpha=1.0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            verbose=-1,\n",
    "            objective='mae',\n",
    "            metric='mae'\n",
    "        )),\n",
    "        (\"mlp\", MLPRegressor(\n",
    "            hidden_layer_sizes=(128, 64),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.1,\n",
    "            learning_rate_init=0.003,\n",
    "            max_iter=5000,\n",
    "            random_state=67,\n",
    "            batch_size=128\n",
    "        ))\n",
    "    ],\n",
    "    final_estimator=Ridge(alpha=0.1)\n",
    ")\n",
    "\n",
    "stack.fit(X_train_std, y_train)\n",
    "y_pred = stack.predict(X_val_std)\n",
    "y_test = stack.predict(X_train_std)\n",
    "\n",
    "print(\"Actual result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf1ead",
   "metadata": {},
   "source": [
    "Our hypothesis that stacking the two regressors increases performance does not prove true; the tuned LGBMRegressor still leads to roughly equivalent performance, and the slight improvement observed might just be due to noise.  \n",
    "We thus then retrain the single LGBMRegressor model with tuned parameters on the full dataset, to prepare it to predict on the final batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d6626",
   "metadata": {},
   "source": [
    "### Preparing final regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3fa774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training final model on full dataset\n",
    "\n",
    "# scaler_full = StandardScaler()\n",
    "# X_all_std = scaler_full.fit_transform(X_lr)\n",
    "\n",
    "# lgbm_final = LGBMRegressor(\n",
    "#     random_state=67,\n",
    "#     max_depth=6,\n",
    "#     num_leaves=64,\n",
    "#     n_estimators=5000,\n",
    "#     learning_rate=0.01,\n",
    "#     reg_lambda=0.1,\n",
    "#     reg_alpha=1.0,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     verbose=-1,\n",
    "#     objective='mae',\n",
    "#     metric='mae'\n",
    "# )\n",
    "\n",
    "# lgbm_final.fit(X_all_std, y_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "448f3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating features with optimized parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 326\n",
      "\n",
      "=== Retraining Feature Selector on Optimized Features ===\n",
      "\n",
      "Top 20 most important features:\n",
      "                  feature  importance\n",
      "322   avg_item_popularity         602\n",
      "1                   svd_2         417\n",
      "0                   svd_1         382\n",
      "314       rating_kurtosis         265\n",
      "2                   svd_3         247\n",
      "300           mean_rating         178\n",
      "307            count_like         176\n",
      "5                   svd_6         175\n",
      "317          outlier_frac         162\n",
      "3                   svd_4         144\n",
      "4                   svd_5         131\n",
      "7                   svd_8         122\n",
      "319        rating_entropy         120\n",
      "308    total_interactions         120\n",
      "309        normalized_std         114\n",
      "302            std_rating         109\n",
      "315       rating_skewness         108\n",
      "310            like_ratio         103\n",
      "325  rating_concentration         102\n",
      "6                   svd_7         100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: mean\n",
      "Features selected: 86 / 326\n",
      "Validation MAE: 0.0607\n",
      "Training MAE: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: median\n",
      "Features selected: 163 / 326\n",
      "Validation MAE: 0.0631\n",
      "Training MAE: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: 0.75*mean\n",
      "Features selected: 174 / 326\n",
      "Validation MAE: 0.0619\n",
      "Training MAE: 0.0212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>n_features</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>86</td>\n",
       "      <td>0.060657</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.038260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>163</td>\n",
       "      <td>0.063082</td>\n",
       "      <td>0.021701</td>\n",
       "      <td>0.041381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75*mean</td>\n",
       "      <td>174</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>0.040641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  n_features   val_mae  train_mae  overfitting\n",
       "0       mean          86  0.060657   0.022397     0.038260\n",
       "1     median         163  0.063082   0.021701     0.041381\n",
       "2  0.75*mean         174  0.061883   0.021242     0.040641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best threshold: mean\n",
      "\n",
      "Selected features: 86 / 326\n",
      "\n",
      "=== Training Final Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Model Performance:\n",
      "Validation MAE: 0.0607\n",
      "\n",
      "Final model trained on full dataset with optimized features!\n"
     ]
    }
   ],
   "source": [
    "### Training Final Model with Optimized Features\n",
    "\n",
    "# Regenerate features with optimal SVD components\n",
    "print(\"Regenerating features with optimized parameters...\")\n",
    "df_reg_optimized = engineer_features(X, y, n_svd_components=300)\n",
    "df_reg_optimized.columns = df_reg_optimized.columns.astype(str)\n",
    "\n",
    "# Extract features and labels\n",
    "X_lr_optimized = df_reg_optimized.drop(columns=[\"label\"]).values\n",
    "y_lr_optimized = df_reg_optimized[\"label\"].values\n",
    "\n",
    "# Split for validation\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_lr_optimized, y_lr_optimized, test_size=0.2, random_state=67\n",
    ")\n",
    "\n",
    "print(f\"Original features: {X_train_opt.shape[1]}\")\n",
    "\n",
    "# ==========================================\n",
    "# RETRAIN FEATURE SELECTOR ON NEW FEATURES\n",
    "# ==========================================\n",
    "print(\"\\n=== Retraining Feature Selector on Optimized Features ===\")\n",
    "\n",
    "# Train new LGBM for feature importance\n",
    "lgbm_selector_optimized = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    learning_rate=0.05,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_selector_optimized.fit(X_train_opt, y_train_opt)\n",
    "\n",
    "# Get feature importances for optimized features\n",
    "feature_importance_opt = pd.DataFrame({\n",
    "    'feature': df_reg_optimized.drop(columns=[\"label\"]).columns,\n",
    "    'importance': lgbm_selector_optimized.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(feature_importance_opt.head(20))\n",
    "\n",
    "# Try different thresholds to find best one\n",
    "thresholds = ['mean', 'median', '0.75*mean']\n",
    "results_comparison_opt = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    \n",
    "    selector_temp = SelectFromModel(lgbm_selector_optimized, threshold=threshold, prefit=True)\n",
    "    X_train_selected_temp = selector_temp.transform(X_train_opt)\n",
    "    X_val_selected_temp = selector_temp.transform(X_val_opt)\n",
    "    \n",
    "    # Train model on selected features\n",
    "    lgbm_test = LGBMRegressor(\n",
    "        random_state=67,\n",
    "        max_depth=6,\n",
    "        num_leaves=64,\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.01,\n",
    "        reg_lambda=0.1,\n",
    "        reg_alpha=1.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbose=-1,\n",
    "        objective='mae',\n",
    "        metric='mae',\n",
    "        min_child_samples=20,\n",
    "        min_child_weight=0.001\n",
    "    )\n",
    "    \n",
    "    lgbm_test.fit(X_train_selected_temp, y_train_opt)\n",
    "    y_pred_val_temp = lgbm_test.predict(X_val_selected_temp)\n",
    "    y_pred_train_temp = lgbm_test.predict(X_train_selected_temp)\n",
    "    \n",
    "    mae_val_temp = mean_absolute_error(y_val_opt, y_pred_val_temp)\n",
    "    mae_train_temp = mean_absolute_error(y_train_opt, y_pred_train_temp)\n",
    "    \n",
    "    results_comparison_opt.append({\n",
    "        'threshold': threshold,\n",
    "        'n_features': X_train_selected_temp.shape[1],\n",
    "        'val_mae': mae_val_temp,\n",
    "        'train_mae': mae_train_temp,\n",
    "        'overfitting': mae_val_temp - mae_train_temp\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"Features selected: {X_train_selected_temp.shape[1]} / {X_train_opt.shape[1]}\")\n",
    "    print(f\"Validation MAE: {mae_val_temp:.4f}\")\n",
    "    print(f\"Training MAE: {mae_train_temp:.4f}\")\n",
    "\n",
    "comparison_df_opt = pd.DataFrame(results_comparison_opt)\n",
    "display(comparison_df_opt)\n",
    "\n",
    "# Choose best threshold\n",
    "best_threshold_opt = comparison_df_opt.loc[comparison_df_opt['val_mae'].idxmin(), 'threshold']\n",
    "print(f\"\\nBest threshold: {best_threshold_opt}\")\n",
    "\n",
    "# Create FINAL selector for optimized features\n",
    "selector_final_optimized = SelectFromModel(\n",
    "    lgbm_selector_optimized, \n",
    "    threshold=best_threshold_opt, \n",
    "    prefit=True\n",
    ")\n",
    "\n",
    "# Apply feature selection\n",
    "X_train_opt_selected = selector_final_optimized.transform(X_train_opt)\n",
    "X_val_opt_selected = selector_final_optimized.transform(X_val_opt)\n",
    "\n",
    "print(f\"\\nSelected features: {X_train_opt_selected.shape[1]} / {X_train_opt.shape[1]}\")\n",
    "\n",
    "# ==========================================\n",
    "# TRAIN FINAL MODEL\n",
    "# ==========================================\n",
    "print(\"\\n=== Training Final Model ===\")\n",
    "\n",
    "lgbm_final = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=0.001\n",
    ")\n",
    "\n",
    "lgbm_final.fit(X_train_opt_selected, y_train_opt)\n",
    "y_pred_final = lgbm_final.predict(X_val_opt_selected)\n",
    "\n",
    "print(\"\\nOptimized Model Performance:\")\n",
    "print(f\"Validation MAE: {mean_absolute_error(y_val_opt, y_pred_final):.4f}\")\n",
    "\n",
    "# Train on full dataset\n",
    "X_all_opt_selected = selector_final_optimized.transform(X_lr_optimized)\n",
    "lgbm_final.fit(X_all_opt_selected, y_lr_optimized)\n",
    "\n",
    "print(\"\\nFinal model trained on full dataset with optimized features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1656f",
   "metadata": {},
   "source": [
    "## Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9357c7",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca475fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_1</th>\n",
       "      <th>svd_2</th>\n",
       "      <th>svd_3</th>\n",
       "      <th>svd_4</th>\n",
       "      <th>svd_5</th>\n",
       "      <th>svd_6</th>\n",
       "      <th>svd_7</th>\n",
       "      <th>svd_8</th>\n",
       "      <th>svd_9</th>\n",
       "      <th>svd_10</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.405226</td>\n",
       "      <td>1.056769</td>\n",
       "      <td>-21.038527</td>\n",
       "      <td>1.067305</td>\n",
       "      <td>26.873600</td>\n",
       "      <td>3.613976</td>\n",
       "      <td>-2.008504</td>\n",
       "      <td>-1.611251</td>\n",
       "      <td>4.041583</td>\n",
       "      <td>-4.170670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399362</td>\n",
       "      <td>1.503556</td>\n",
       "      <td>0.151639</td>\n",
       "      <td>0.248333</td>\n",
       "      <td>1296.213115</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.395062</td>\n",
       "      <td>0.254065</td>\n",
       "      <td>0.558222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>24.701986</td>\n",
       "      <td>4.278532</td>\n",
       "      <td>-6.918118</td>\n",
       "      <td>-14.194389</td>\n",
       "      <td>4.933048</td>\n",
       "      <td>8.060955</td>\n",
       "      <td>6.287393</td>\n",
       "      <td>4.449363</td>\n",
       "      <td>-1.056309</td>\n",
       "      <td>-0.967260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100368</td>\n",
       "      <td>1.441563</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>0.515833</td>\n",
       "      <td>1429.568720</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>1.219048</td>\n",
       "      <td>0.276117</td>\n",
       "      <td>0.165112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-30.651479</td>\n",
       "      <td>27.972663</td>\n",
       "      <td>-1.808480</td>\n",
       "      <td>-11.094628</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.539192</td>\n",
       "      <td>-0.447467</td>\n",
       "      <td>-1.427426</td>\n",
       "      <td>-0.590070</td>\n",
       "      <td>0.858616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477618</td>\n",
       "      <td>0.745018</td>\n",
       "      <td>0.471963</td>\n",
       "      <td>0.987222</td>\n",
       "      <td>873.476636</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.516432</td>\n",
       "      <td>0.491833</td>\n",
       "      <td>0.925028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>4.077090</td>\n",
       "      <td>27.859159</td>\n",
       "      <td>17.445291</td>\n",
       "      <td>-5.786915</td>\n",
       "      <td>-12.944545</td>\n",
       "      <td>-2.128206</td>\n",
       "      <td>-3.253641</td>\n",
       "      <td>-6.447440</td>\n",
       "      <td>2.451911</td>\n",
       "      <td>-3.451880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170565</td>\n",
       "      <td>1.399122</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>0.651111</td>\n",
       "      <td>1128.376022</td>\n",
       "      <td>0.250681</td>\n",
       "      <td>1.019126</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.380860</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>34.053427</td>\n",
       "      <td>3.629981</td>\n",
       "      <td>-14.458684</td>\n",
       "      <td>-5.522036</td>\n",
       "      <td>-10.271650</td>\n",
       "      <td>-4.786838</td>\n",
       "      <td>2.645228</td>\n",
       "      <td>9.534818</td>\n",
       "      <td>-3.295730</td>\n",
       "      <td>0.831030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.633656</td>\n",
       "      <td>1.384519</td>\n",
       "      <td>0.265363</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>1383.776536</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>1.263305</td>\n",
       "      <td>0.250882</td>\n",
       "      <td>0.951103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>19.995440</td>\n",
       "      <td>-3.963606</td>\n",
       "      <td>36.543998</td>\n",
       "      <td>-12.160488</td>\n",
       "      <td>-0.507532</td>\n",
       "      <td>10.580456</td>\n",
       "      <td>-0.204711</td>\n",
       "      <td>1.751987</td>\n",
       "      <td>4.764085</td>\n",
       "      <td>-3.733991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846178</td>\n",
       "      <td>0.858244</td>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.859167</td>\n",
       "      <td>1249.322870</td>\n",
       "      <td>0.251121</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.511392</td>\n",
       "      <td>0.055162</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>19.515725</td>\n",
       "      <td>1.946307</td>\n",
       "      <td>-11.297455</td>\n",
       "      <td>-12.750778</td>\n",
       "      <td>-1.955778</td>\n",
       "      <td>5.562889</td>\n",
       "      <td>2.733605</td>\n",
       "      <td>-0.980205</td>\n",
       "      <td>-1.747588</td>\n",
       "      <td>-3.314120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079770</td>\n",
       "      <td>1.255755</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>1388.605000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.327250</td>\n",
       "      <td>0.255181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>19.229018</td>\n",
       "      <td>-4.886454</td>\n",
       "      <td>20.073144</td>\n",
       "      <td>-14.276159</td>\n",
       "      <td>7.799430</td>\n",
       "      <td>1.149055</td>\n",
       "      <td>9.521034</td>\n",
       "      <td>-0.200415</td>\n",
       "      <td>-3.550065</td>\n",
       "      <td>5.731454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395315</td>\n",
       "      <td>1.475948</td>\n",
       "      <td>0.256522</td>\n",
       "      <td>0.178333</td>\n",
       "      <td>1343.843478</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>1.371179</td>\n",
       "      <td>0.242722</td>\n",
       "      <td>0.739300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>15.833040</td>\n",
       "      <td>10.900338</td>\n",
       "      <td>17.698568</td>\n",
       "      <td>-12.457373</td>\n",
       "      <td>2.723195</td>\n",
       "      <td>-2.610725</td>\n",
       "      <td>8.489573</td>\n",
       "      <td>-7.575831</td>\n",
       "      <td>-0.392949</td>\n",
       "      <td>-2.492455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079066</td>\n",
       "      <td>1.315856</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.533056</td>\n",
       "      <td>1279.308550</td>\n",
       "      <td>0.249071</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.311618</td>\n",
       "      <td>0.233492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>12.384787</td>\n",
       "      <td>-1.564100</td>\n",
       "      <td>-9.387893</td>\n",
       "      <td>-5.248689</td>\n",
       "      <td>2.829056</td>\n",
       "      <td>3.688155</td>\n",
       "      <td>-6.718119</td>\n",
       "      <td>-1.332926</td>\n",
       "      <td>-4.809891</td>\n",
       "      <td>2.177380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143844</td>\n",
       "      <td>1.446732</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.543611</td>\n",
       "      <td>1214.692683</td>\n",
       "      <td>0.248780</td>\n",
       "      <td>1.171569</td>\n",
       "      <td>0.263534</td>\n",
       "      <td>0.383316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>35.282715</td>\n",
       "      <td>1.605001</td>\n",
       "      <td>8.985617</td>\n",
       "      <td>3.324267</td>\n",
       "      <td>-25.776958</td>\n",
       "      <td>8.908518</td>\n",
       "      <td>-4.520455</td>\n",
       "      <td>3.688603</td>\n",
       "      <td>1.437349</td>\n",
       "      <td>4.809446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.362430</td>\n",
       "      <td>1.339007</td>\n",
       "      <td>0.173423</td>\n",
       "      <td>0.137222</td>\n",
       "      <td>1260.198198</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.031603</td>\n",
       "      <td>0.285965</td>\n",
       "      <td>0.558745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>22.899398</td>\n",
       "      <td>28.790211</td>\n",
       "      <td>-2.853864</td>\n",
       "      <td>0.518014</td>\n",
       "      <td>-5.737385</td>\n",
       "      <td>13.667635</td>\n",
       "      <td>7.112067</td>\n",
       "      <td>7.904376</td>\n",
       "      <td>-0.047028</td>\n",
       "      <td>1.585041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315420</td>\n",
       "      <td>1.338535</td>\n",
       "      <td>0.246649</td>\n",
       "      <td>0.706111</td>\n",
       "      <td>1237.621984</td>\n",
       "      <td>0.249330</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.297242</td>\n",
       "      <td>0.261752</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>10.109697</td>\n",
       "      <td>-0.534179</td>\n",
       "      <td>9.083594</td>\n",
       "      <td>-9.989619</td>\n",
       "      <td>5.325208</td>\n",
       "      <td>3.840458</td>\n",
       "      <td>-0.516786</td>\n",
       "      <td>-0.355747</td>\n",
       "      <td>-4.024504</td>\n",
       "      <td>4.291348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352857</td>\n",
       "      <td>1.234931</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>0.249583</td>\n",
       "      <td>1190.303419</td>\n",
       "      <td>0.252137</td>\n",
       "      <td>0.948498</td>\n",
       "      <td>0.341369</td>\n",
       "      <td>0.311928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>28.727091</td>\n",
       "      <td>4.460696</td>\n",
       "      <td>-17.737821</td>\n",
       "      <td>-8.568014</td>\n",
       "      <td>2.184781</td>\n",
       "      <td>-4.353866</td>\n",
       "      <td>-7.278990</td>\n",
       "      <td>8.086681</td>\n",
       "      <td>-4.452230</td>\n",
       "      <td>-3.061602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.529118</td>\n",
       "      <td>1.389305</td>\n",
       "      <td>0.174216</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>1406.651568</td>\n",
       "      <td>0.250871</td>\n",
       "      <td>1.220280</td>\n",
       "      <td>0.269920</td>\n",
       "      <td>0.549116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>16.708753</td>\n",
       "      <td>-1.445953</td>\n",
       "      <td>4.006640</td>\n",
       "      <td>-10.407775</td>\n",
       "      <td>-1.372284</td>\n",
       "      <td>4.169791</td>\n",
       "      <td>-7.593610</td>\n",
       "      <td>-1.823122</td>\n",
       "      <td>-0.533504</td>\n",
       "      <td>-0.710331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>1.200819</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>1270.547170</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.359381</td>\n",
       "      <td>0.301816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>-14.589255</td>\n",
       "      <td>24.841091</td>\n",
       "      <td>0.592879</td>\n",
       "      <td>-12.930398</td>\n",
       "      <td>3.003011</td>\n",
       "      <td>-0.479167</td>\n",
       "      <td>0.592950</td>\n",
       "      <td>1.268763</td>\n",
       "      <td>0.800157</td>\n",
       "      <td>-2.415303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238899</td>\n",
       "      <td>1.287192</td>\n",
       "      <td>0.387234</td>\n",
       "      <td>0.809722</td>\n",
       "      <td>1072.093617</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>1.004274</td>\n",
       "      <td>0.321865</td>\n",
       "      <td>0.638008</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>50.942770</td>\n",
       "      <td>10.806731</td>\n",
       "      <td>-19.721361</td>\n",
       "      <td>8.340857</td>\n",
       "      <td>-32.353401</td>\n",
       "      <td>0.202926</td>\n",
       "      <td>9.124116</td>\n",
       "      <td>0.243394</td>\n",
       "      <td>-3.626521</td>\n",
       "      <td>2.615711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540825</td>\n",
       "      <td>1.136690</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>1306.529851</td>\n",
       "      <td>0.251244</td>\n",
       "      <td>0.807980</td>\n",
       "      <td>0.428195</td>\n",
       "      <td>0.057549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>6.660772</td>\n",
       "      <td>18.774476</td>\n",
       "      <td>2.211635</td>\n",
       "      <td>-13.141572</td>\n",
       "      <td>-12.306636</td>\n",
       "      <td>3.991652</td>\n",
       "      <td>11.318564</td>\n",
       "      <td>2.828238</td>\n",
       "      <td>3.027072</td>\n",
       "      <td>-6.186215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381093</td>\n",
       "      <td>1.201517</td>\n",
       "      <td>0.214575</td>\n",
       "      <td>0.741111</td>\n",
       "      <td>1248.655870</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.334065</td>\n",
       "      <td>0.339369</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>4.565552</td>\n",
       "      <td>2.001047</td>\n",
       "      <td>-4.991591</td>\n",
       "      <td>-2.488794</td>\n",
       "      <td>3.774549</td>\n",
       "      <td>-0.880130</td>\n",
       "      <td>-1.752852</td>\n",
       "      <td>-1.050395</td>\n",
       "      <td>1.504125</td>\n",
       "      <td>-2.953061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292262</td>\n",
       "      <td>1.300817</td>\n",
       "      <td>0.210046</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>1109.118721</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.922018</td>\n",
       "      <td>0.316862</td>\n",
       "      <td>0.625202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>55.143343</td>\n",
       "      <td>12.673649</td>\n",
       "      <td>21.268550</td>\n",
       "      <td>27.536053</td>\n",
       "      <td>-8.691158</td>\n",
       "      <td>1.590301</td>\n",
       "      <td>1.867865</td>\n",
       "      <td>2.002636</td>\n",
       "      <td>-10.896158</td>\n",
       "      <td>5.159406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150875</td>\n",
       "      <td>1.158217</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.300278</td>\n",
       "      <td>1184.531250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.784038</td>\n",
       "      <td>0.419419</td>\n",
       "      <td>0.099430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>-3.964172</td>\n",
       "      <td>22.893215</td>\n",
       "      <td>-5.001387</td>\n",
       "      <td>-11.151228</td>\n",
       "      <td>4.800311</td>\n",
       "      <td>5.292133</td>\n",
       "      <td>4.868103</td>\n",
       "      <td>5.233376</td>\n",
       "      <td>-2.822094</td>\n",
       "      <td>1.064827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362125</td>\n",
       "      <td>1.069240</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>1130.827731</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.755274</td>\n",
       "      <td>0.413424</td>\n",
       "      <td>0.529647</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>9.982597</td>\n",
       "      <td>1.233299</td>\n",
       "      <td>4.366212</td>\n",
       "      <td>-4.498651</td>\n",
       "      <td>-0.542160</td>\n",
       "      <td>-3.126854</td>\n",
       "      <td>5.185793</td>\n",
       "      <td>1.389276</td>\n",
       "      <td>-1.535660</td>\n",
       "      <td>-0.518658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148607</td>\n",
       "      <td>1.239811</td>\n",
       "      <td>0.078067</td>\n",
       "      <td>0.373472</td>\n",
       "      <td>1155.007435</td>\n",
       "      <td>0.245353</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.346416</td>\n",
       "      <td>0.507218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>33.229563</td>\n",
       "      <td>5.877482</td>\n",
       "      <td>-19.276149</td>\n",
       "      <td>-9.445955</td>\n",
       "      <td>-9.482312</td>\n",
       "      <td>-5.610207</td>\n",
       "      <td>2.965458</td>\n",
       "      <td>-3.737145</td>\n",
       "      <td>-8.958739</td>\n",
       "      <td>1.904718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260829</td>\n",
       "      <td>1.496491</td>\n",
       "      <td>0.186851</td>\n",
       "      <td>0.235556</td>\n",
       "      <td>1422.553633</td>\n",
       "      <td>0.245675</td>\n",
       "      <td>1.253472</td>\n",
       "      <td>0.240550</td>\n",
       "      <td>0.426661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>-9.410127</td>\n",
       "      <td>1.405786</td>\n",
       "      <td>-4.863681</td>\n",
       "      <td>4.097648</td>\n",
       "      <td>-0.428274</td>\n",
       "      <td>0.271327</td>\n",
       "      <td>6.355352</td>\n",
       "      <td>-0.929348</td>\n",
       "      <td>-0.500602</td>\n",
       "      <td>-0.158288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.687801</td>\n",
       "      <td>1.301151</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.153889</td>\n",
       "      <td>919.012308</td>\n",
       "      <td>0.249231</td>\n",
       "      <td>1.046296</td>\n",
       "      <td>0.300317</td>\n",
       "      <td>0.979873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>18.630213</td>\n",
       "      <td>0.309461</td>\n",
       "      <td>4.104553</td>\n",
       "      <td>-12.629557</td>\n",
       "      <td>-7.013169</td>\n",
       "      <td>-0.524197</td>\n",
       "      <td>-2.458145</td>\n",
       "      <td>4.225193</td>\n",
       "      <td>4.917298</td>\n",
       "      <td>-0.953974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>1.513973</td>\n",
       "      <td>0.182222</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>1292.951111</td>\n",
       "      <td>0.248889</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.258746</td>\n",
       "      <td>0.245883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>46.562663</td>\n",
       "      <td>8.590602</td>\n",
       "      <td>-5.380619</td>\n",
       "      <td>17.140850</td>\n",
       "      <td>-0.568038</td>\n",
       "      <td>1.429418</td>\n",
       "      <td>4.274856</td>\n",
       "      <td>11.332335</td>\n",
       "      <td>-3.256507</td>\n",
       "      <td>-1.132952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379207</td>\n",
       "      <td>1.188743</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>1250.078394</td>\n",
       "      <td>0.248566</td>\n",
       "      <td>0.768199</td>\n",
       "      <td>0.352295</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>16.365574</td>\n",
       "      <td>-5.234236</td>\n",
       "      <td>18.390408</td>\n",
       "      <td>-18.399680</td>\n",
       "      <td>-0.653204</td>\n",
       "      <td>-1.960561</td>\n",
       "      <td>-5.301496</td>\n",
       "      <td>-8.029610</td>\n",
       "      <td>-0.166289</td>\n",
       "      <td>-5.879510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554063</td>\n",
       "      <td>1.378308</td>\n",
       "      <td>0.200893</td>\n",
       "      <td>0.083889</td>\n",
       "      <td>1359.825893</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228700</td>\n",
       "      <td>0.253866</td>\n",
       "      <td>0.903824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>48.347671</td>\n",
       "      <td>6.326245</td>\n",
       "      <td>26.561090</td>\n",
       "      <td>3.600284</td>\n",
       "      <td>-18.627871</td>\n",
       "      <td>4.337232</td>\n",
       "      <td>-23.631518</td>\n",
       "      <td>0.098326</td>\n",
       "      <td>-5.827032</td>\n",
       "      <td>3.219304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101233</td>\n",
       "      <td>1.553445</td>\n",
       "      <td>0.273707</td>\n",
       "      <td>0.433056</td>\n",
       "      <td>1268.467672</td>\n",
       "      <td>0.247845</td>\n",
       "      <td>1.382289</td>\n",
       "      <td>0.219986</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>-8.846367</td>\n",
       "      <td>22.965476</td>\n",
       "      <td>-9.353197</td>\n",
       "      <td>-9.323590</td>\n",
       "      <td>-1.110193</td>\n",
       "      <td>-6.428081</td>\n",
       "      <td>-0.770292</td>\n",
       "      <td>-3.809209</td>\n",
       "      <td>-0.839959</td>\n",
       "      <td>1.847927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252205</td>\n",
       "      <td>1.291371</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>1076.918367</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.327514</td>\n",
       "      <td>0.587148</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>32.194061</td>\n",
       "      <td>4.328843</td>\n",
       "      <td>-12.861302</td>\n",
       "      <td>-8.997731</td>\n",
       "      <td>-0.403383</td>\n",
       "      <td>5.000128</td>\n",
       "      <td>3.642669</td>\n",
       "      <td>-3.721963</td>\n",
       "      <td>-6.917732</td>\n",
       "      <td>0.772333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000830</td>\n",
       "      <td>1.565127</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.401944</td>\n",
       "      <td>1381.936567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228464</td>\n",
       "      <td>0.229923</td>\n",
       "      <td>0.150973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>-11.240644</td>\n",
       "      <td>38.191345</td>\n",
       "      <td>2.053651</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>-3.187149</td>\n",
       "      <td>0.353069</td>\n",
       "      <td>-3.564333</td>\n",
       "      <td>-4.061618</td>\n",
       "      <td>2.608191</td>\n",
       "      <td>-3.025811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144829</td>\n",
       "      <td>1.375909</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.735556</td>\n",
       "      <td>1036.200000</td>\n",
       "      <td>0.243678</td>\n",
       "      <td>0.907834</td>\n",
       "      <td>0.274652</td>\n",
       "      <td>0.580426</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>-22.530234</td>\n",
       "      <td>31.830443</td>\n",
       "      <td>-5.155541</td>\n",
       "      <td>-6.073602</td>\n",
       "      <td>2.675920</td>\n",
       "      <td>1.076088</td>\n",
       "      <td>-2.166935</td>\n",
       "      <td>-1.123116</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>-0.718346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342201</td>\n",
       "      <td>1.073918</td>\n",
       "      <td>0.371025</td>\n",
       "      <td>0.890833</td>\n",
       "      <td>960.448763</td>\n",
       "      <td>0.250883</td>\n",
       "      <td>0.687943</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.746696</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>-25.681521</td>\n",
       "      <td>25.066416</td>\n",
       "      <td>-0.517062</td>\n",
       "      <td>-11.098346</td>\n",
       "      <td>0.320960</td>\n",
       "      <td>-0.722261</td>\n",
       "      <td>-0.316471</td>\n",
       "      <td>-1.407688</td>\n",
       "      <td>-1.067839</td>\n",
       "      <td>2.649285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388896</td>\n",
       "      <td>1.000470</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.928611</td>\n",
       "      <td>929.228070</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.638767</td>\n",
       "      <td>0.416551</td>\n",
       "      <td>0.873609</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>12.691205</td>\n",
       "      <td>1.361096</td>\n",
       "      <td>1.730498</td>\n",
       "      <td>-4.063565</td>\n",
       "      <td>2.698011</td>\n",
       "      <td>-0.263857</td>\n",
       "      <td>-2.108461</td>\n",
       "      <td>0.051035</td>\n",
       "      <td>-0.066095</td>\n",
       "      <td>0.649859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131732</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.388194</td>\n",
       "      <td>1176.053640</td>\n",
       "      <td>0.249042</td>\n",
       "      <td>0.776923</td>\n",
       "      <td>0.375024</td>\n",
       "      <td>0.520802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>26.208748</td>\n",
       "      <td>1.223819</td>\n",
       "      <td>-0.750656</td>\n",
       "      <td>-14.497578</td>\n",
       "      <td>21.059123</td>\n",
       "      <td>1.539668</td>\n",
       "      <td>3.829611</td>\n",
       "      <td>-2.383363</td>\n",
       "      <td>5.219389</td>\n",
       "      <td>4.505694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490975</td>\n",
       "      <td>1.552178</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.184722</td>\n",
       "      <td>1452.404545</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.534247</td>\n",
       "      <td>0.227603</td>\n",
       "      <td>0.805789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>25.898912</td>\n",
       "      <td>-0.875074</td>\n",
       "      <td>17.089111</td>\n",
       "      <td>-16.163478</td>\n",
       "      <td>-3.181579</td>\n",
       "      <td>-3.717163</td>\n",
       "      <td>-12.909274</td>\n",
       "      <td>0.747702</td>\n",
       "      <td>7.321352</td>\n",
       "      <td>-2.666413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345681</td>\n",
       "      <td>1.414951</td>\n",
       "      <td>0.164794</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>1379.501873</td>\n",
       "      <td>0.250936</td>\n",
       "      <td>1.135338</td>\n",
       "      <td>0.263982</td>\n",
       "      <td>0.671698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>5.944021</td>\n",
       "      <td>10.971361</td>\n",
       "      <td>-7.028612</td>\n",
       "      <td>-3.729516</td>\n",
       "      <td>16.252878</td>\n",
       "      <td>4.574362</td>\n",
       "      <td>-5.971668</td>\n",
       "      <td>6.157987</td>\n",
       "      <td>4.949524</td>\n",
       "      <td>5.259026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066686</td>\n",
       "      <td>1.431018</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.595556</td>\n",
       "      <td>1206.288136</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>1.068085</td>\n",
       "      <td>0.281313</td>\n",
       "      <td>0.269215</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>26.131340</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>23.011535</td>\n",
       "      <td>-12.728496</td>\n",
       "      <td>8.075412</td>\n",
       "      <td>2.314124</td>\n",
       "      <td>10.166097</td>\n",
       "      <td>-1.258650</td>\n",
       "      <td>-5.445485</td>\n",
       "      <td>4.485016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507646</td>\n",
       "      <td>1.174809</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>1314.291304</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>0.359282</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>16.477927</td>\n",
       "      <td>17.254603</td>\n",
       "      <td>15.624358</td>\n",
       "      <td>22.925548</td>\n",
       "      <td>-3.732207</td>\n",
       "      <td>-5.545847</td>\n",
       "      <td>-0.287058</td>\n",
       "      <td>2.493493</td>\n",
       "      <td>-2.856931</td>\n",
       "      <td>-1.472151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829380</td>\n",
       "      <td>1.724176</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.034167</td>\n",
       "      <td>1052.803008</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>1.692771</td>\n",
       "      <td>0.187160</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>30.734405</td>\n",
       "      <td>0.195722</td>\n",
       "      <td>12.751227</td>\n",
       "      <td>-7.813799</td>\n",
       "      <td>6.543426</td>\n",
       "      <td>-2.407274</td>\n",
       "      <td>-2.644401</td>\n",
       "      <td>1.560591</td>\n",
       "      <td>-2.331377</td>\n",
       "      <td>13.273872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257941</td>\n",
       "      <td>1.568366</td>\n",
       "      <td>0.281967</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>1335.285246</td>\n",
       "      <td>0.249180</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.216469</td>\n",
       "      <td>0.567205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>11.901789</td>\n",
       "      <td>2.450261</td>\n",
       "      <td>-10.521368</td>\n",
       "      <td>-6.382197</td>\n",
       "      <td>-5.168251</td>\n",
       "      <td>-3.993819</td>\n",
       "      <td>-6.422926</td>\n",
       "      <td>3.484094</td>\n",
       "      <td>-3.876889</td>\n",
       "      <td>4.011051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379774</td>\n",
       "      <td>1.355320</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>0.668611</td>\n",
       "      <td>1236.621359</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.287445</td>\n",
       "      <td>0.441319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>28.762754</td>\n",
       "      <td>-1.056746</td>\n",
       "      <td>-3.490334</td>\n",
       "      <td>-7.397567</td>\n",
       "      <td>-6.997795</td>\n",
       "      <td>20.513098</td>\n",
       "      <td>6.709869</td>\n",
       "      <td>1.375369</td>\n",
       "      <td>-1.890947</td>\n",
       "      <td>-4.380331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519832</td>\n",
       "      <td>1.414330</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.056944</td>\n",
       "      <td>1375.405844</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>1.211726</td>\n",
       "      <td>0.250042</td>\n",
       "      <td>0.779798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>0.742950</td>\n",
       "      <td>-2.627058</td>\n",
       "      <td>-3.835256</td>\n",
       "      <td>-6.145804</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>-3.033757</td>\n",
       "      <td>-0.100329</td>\n",
       "      <td>-0.043938</td>\n",
       "      <td>-1.506061</td>\n",
       "      <td>-1.085429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>1.577346</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.459306</td>\n",
       "      <td>1072.140000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.567839</td>\n",
       "      <td>0.235550</td>\n",
       "      <td>0.698647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>22.258937</td>\n",
       "      <td>-0.865007</td>\n",
       "      <td>15.276752</td>\n",
       "      <td>-17.701465</td>\n",
       "      <td>14.155350</td>\n",
       "      <td>-4.693674</td>\n",
       "      <td>-2.443168</td>\n",
       "      <td>-2.805695</td>\n",
       "      <td>-2.804628</td>\n",
       "      <td>-2.417563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244673</td>\n",
       "      <td>1.444424</td>\n",
       "      <td>0.187793</td>\n",
       "      <td>0.311389</td>\n",
       "      <td>1383.929577</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>1.179245</td>\n",
       "      <td>0.269149</td>\n",
       "      <td>0.440071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>25.401560</td>\n",
       "      <td>8.282808</td>\n",
       "      <td>-31.277136</td>\n",
       "      <td>13.145835</td>\n",
       "      <td>16.997314</td>\n",
       "      <td>-7.945109</td>\n",
       "      <td>-8.147648</td>\n",
       "      <td>-8.896687</td>\n",
       "      <td>-5.843419</td>\n",
       "      <td>-0.626392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206216</td>\n",
       "      <td>1.502625</td>\n",
       "      <td>0.374194</td>\n",
       "      <td>0.606944</td>\n",
       "      <td>1226.735484</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>1.291262</td>\n",
       "      <td>0.242310</td>\n",
       "      <td>0.074802</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>13.706690</td>\n",
       "      <td>7.791517</td>\n",
       "      <td>7.306411</td>\n",
       "      <td>10.541593</td>\n",
       "      <td>-9.516593</td>\n",
       "      <td>3.309520</td>\n",
       "      <td>-1.542335</td>\n",
       "      <td>4.351730</td>\n",
       "      <td>-3.554876</td>\n",
       "      <td>-5.031538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099977</td>\n",
       "      <td>0.877015</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.395417</td>\n",
       "      <td>1083.274038</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>0.573494</td>\n",
       "      <td>0.482329</td>\n",
       "      <td>0.491294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>26.912932</td>\n",
       "      <td>-0.454641</td>\n",
       "      <td>14.015744</td>\n",
       "      <td>-9.998340</td>\n",
       "      <td>-8.905399</td>\n",
       "      <td>8.952896</td>\n",
       "      <td>-11.056320</td>\n",
       "      <td>6.633441</td>\n",
       "      <td>-5.127221</td>\n",
       "      <td>-1.530622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206427</td>\n",
       "      <td>0.998815</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>1320.083893</td>\n",
       "      <td>0.251678</td>\n",
       "      <td>0.690236</td>\n",
       "      <td>0.473807</td>\n",
       "      <td>0.210702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>-30.570426</td>\n",
       "      <td>24.732419</td>\n",
       "      <td>-2.079454</td>\n",
       "      <td>-11.137881</td>\n",
       "      <td>1.518234</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.446444</td>\n",
       "      <td>-1.345274</td>\n",
       "      <td>1.313954</td>\n",
       "      <td>0.345504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559664</td>\n",
       "      <td>0.747592</td>\n",
       "      <td>0.563981</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>856.459716</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.496193</td>\n",
       "      <td>0.976002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-5.828762</td>\n",
       "      <td>2.545959</td>\n",
       "      <td>-0.645852</td>\n",
       "      <td>1.055244</td>\n",
       "      <td>-12.794977</td>\n",
       "      <td>-0.612679</td>\n",
       "      <td>-6.507652</td>\n",
       "      <td>0.730545</td>\n",
       "      <td>-2.138035</td>\n",
       "      <td>-1.641358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006055</td>\n",
       "      <td>1.310332</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.496944</td>\n",
       "      <td>986.786713</td>\n",
       "      <td>0.251748</td>\n",
       "      <td>1.017544</td>\n",
       "      <td>0.306739</td>\n",
       "      <td>0.998705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>22.675271</td>\n",
       "      <td>-1.925411</td>\n",
       "      <td>13.604115</td>\n",
       "      <td>-20.048106</td>\n",
       "      <td>-3.945546</td>\n",
       "      <td>2.531374</td>\n",
       "      <td>5.503931</td>\n",
       "      <td>-0.571000</td>\n",
       "      <td>0.317331</td>\n",
       "      <td>7.455540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073768</td>\n",
       "      <td>1.407055</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>1436.803828</td>\n",
       "      <td>0.248804</td>\n",
       "      <td>1.139423</td>\n",
       "      <td>0.290264</td>\n",
       "      <td>0.143894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>-28.685449</td>\n",
       "      <td>28.085449</td>\n",
       "      <td>2.383516</td>\n",
       "      <td>-6.788596</td>\n",
       "      <td>2.347656</td>\n",
       "      <td>0.579053</td>\n",
       "      <td>0.203732</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-0.268027</td>\n",
       "      <td>-1.756875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403970</td>\n",
       "      <td>1.070504</td>\n",
       "      <td>0.466135</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>901.521912</td>\n",
       "      <td>0.239044</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.400232</td>\n",
       "      <td>0.802037</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>15.635537</td>\n",
       "      <td>-4.674515</td>\n",
       "      <td>-21.750005</td>\n",
       "      <td>-6.050642</td>\n",
       "      <td>-6.309088</td>\n",
       "      <td>-6.900510</td>\n",
       "      <td>-12.422318</td>\n",
       "      <td>-2.050268</td>\n",
       "      <td>-4.072678</td>\n",
       "      <td>3.729193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569125</td>\n",
       "      <td>1.468102</td>\n",
       "      <td>0.294372</td>\n",
       "      <td>0.048333</td>\n",
       "      <td>1323.142857</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>1.339130</td>\n",
       "      <td>0.239145</td>\n",
       "      <td>0.779076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>31.667955</td>\n",
       "      <td>3.968138</td>\n",
       "      <td>-15.258337</td>\n",
       "      <td>-11.228208</td>\n",
       "      <td>-4.849364</td>\n",
       "      <td>-6.858703</td>\n",
       "      <td>2.468430</td>\n",
       "      <td>-1.729865</td>\n",
       "      <td>-0.044167</td>\n",
       "      <td>0.559501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530439</td>\n",
       "      <td>1.379800</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>1479.183099</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.190813</td>\n",
       "      <td>0.253323</td>\n",
       "      <td>0.972140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>5.693312</td>\n",
       "      <td>3.724910</td>\n",
       "      <td>0.837205</td>\n",
       "      <td>-2.108318</td>\n",
       "      <td>3.092695</td>\n",
       "      <td>0.562513</td>\n",
       "      <td>0.972588</td>\n",
       "      <td>1.002036</td>\n",
       "      <td>4.446282</td>\n",
       "      <td>0.449313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322635</td>\n",
       "      <td>1.302367</td>\n",
       "      <td>0.220408</td>\n",
       "      <td>0.696111</td>\n",
       "      <td>1116.616327</td>\n",
       "      <td>0.236735</td>\n",
       "      <td>0.897541</td>\n",
       "      <td>0.301858</td>\n",
       "      <td>0.677369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>-3.423319</td>\n",
       "      <td>17.990912</td>\n",
       "      <td>-9.466775</td>\n",
       "      <td>-8.092630</td>\n",
       "      <td>0.987898</td>\n",
       "      <td>3.026215</td>\n",
       "      <td>-4.959364</td>\n",
       "      <td>-1.497612</td>\n",
       "      <td>7.117767</td>\n",
       "      <td>1.759992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>1.537289</td>\n",
       "      <td>0.336032</td>\n",
       "      <td>0.601389</td>\n",
       "      <td>1153.404858</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>1.264228</td>\n",
       "      <td>0.248472</td>\n",
       "      <td>0.449470</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>-2.689915</td>\n",
       "      <td>5.327463</td>\n",
       "      <td>0.237086</td>\n",
       "      <td>8.228715</td>\n",
       "      <td>-0.121224</td>\n",
       "      <td>0.351049</td>\n",
       "      <td>-2.448782</td>\n",
       "      <td>-0.694674</td>\n",
       "      <td>-3.174310</td>\n",
       "      <td>-1.754148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129394</td>\n",
       "      <td>1.087075</td>\n",
       "      <td>0.029326</td>\n",
       "      <td>0.446528</td>\n",
       "      <td>976.988270</td>\n",
       "      <td>0.249267</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.393951</td>\n",
       "      <td>0.790033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>2.436708</td>\n",
       "      <td>2.565077</td>\n",
       "      <td>2.710996</td>\n",
       "      <td>2.542875</td>\n",
       "      <td>-0.756122</td>\n",
       "      <td>-1.675883</td>\n",
       "      <td>0.576098</td>\n",
       "      <td>-1.094740</td>\n",
       "      <td>1.337314</td>\n",
       "      <td>-3.562950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647279</td>\n",
       "      <td>1.152592</td>\n",
       "      <td>0.329218</td>\n",
       "      <td>0.849167</td>\n",
       "      <td>1036.823045</td>\n",
       "      <td>0.251029</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.354096</td>\n",
       "      <td>0.779507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>-36.729628</td>\n",
       "      <td>36.734406</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-7.837553</td>\n",
       "      <td>-0.683447</td>\n",
       "      <td>1.128580</td>\n",
       "      <td>0.061302</td>\n",
       "      <td>-1.135128</td>\n",
       "      <td>0.110130</td>\n",
       "      <td>0.619779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473018</td>\n",
       "      <td>0.800311</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.987778</td>\n",
       "      <td>851.797101</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.477421</td>\n",
       "      <td>0.954013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>15.530337</td>\n",
       "      <td>-4.464295</td>\n",
       "      <td>12.255507</td>\n",
       "      <td>-9.370663</td>\n",
       "      <td>-5.453862</td>\n",
       "      <td>1.494034</td>\n",
       "      <td>7.607356</td>\n",
       "      <td>-7.700408</td>\n",
       "      <td>-5.541105</td>\n",
       "      <td>9.505316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316729</td>\n",
       "      <td>1.405600</td>\n",
       "      <td>0.162055</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>1246.418972</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>1.146825</td>\n",
       "      <td>0.261010</td>\n",
       "      <td>0.590148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>11.747589</td>\n",
       "      <td>45.567337</td>\n",
       "      <td>6.371927</td>\n",
       "      <td>23.781958</td>\n",
       "      <td>9.076805</td>\n",
       "      <td>-1.852480</td>\n",
       "      <td>-0.988570</td>\n",
       "      <td>1.938058</td>\n",
       "      <td>-2.953191</td>\n",
       "      <td>-0.210857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416527</td>\n",
       "      <td>1.057506</td>\n",
       "      <td>0.228041</td>\n",
       "      <td>0.816944</td>\n",
       "      <td>1059.508446</td>\n",
       "      <td>0.241554</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.429151</td>\n",
       "      <td>0.403759</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          svd_1      svd_2      svd_3      svd_4      svd_5      svd_6  \\\n",
       "user                                                                     \n",
       "26    19.405226   1.056769 -21.038527   1.067305  26.873600   3.613976   \n",
       "199   24.701986   4.278532  -6.918118 -14.194389   4.933048   8.060955   \n",
       "202  -30.651479  27.972663  -1.808480 -11.094628   0.104153   0.539192   \n",
       "205    4.077090  27.859159  17.445291  -5.786915 -12.944545  -2.128206   \n",
       "231   34.053427   3.629981 -14.458684  -5.522036 -10.271650  -4.786838   \n",
       "284   19.995440  -3.963606  36.543998 -12.160488  -0.507532  10.580456   \n",
       "424   19.515725   1.946307 -11.297455 -12.750778  -1.955778   5.562889   \n",
       "459   19.229018  -4.886454  20.073144 -14.276159   7.799430   1.149055   \n",
       "469   15.833040  10.900338  17.698568 -12.457373   2.723195  -2.610725   \n",
       "561   12.384787  -1.564100  -9.387893  -5.248689   2.829056   3.688155   \n",
       "667   35.282715   1.605001   8.985617   3.324267 -25.776958   8.908518   \n",
       "699   22.899398  28.790211  -2.853864   0.518014  -5.737385  13.667635   \n",
       "730   10.109697  -0.534179   9.083594  -9.989619   5.325208   3.840458   \n",
       "786   28.727091   4.460696 -17.737821  -8.568014   2.184781  -4.353866   \n",
       "849   16.708753  -1.445953   4.006640 -10.407775  -1.372284   4.169791   \n",
       "1045 -14.589255  24.841091   0.592879 -12.930398   3.003011  -0.479167   \n",
       "1057  50.942770  10.806731 -19.721361   8.340857 -32.353401   0.202926   \n",
       "1066   6.660772  18.774476   2.211635 -13.141572 -12.306636   3.991652   \n",
       "1151   4.565552   2.001047  -4.991591  -2.488794   3.774549  -0.880130   \n",
       "1279  55.143343  12.673649  21.268550  27.536053  -8.691158   1.590301   \n",
       "1289  -3.964172  22.893215  -5.001387 -11.151228   4.800311   5.292133   \n",
       "1307   9.982597   1.233299   4.366212  -4.498651  -0.542160  -3.126854   \n",
       "1450  33.229563   5.877482 -19.276149  -9.445955  -9.482312  -5.610207   \n",
       "1455  -9.410127   1.405786  -4.863681   4.097648  -0.428274   0.271327   \n",
       "1561  18.630213   0.309461   4.104553 -12.629557  -7.013169  -0.524197   \n",
       "1563  46.562663   8.590602  -5.380619  17.140850  -0.568038   1.429418   \n",
       "1617  16.365574  -5.234236  18.390408 -18.399680  -0.653204  -1.960561   \n",
       "1640  48.347671   6.326245  26.561090   3.600284 -18.627871   4.337232   \n",
       "1682  -8.846367  22.965476  -9.353197  -9.323590  -1.110193  -6.428081   \n",
       "1722  32.194061   4.328843 -12.861302  -8.997731  -0.403383   5.000128   \n",
       "1818 -11.240644  38.191345   2.053651   0.160269  -3.187149   0.353069   \n",
       "1822 -22.530234  31.830443  -5.155541  -6.073602   2.675920   1.076088   \n",
       "1865 -25.681521  25.066416  -0.517062 -11.098346   0.320960  -0.722261   \n",
       "1903  12.691205   1.361096   1.730498  -4.063565   2.698011  -0.263857   \n",
       "2093  26.208748   1.223819  -0.750656 -14.497578  21.059123   1.539668   \n",
       "2120  25.898912  -0.875074  17.089111 -16.163478  -3.181579  -3.717163   \n",
       "2163   5.944021  10.971361  -7.028612  -3.729516  16.252878   4.574362   \n",
       "2225  26.131340   0.000671  23.011535 -12.728496   8.075412   2.314124   \n",
       "2328  16.477927  17.254603  15.624358  22.925548  -3.732207  -5.545847   \n",
       "2390  30.734405   0.195722  12.751227  -7.813799   6.543426  -2.407274   \n",
       "2458  11.901789   2.450261 -10.521368  -6.382197  -5.168251  -3.993819   \n",
       "2543  28.762754  -1.056746  -3.490334  -7.397567  -6.997795  20.513098   \n",
       "2548   0.742950  -2.627058  -3.835256  -6.145804   0.009457  -3.033757   \n",
       "2595  22.258937  -0.865007  15.276752 -17.701465  14.155350  -4.693674   \n",
       "2648  25.401560   8.282808 -31.277136  13.145835  16.997314  -7.945109   \n",
       "2767  13.706690   7.791517   7.306411  10.541593  -9.516593   3.309520   \n",
       "2778  26.912932  -0.454641  14.015744  -9.998340  -8.905399   8.952896   \n",
       "2782 -30.570426  24.732419  -2.079454 -11.137881   1.518234   0.011092   \n",
       "2800  -5.828762   2.545959  -0.645852   1.055244 -12.794977  -0.612679   \n",
       "2807  22.675271  -1.925411  13.604115 -20.048106  -3.945546   2.531374   \n",
       "2938 -28.685449  28.085449   2.383516  -6.788596   2.347656   0.579053   \n",
       "2980  15.635537  -4.674515 -21.750005  -6.050642  -6.309088  -6.900510   \n",
       "3082  31.667955   3.968138 -15.258337 -11.228208  -4.849364  -6.858703   \n",
       "3123   5.693312   3.724910   0.837205  -2.108318   3.092695   0.562513   \n",
       "3153  -3.423319  17.990912  -9.466775  -8.092630   0.987898   3.026215   \n",
       "3292  -2.689915   5.327463   0.237086   8.228715  -0.121224   0.351049   \n",
       "3401   2.436708   2.565077   2.710996   2.542875  -0.756122  -1.675883   \n",
       "3465 -36.729628  36.734406  -0.051635  -7.837553  -0.683447   1.128580   \n",
       "3499  15.530337  -4.464295  12.255507  -9.370663  -5.453862   1.494034   \n",
       "3576  11.747589  45.567337   6.371927  23.781958   9.076805  -1.852480   \n",
       "\n",
       "          svd_7      svd_8      svd_9     svd_10  ...  mean_item_alignment  \\\n",
       "user                                              ...                        \n",
       "26    -2.008504  -1.611251   4.041583  -4.170670  ...            -0.399362   \n",
       "199    6.287393   4.449363  -1.056309  -0.967260  ...             0.100368   \n",
       "202   -0.447467  -1.427426  -0.590070   0.858616  ...             0.477618   \n",
       "205   -3.253641  -6.447440   2.451911  -3.451880  ...             0.170565   \n",
       "231    2.645228   9.534818  -3.295730   0.831030  ...            -0.633656   \n",
       "284   -0.204711   1.751987   4.764085  -3.733991  ...             0.846178   \n",
       "424    2.733605  -0.980205  -1.747588  -3.314120  ...             0.079770   \n",
       "459    9.521034  -0.200415  -3.550065   5.731454  ...            -0.395315   \n",
       "469    8.489573  -7.575831  -0.392949  -2.492455  ...             0.079066   \n",
       "561   -6.718119  -1.332926  -4.809891   2.177380  ...             0.143844   \n",
       "667   -4.520455   3.688603   1.437349   4.809446  ...            -0.362430   \n",
       "699    7.112067   7.904376  -0.047028   1.585041  ...             0.315420   \n",
       "730   -0.516786  -0.355747  -4.024504   4.291348  ...            -0.352857   \n",
       "786   -7.278990   8.086681  -4.452230  -3.061602  ...            -0.529118   \n",
       "849   -7.593610  -1.823122  -0.533504  -0.710331  ...             0.282600   \n",
       "1045   0.592950   1.268763   0.800157  -2.415303  ...             0.238899   \n",
       "1057   9.124116   0.243394  -3.626521   2.615711  ...             0.540825   \n",
       "1066  11.318564   2.828238   3.027072  -6.186215  ...             0.381093   \n",
       "1151  -1.752852  -1.050395   1.504125  -2.953061  ...             0.292262   \n",
       "1279   1.867865   2.002636 -10.896158   5.159406  ...            -0.150875   \n",
       "1289   4.868103   5.233376  -2.822094   1.064827  ...             0.362125   \n",
       "1307   5.185793   1.389276  -1.535660  -0.518658  ...            -0.148607   \n",
       "1450   2.965458  -3.737145  -8.958739   1.904718  ...            -0.260829   \n",
       "1455   6.355352  -0.929348  -0.500602  -0.158288  ...            -0.687801   \n",
       "1561  -2.458145   4.225193   4.917298  -0.953974  ...             0.040100   \n",
       "1563   4.274856  11.332335  -3.256507  -1.132952  ...            -0.379207   \n",
       "1617  -5.301496  -8.029610  -0.166289  -5.879510  ...            -0.554063   \n",
       "1640 -23.631518   0.098326  -5.827032   3.219304  ...             0.101233   \n",
       "1682  -0.770292  -3.809209  -0.839959   1.847927  ...             0.252205   \n",
       "1722   3.642669  -3.721963  -6.917732   0.772333  ...            -0.000830   \n",
       "1818  -3.564333  -4.061618   2.608191  -3.025811  ...             0.144829   \n",
       "1822  -2.166935  -1.123116   0.019499  -0.718346  ...             0.342201   \n",
       "1865  -0.316471  -1.407688  -1.067839   2.649285  ...             0.388896   \n",
       "1903  -2.108461   0.051035  -0.066095   0.649859  ...            -0.131732   \n",
       "2093   3.829611  -2.383363   5.219389   4.505694  ...            -0.490975   \n",
       "2120 -12.909274   0.747702   7.321352  -2.666413  ...            -0.345681   \n",
       "2163  -5.971668   6.157987   4.949524   5.259026  ...             0.066686   \n",
       "2225  10.166097  -1.258650  -5.445485   4.485016  ...             0.507646   \n",
       "2328  -0.287058   2.493493  -2.856931  -1.472151  ...            -0.829380   \n",
       "2390  -2.644401   1.560591  -2.331377  13.273872  ...            -0.257941   \n",
       "2458  -6.422926   3.484094  -3.876889   4.011051  ...             0.379774   \n",
       "2543   6.709869   1.375369  -1.890947  -4.380331  ...            -0.519832   \n",
       "2548  -0.100329  -0.043938  -1.506061  -1.085429  ...            -0.049011   \n",
       "2595  -2.443168  -2.805695  -2.804628  -2.417563  ...            -0.244673   \n",
       "2648  -8.147648  -8.896687  -5.843419  -0.626392  ...             0.206216   \n",
       "2767  -1.542335   4.351730  -3.554876  -5.031538  ...            -0.099977   \n",
       "2778 -11.056320   6.633441  -5.127221  -1.530622  ...            -0.206427   \n",
       "2782   0.446444  -1.345274   1.313954   0.345504  ...             0.559664   \n",
       "2800  -6.507652   0.730545  -2.138035  -1.641358  ...            -0.006055   \n",
       "2807   5.503931  -0.571000   0.317331   7.455540  ...            -0.073768   \n",
       "2938   0.203732  -0.804791  -0.268027  -1.756875  ...             0.403970   \n",
       "2980 -12.422318  -2.050268  -4.072678   3.729193  ...            -0.569125   \n",
       "3082   2.468430  -1.729865  -0.044167   0.559501  ...            -0.530439   \n",
       "3123   0.972588   1.002036   4.446282   0.449313  ...             0.322635   \n",
       "3153  -4.959364  -1.497612   7.117767   1.759992  ...             0.000731   \n",
       "3292  -2.448782  -0.694674  -3.174310  -1.754148  ...            -0.129394   \n",
       "3401   0.576098  -1.094740   1.337314  -3.562950  ...             0.647279   \n",
       "3465   0.061302  -1.135128   0.110130   0.619779  ...             0.473018   \n",
       "3499   7.607356  -7.700408  -5.541105   9.505316  ...            -0.316729   \n",
       "3576  -0.988570   1.938058  -2.953191  -0.210857  ...             0.416527   \n",
       "\n",
       "      rating_entropy  extreme_ratio  user_mean_rank  avg_item_popularity  \\\n",
       "user                                                                       \n",
       "26          1.503556       0.151639        0.248333          1296.213115   \n",
       "199         1.441563       0.189573        0.515833          1429.568720   \n",
       "202         0.745018       0.471963        0.987222           873.476636   \n",
       "205         1.399122       0.234332        0.651111          1128.376022   \n",
       "231         1.384519       0.265363        0.028056          1383.776536   \n",
       "284         0.858244       0.219731        0.859167          1249.322870   \n",
       "424         1.255755       0.075000        0.479167          1388.605000   \n",
       "459         1.475948       0.256522        0.178333          1343.843478   \n",
       "469         1.315856       0.148699        0.533056          1279.308550   \n",
       "561         1.446732       0.214634        0.543611          1214.692683   \n",
       "667         1.339007       0.173423        0.137222          1260.198198   \n",
       "699         1.338535       0.246649        0.706111          1237.621984   \n",
       "730         1.234931       0.047009        0.249583          1190.303419   \n",
       "786         1.389305       0.174216        0.098611          1406.651568   \n",
       "849         1.200819       0.099057        0.604167          1270.547170   \n",
       "1045        1.287192       0.387234        0.809722          1072.093617   \n",
       "1057        1.136690       0.119403        0.678333          1306.529851   \n",
       "1066        1.201517       0.214575        0.741111          1248.655870   \n",
       "1151        1.300817       0.210046        0.677778          1109.118721   \n",
       "1279        1.158217       0.070312        0.300278          1184.531250   \n",
       "1289        1.069240       0.256303        0.836667          1130.827731   \n",
       "1307        1.239811       0.078067        0.373472          1155.007435   \n",
       "1450        1.496491       0.186851        0.235556          1422.553633   \n",
       "1455        1.301151       0.092308        0.153889           919.012308   \n",
       "1561        1.513973       0.182222        0.436944          1292.951111   \n",
       "1563        1.188743       0.053537        0.162500          1250.078394   \n",
       "1617        1.378308       0.200893        0.083889          1359.825893   \n",
       "1640        1.553445       0.273707        0.433056          1268.467672   \n",
       "1682        1.291371       0.424490        0.792500          1076.918367   \n",
       "1722        1.565127       0.223881        0.401944          1381.936567   \n",
       "1818        1.375909       0.310345        0.735556          1036.200000   \n",
       "1822        1.073918       0.371025        0.890833           960.448763   \n",
       "1865        1.000470       0.412281        0.928611           929.228070   \n",
       "1903        1.154581       0.053640        0.388194          1176.053640   \n",
       "2093        1.552178       0.345455        0.184722          1452.404545   \n",
       "2120        1.414951       0.164794        0.192500          1379.501873   \n",
       "2163        1.431018       0.211864        0.595556          1206.288136   \n",
       "2225        1.174809       0.182609        0.729444          1314.291304   \n",
       "2328        1.724176       0.345865        0.034167          1052.803008   \n",
       "2390        1.568366       0.281967        0.243056          1335.285246   \n",
       "2458        1.355320       0.242718        0.668611          1236.621359   \n",
       "2543        1.414330       0.211039        0.056944          1375.405844   \n",
       "2548        1.577346       0.230000        0.459306          1072.140000   \n",
       "2595        1.444424       0.187793        0.311389          1383.929577   \n",
       "2648        1.502625       0.374194        0.606944          1226.735484   \n",
       "2767        0.877015       0.002404        0.395417          1083.274038   \n",
       "2778        0.998815       0.063758        0.253333          1320.083893   \n",
       "2782        0.747592       0.563981        0.998056           856.459716   \n",
       "2800        1.310332       0.115385        0.496944           986.786713   \n",
       "2807        1.407055       0.114833        0.317500          1436.803828   \n",
       "2938        1.070504       0.466135        0.923889           901.521912   \n",
       "2980        1.468102       0.294372        0.048333          1323.142857   \n",
       "3082        1.379800       0.225352        0.071111          1479.183099   \n",
       "3123        1.302367       0.220408        0.696111          1116.616327   \n",
       "3153        1.537289       0.336032        0.601389          1153.404858   \n",
       "3292        1.087075       0.029326        0.446528           976.988270   \n",
       "3401        1.152592       0.329218        0.849167          1036.823045   \n",
       "3465        0.800311       0.489855        0.987778           851.797101   \n",
       "3499        1.405600       0.162055        0.205556          1246.418972   \n",
       "3576        1.057506       0.228041        0.816944          1059.508446   \n",
       "\n",
       "      rare_item_ratio  rating_volatility  rating_concentration     label  \\\n",
       "user                                                                       \n",
       "26           0.250000           1.395062              0.254065  0.558222   \n",
       "199          0.251185           1.219048              0.276117  0.165112   \n",
       "202          0.252336           0.516432              0.491833  0.925028   \n",
       "205          0.250681           1.019126              0.279347  0.380860   \n",
       "231          0.251397           1.263305              0.250882  0.951103   \n",
       "284          0.251121           0.540541              0.511392  0.055162   \n",
       "424          0.250000           0.864322              0.327250  0.255181   \n",
       "459          0.252174           1.371179              0.242722  0.739300   \n",
       "469          0.249071           0.891791              0.311618  0.233492   \n",
       "561          0.248780           1.171569              0.263534  0.383316   \n",
       "667          0.250000           1.031603              0.285965  0.558745   \n",
       "699          0.249330           0.930108              0.297242  0.261752   \n",
       "730          0.252137           0.948498              0.341369  0.311928   \n",
       "786          0.250871           1.220280              0.269920  0.549116   \n",
       "849          0.250000           0.895735              0.359381  0.301816   \n",
       "1045         0.246809           1.004274              0.321865  0.638008   \n",
       "1057         0.251244           0.807980              0.428195  0.057549   \n",
       "1066         0.251012           0.841463              0.334065  0.339369   \n",
       "1151         0.251142           0.922018              0.316862  0.625202   \n",
       "1279         0.250000           0.784038              0.419419  0.099430   \n",
       "1289         0.252101           0.755274              0.413424  0.529647   \n",
       "1307         0.245353           0.888060              0.346416  0.507218   \n",
       "1450         0.245675           1.253472              0.240550  0.426661   \n",
       "1455         0.249231           1.046296              0.300317  0.979873   \n",
       "1561         0.248889           1.187500              0.258746  0.245883   \n",
       "1563         0.248566           0.768199              0.352295  0.008819   \n",
       "1617         0.250000           1.228700              0.253866  0.903824   \n",
       "1640         0.247845           1.382289              0.219986  0.203252   \n",
       "1682         0.244898           0.991803              0.327514  0.587148   \n",
       "1722         0.250000           1.228464              0.229923  0.150973   \n",
       "1818         0.243678           0.907834              0.274652  0.580426   \n",
       "1822         0.250883           0.687943              0.393200  0.746696   \n",
       "1865         0.245614           0.638767              0.416551  0.873609   \n",
       "1903         0.249042           0.776923              0.375024  0.520802   \n",
       "2093         0.250000           1.534247              0.227603  0.805789   \n",
       "2120         0.250936           1.135338              0.263982  0.671698   \n",
       "2163         0.245763           1.068085              0.281313  0.269215   \n",
       "2225         0.252174           0.786026              0.359282  0.072081   \n",
       "2328         0.248120           1.692771              0.187160  0.288185   \n",
       "2390         0.249180           1.312500              0.216469  0.567205   \n",
       "2458         0.252427           0.975610              0.287445  0.441319   \n",
       "2543         0.246753           1.211726              0.250042  0.779798   \n",
       "2548         0.250000           1.567839              0.235550  0.698647   \n",
       "2595         0.248826           1.179245              0.269149  0.440071   \n",
       "2648         0.251613           1.291262              0.242310  0.074802   \n",
       "2767         0.247596           0.573494              0.482329  0.491294   \n",
       "2778         0.251678           0.690236              0.473807  0.210702   \n",
       "2782         0.251185           0.571429              0.496193  0.976002   \n",
       "2800         0.251748           1.017544              0.306739  0.998705   \n",
       "2807         0.248804           1.139423              0.290264  0.143894   \n",
       "2938         0.239044           0.792000              0.400232  0.802037   \n",
       "2980         0.251082           1.339130              0.239145  0.779076   \n",
       "3082         0.250000           1.190813              0.253323  0.972140   \n",
       "3123         0.236735           0.897541              0.301858  0.677369   \n",
       "3153         0.251012           1.264228              0.248472  0.449470   \n",
       "3292         0.249267           0.785294              0.393951  0.790033   \n",
       "3401         0.251029           0.867769              0.354096  0.779507   \n",
       "3465         0.246377           0.546512              0.477421  0.954013   \n",
       "3499         0.249012           1.146825              0.261010  0.590148   \n",
       "3576         0.241554           0.639594              0.429151  0.403759   \n",
       "\n",
       "      anomtype  \n",
       "user            \n",
       "26           0  \n",
       "199          1  \n",
       "202          2  \n",
       "205          2  \n",
       "231          0  \n",
       "284          2  \n",
       "424          1  \n",
       "459          0  \n",
       "469          2  \n",
       "561          1  \n",
       "667          0  \n",
       "699          2  \n",
       "730          1  \n",
       "786          0  \n",
       "849          1  \n",
       "1045         2  \n",
       "1057         0  \n",
       "1066         2  \n",
       "1151         1  \n",
       "1279         0  \n",
       "1289         2  \n",
       "1307         1  \n",
       "1450         0  \n",
       "1455         1  \n",
       "1561         1  \n",
       "1563         2  \n",
       "1617         0  \n",
       "1640         0  \n",
       "1682         2  \n",
       "1722         1  \n",
       "1818         2  \n",
       "1822         2  \n",
       "1865         2  \n",
       "1903         1  \n",
       "2093         0  \n",
       "2120         0  \n",
       "2163         2  \n",
       "2225         1  \n",
       "2328         1  \n",
       "2390         0  \n",
       "2458         1  \n",
       "2543         0  \n",
       "2548         1  \n",
       "2595         0  \n",
       "2648         2  \n",
       "2767         1  \n",
       "2778         0  \n",
       "2782         2  \n",
       "2800         1  \n",
       "2807         0  \n",
       "2938         2  \n",
       "2980         0  \n",
       "3082         0  \n",
       "3123         1  \n",
       "3153         2  \n",
       "3292         1  \n",
       "3401         1  \n",
       "3465         2  \n",
       "3499         0  \n",
       "3576         2  \n",
       "\n",
       "[60 rows x 328 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "anomtype\n",
       "0    20\n",
       "1    20\n",
       "2    20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe with anomtype: use y_cat instead of y\n",
    "\n",
    "df_cla = engineer_features(X, y_cat)\n",
    "\n",
    "# We convert all column names to strings so it does not throw an error later\n",
    "df_cla.columns = df_cla.columns.astype(str)\n",
    "display(df_cla)\n",
    "df_cla[\"anomtype\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ecca7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_1</th>\n",
       "      <th>svd_2</th>\n",
       "      <th>svd_3</th>\n",
       "      <th>svd_4</th>\n",
       "      <th>svd_5</th>\n",
       "      <th>svd_6</th>\n",
       "      <th>svd_7</th>\n",
       "      <th>svd_8</th>\n",
       "      <th>svd_9</th>\n",
       "      <th>svd_10</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.405226</td>\n",
       "      <td>1.056769</td>\n",
       "      <td>-21.038527</td>\n",
       "      <td>1.067305</td>\n",
       "      <td>26.873600</td>\n",
       "      <td>3.613976</td>\n",
       "      <td>-2.008504</td>\n",
       "      <td>-1.611251</td>\n",
       "      <td>4.041583</td>\n",
       "      <td>-4.170670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439797</td>\n",
       "      <td>0.143443</td>\n",
       "      <td>-0.399362</td>\n",
       "      <td>1.503556</td>\n",
       "      <td>0.151639</td>\n",
       "      <td>0.248333</td>\n",
       "      <td>1296.213115</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.395062</td>\n",
       "      <td>0.254065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>24.701986</td>\n",
       "      <td>4.278532</td>\n",
       "      <td>-6.918118</td>\n",
       "      <td>-14.194389</td>\n",
       "      <td>4.933048</td>\n",
       "      <td>8.060955</td>\n",
       "      <td>6.287393</td>\n",
       "      <td>4.449363</td>\n",
       "      <td>-1.056309</td>\n",
       "      <td>-0.967260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.100368</td>\n",
       "      <td>1.441563</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>0.515833</td>\n",
       "      <td>1429.568720</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>1.219048</td>\n",
       "      <td>0.276117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-30.651479</td>\n",
       "      <td>27.972663</td>\n",
       "      <td>-1.808480</td>\n",
       "      <td>-11.094628</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.539192</td>\n",
       "      <td>-0.447467</td>\n",
       "      <td>-1.427426</td>\n",
       "      <td>-0.590070</td>\n",
       "      <td>0.858616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500438</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.477618</td>\n",
       "      <td>0.745018</td>\n",
       "      <td>0.471963</td>\n",
       "      <td>0.987222</td>\n",
       "      <td>873.476636</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.516432</td>\n",
       "      <td>0.491833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>4.077090</td>\n",
       "      <td>27.859159</td>\n",
       "      <td>17.445291</td>\n",
       "      <td>-5.786915</td>\n",
       "      <td>-12.944545</td>\n",
       "      <td>-2.128206</td>\n",
       "      <td>-3.253641</td>\n",
       "      <td>-6.447440</td>\n",
       "      <td>2.451911</td>\n",
       "      <td>-3.451880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171325</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.170565</td>\n",
       "      <td>1.399122</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>0.651111</td>\n",
       "      <td>1128.376022</td>\n",
       "      <td>0.250681</td>\n",
       "      <td>1.019126</td>\n",
       "      <td>0.279347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>34.053427</td>\n",
       "      <td>3.629981</td>\n",
       "      <td>-14.458684</td>\n",
       "      <td>-5.522036</td>\n",
       "      <td>-10.271650</td>\n",
       "      <td>-4.786838</td>\n",
       "      <td>2.645228</td>\n",
       "      <td>9.534818</td>\n",
       "      <td>-3.295730</td>\n",
       "      <td>0.831030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.691520</td>\n",
       "      <td>0.270950</td>\n",
       "      <td>-0.633656</td>\n",
       "      <td>1.384519</td>\n",
       "      <td>0.265363</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>1383.776536</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>1.263305</td>\n",
       "      <td>0.250882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>19.995440</td>\n",
       "      <td>-3.963606</td>\n",
       "      <td>36.543998</td>\n",
       "      <td>-12.160488</td>\n",
       "      <td>-0.507532</td>\n",
       "      <td>10.580456</td>\n",
       "      <td>-0.204711</td>\n",
       "      <td>1.751987</td>\n",
       "      <td>4.764085</td>\n",
       "      <td>-3.733991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915377</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>0.846178</td>\n",
       "      <td>0.858244</td>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.859167</td>\n",
       "      <td>1249.322870</td>\n",
       "      <td>0.251121</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.511392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>19.515725</td>\n",
       "      <td>1.946307</td>\n",
       "      <td>-11.297455</td>\n",
       "      <td>-12.750778</td>\n",
       "      <td>-1.955778</td>\n",
       "      <td>5.562889</td>\n",
       "      <td>2.733605</td>\n",
       "      <td>-0.980205</td>\n",
       "      <td>-1.747588</td>\n",
       "      <td>-3.314120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097125</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.079770</td>\n",
       "      <td>1.255755</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>1388.605000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.327250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>19.229018</td>\n",
       "      <td>-4.886454</td>\n",
       "      <td>20.073144</td>\n",
       "      <td>-14.276159</td>\n",
       "      <td>7.799430</td>\n",
       "      <td>1.149055</td>\n",
       "      <td>9.521034</td>\n",
       "      <td>-0.200415</td>\n",
       "      <td>-3.550065</td>\n",
       "      <td>5.731454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441914</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>-0.395315</td>\n",
       "      <td>1.475948</td>\n",
       "      <td>0.256522</td>\n",
       "      <td>0.178333</td>\n",
       "      <td>1343.843478</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>1.371179</td>\n",
       "      <td>0.242722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>15.833040</td>\n",
       "      <td>10.900338</td>\n",
       "      <td>17.698568</td>\n",
       "      <td>-12.457373</td>\n",
       "      <td>2.723195</td>\n",
       "      <td>-2.610725</td>\n",
       "      <td>8.489573</td>\n",
       "      <td>-7.575831</td>\n",
       "      <td>-0.392949</td>\n",
       "      <td>-2.492455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082205</td>\n",
       "      <td>0.048327</td>\n",
       "      <td>0.079066</td>\n",
       "      <td>1.315856</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.533056</td>\n",
       "      <td>1279.308550</td>\n",
       "      <td>0.249071</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.311618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>12.384787</td>\n",
       "      <td>-1.564100</td>\n",
       "      <td>-9.387893</td>\n",
       "      <td>-5.248689</td>\n",
       "      <td>2.829056</td>\n",
       "      <td>3.688155</td>\n",
       "      <td>-6.718119</td>\n",
       "      <td>-1.332926</td>\n",
       "      <td>-4.809891</td>\n",
       "      <td>2.177380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171960</td>\n",
       "      <td>0.180488</td>\n",
       "      <td>0.143844</td>\n",
       "      <td>1.446732</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.543611</td>\n",
       "      <td>1214.692683</td>\n",
       "      <td>0.248780</td>\n",
       "      <td>1.171569</td>\n",
       "      <td>0.263534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>35.282715</td>\n",
       "      <td>1.605001</td>\n",
       "      <td>8.985617</td>\n",
       "      <td>3.324267</td>\n",
       "      <td>-25.776958</td>\n",
       "      <td>8.908518</td>\n",
       "      <td>-4.520455</td>\n",
       "      <td>3.688603</td>\n",
       "      <td>1.437349</td>\n",
       "      <td>4.809446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393263</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>-0.362430</td>\n",
       "      <td>1.339007</td>\n",
       "      <td>0.173423</td>\n",
       "      <td>0.137222</td>\n",
       "      <td>1260.198198</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.031603</td>\n",
       "      <td>0.285965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>22.899398</td>\n",
       "      <td>28.790211</td>\n",
       "      <td>-2.853864</td>\n",
       "      <td>0.518014</td>\n",
       "      <td>-5.737385</td>\n",
       "      <td>13.667635</td>\n",
       "      <td>7.112067</td>\n",
       "      <td>7.904376</td>\n",
       "      <td>-0.047028</td>\n",
       "      <td>1.585041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339959</td>\n",
       "      <td>0.104558</td>\n",
       "      <td>0.315420</td>\n",
       "      <td>1.338535</td>\n",
       "      <td>0.246649</td>\n",
       "      <td>0.706111</td>\n",
       "      <td>1237.621984</td>\n",
       "      <td>0.249330</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.297242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>10.109697</td>\n",
       "      <td>-0.534179</td>\n",
       "      <td>9.083594</td>\n",
       "      <td>-9.989619</td>\n",
       "      <td>5.325208</td>\n",
       "      <td>3.840458</td>\n",
       "      <td>-0.516786</td>\n",
       "      <td>-0.355747</td>\n",
       "      <td>-4.024504</td>\n",
       "      <td>4.291348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371623</td>\n",
       "      <td>0.068376</td>\n",
       "      <td>-0.352857</td>\n",
       "      <td>1.234931</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>0.249583</td>\n",
       "      <td>1190.303419</td>\n",
       "      <td>0.252137</td>\n",
       "      <td>0.948498</td>\n",
       "      <td>0.341369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>28.727091</td>\n",
       "      <td>4.460696</td>\n",
       "      <td>-17.737821</td>\n",
       "      <td>-8.568014</td>\n",
       "      <td>2.184781</td>\n",
       "      <td>-4.353866</td>\n",
       "      <td>-7.278990</td>\n",
       "      <td>8.086681</td>\n",
       "      <td>-4.452230</td>\n",
       "      <td>-3.061602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579775</td>\n",
       "      <td>0.174216</td>\n",
       "      <td>-0.529118</td>\n",
       "      <td>1.389305</td>\n",
       "      <td>0.174216</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>1406.651568</td>\n",
       "      <td>0.250871</td>\n",
       "      <td>1.220280</td>\n",
       "      <td>0.269920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>16.708753</td>\n",
       "      <td>-1.445953</td>\n",
       "      <td>4.006640</td>\n",
       "      <td>-10.407775</td>\n",
       "      <td>-1.372284</td>\n",
       "      <td>4.169791</td>\n",
       "      <td>-7.593610</td>\n",
       "      <td>-1.823122</td>\n",
       "      <td>-0.533504</td>\n",
       "      <td>-0.710331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309073</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>1.200819</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>1270.547170</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.359381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>-14.589255</td>\n",
       "      <td>24.841091</td>\n",
       "      <td>0.592879</td>\n",
       "      <td>-12.930398</td>\n",
       "      <td>3.003011</td>\n",
       "      <td>-0.479167</td>\n",
       "      <td>0.592950</td>\n",
       "      <td>1.268763</td>\n",
       "      <td>0.800157</td>\n",
       "      <td>-2.415303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246324</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>0.238899</td>\n",
       "      <td>1.287192</td>\n",
       "      <td>0.387234</td>\n",
       "      <td>0.809722</td>\n",
       "      <td>1072.093617</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>1.004274</td>\n",
       "      <td>0.321865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>50.942770</td>\n",
       "      <td>10.806731</td>\n",
       "      <td>-19.721361</td>\n",
       "      <td>8.340857</td>\n",
       "      <td>-32.353401</td>\n",
       "      <td>0.202926</td>\n",
       "      <td>9.124116</td>\n",
       "      <td>0.243394</td>\n",
       "      <td>-3.626521</td>\n",
       "      <td>2.615711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580949</td>\n",
       "      <td>0.092040</td>\n",
       "      <td>0.540825</td>\n",
       "      <td>1.136690</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>1306.529851</td>\n",
       "      <td>0.251244</td>\n",
       "      <td>0.807980</td>\n",
       "      <td>0.428195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>6.660772</td>\n",
       "      <td>18.774476</td>\n",
       "      <td>2.211635</td>\n",
       "      <td>-13.141572</td>\n",
       "      <td>-12.306636</td>\n",
       "      <td>3.991652</td>\n",
       "      <td>11.318564</td>\n",
       "      <td>2.828238</td>\n",
       "      <td>3.027072</td>\n",
       "      <td>-6.186215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399783</td>\n",
       "      <td>0.012146</td>\n",
       "      <td>0.381093</td>\n",
       "      <td>1.201517</td>\n",
       "      <td>0.214575</td>\n",
       "      <td>0.741111</td>\n",
       "      <td>1248.655870</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.334065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>4.565552</td>\n",
       "      <td>2.001047</td>\n",
       "      <td>-4.991591</td>\n",
       "      <td>-2.488794</td>\n",
       "      <td>3.774549</td>\n",
       "      <td>-0.880130</td>\n",
       "      <td>-1.752852</td>\n",
       "      <td>-1.050395</td>\n",
       "      <td>1.504125</td>\n",
       "      <td>-2.953061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329951</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.292262</td>\n",
       "      <td>1.300817</td>\n",
       "      <td>0.210046</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>1109.118721</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.922018</td>\n",
       "      <td>0.316862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>55.143343</td>\n",
       "      <td>12.673649</td>\n",
       "      <td>21.268550</td>\n",
       "      <td>27.536053</td>\n",
       "      <td>-8.691158</td>\n",
       "      <td>1.590301</td>\n",
       "      <td>1.867865</td>\n",
       "      <td>2.002636</td>\n",
       "      <td>-10.896158</td>\n",
       "      <td>5.159406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164866</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>-0.150875</td>\n",
       "      <td>1.158217</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.300278</td>\n",
       "      <td>1184.531250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.784038</td>\n",
       "      <td>0.419419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>-3.964172</td>\n",
       "      <td>22.893215</td>\n",
       "      <td>-5.001387</td>\n",
       "      <td>-11.151228</td>\n",
       "      <td>4.800311</td>\n",
       "      <td>5.292133</td>\n",
       "      <td>4.868103</td>\n",
       "      <td>5.233376</td>\n",
       "      <td>-2.822094</td>\n",
       "      <td>1.064827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385464</td>\n",
       "      <td>0.021008</td>\n",
       "      <td>0.362125</td>\n",
       "      <td>1.069240</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>1130.827731</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.755274</td>\n",
       "      <td>0.413424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>9.982597</td>\n",
       "      <td>1.233299</td>\n",
       "      <td>4.366212</td>\n",
       "      <td>-4.498651</td>\n",
       "      <td>-0.542160</td>\n",
       "      <td>-3.126854</td>\n",
       "      <td>5.185793</td>\n",
       "      <td>1.389276</td>\n",
       "      <td>-1.535660</td>\n",
       "      <td>-0.518658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145973</td>\n",
       "      <td>0.078067</td>\n",
       "      <td>-0.148607</td>\n",
       "      <td>1.239811</td>\n",
       "      <td>0.078067</td>\n",
       "      <td>0.373472</td>\n",
       "      <td>1155.007435</td>\n",
       "      <td>0.245353</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.346416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>33.229563</td>\n",
       "      <td>5.877482</td>\n",
       "      <td>-19.276149</td>\n",
       "      <td>-9.445955</td>\n",
       "      <td>-9.482312</td>\n",
       "      <td>-5.610207</td>\n",
       "      <td>2.965458</td>\n",
       "      <td>-3.737145</td>\n",
       "      <td>-8.958739</td>\n",
       "      <td>1.904718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277095</td>\n",
       "      <td>0.155709</td>\n",
       "      <td>-0.260829</td>\n",
       "      <td>1.496491</td>\n",
       "      <td>0.186851</td>\n",
       "      <td>0.235556</td>\n",
       "      <td>1422.553633</td>\n",
       "      <td>0.245675</td>\n",
       "      <td>1.253472</td>\n",
       "      <td>0.240550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>-9.410127</td>\n",
       "      <td>1.405786</td>\n",
       "      <td>-4.863681</td>\n",
       "      <td>4.097648</td>\n",
       "      <td>-0.428274</td>\n",
       "      <td>0.271327</td>\n",
       "      <td>6.355352</td>\n",
       "      <td>-0.929348</td>\n",
       "      <td>-0.500602</td>\n",
       "      <td>-0.158288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709848</td>\n",
       "      <td>0.181538</td>\n",
       "      <td>-0.687801</td>\n",
       "      <td>1.301151</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.153889</td>\n",
       "      <td>919.012308</td>\n",
       "      <td>0.249231</td>\n",
       "      <td>1.046296</td>\n",
       "      <td>0.300317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>18.630213</td>\n",
       "      <td>0.309461</td>\n",
       "      <td>4.104553</td>\n",
       "      <td>-12.629557</td>\n",
       "      <td>-7.013169</td>\n",
       "      <td>-0.524197</td>\n",
       "      <td>-2.458145</td>\n",
       "      <td>4.225193</td>\n",
       "      <td>4.917298</td>\n",
       "      <td>-0.953974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049099</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>1.513973</td>\n",
       "      <td>0.182222</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>1292.951111</td>\n",
       "      <td>0.248889</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.258746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>46.562663</td>\n",
       "      <td>8.590602</td>\n",
       "      <td>-5.380619</td>\n",
       "      <td>17.140850</td>\n",
       "      <td>-0.568038</td>\n",
       "      <td>1.429418</td>\n",
       "      <td>4.274856</td>\n",
       "      <td>11.332335</td>\n",
       "      <td>-3.256507</td>\n",
       "      <td>-1.132952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405009</td>\n",
       "      <td>0.034417</td>\n",
       "      <td>-0.379207</td>\n",
       "      <td>1.188743</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>1250.078394</td>\n",
       "      <td>0.248566</td>\n",
       "      <td>0.768199</td>\n",
       "      <td>0.352295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>16.365574</td>\n",
       "      <td>-5.234236</td>\n",
       "      <td>18.390408</td>\n",
       "      <td>-18.399680</td>\n",
       "      <td>-0.653204</td>\n",
       "      <td>-1.960561</td>\n",
       "      <td>-5.301496</td>\n",
       "      <td>-8.029610</td>\n",
       "      <td>-0.166289</td>\n",
       "      <td>-5.879510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613316</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>-0.554063</td>\n",
       "      <td>1.378308</td>\n",
       "      <td>0.200893</td>\n",
       "      <td>0.083889</td>\n",
       "      <td>1359.825893</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228700</td>\n",
       "      <td>0.253866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>48.347671</td>\n",
       "      <td>6.326245</td>\n",
       "      <td>26.561090</td>\n",
       "      <td>3.600284</td>\n",
       "      <td>-18.627871</td>\n",
       "      <td>4.337232</td>\n",
       "      <td>-23.631518</td>\n",
       "      <td>0.098326</td>\n",
       "      <td>-5.827032</td>\n",
       "      <td>3.219304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106968</td>\n",
       "      <td>0.198276</td>\n",
       "      <td>0.101233</td>\n",
       "      <td>1.553445</td>\n",
       "      <td>0.273707</td>\n",
       "      <td>0.433056</td>\n",
       "      <td>1268.467672</td>\n",
       "      <td>0.247845</td>\n",
       "      <td>1.382289</td>\n",
       "      <td>0.219986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>-8.846367</td>\n",
       "      <td>22.965476</td>\n",
       "      <td>-9.353197</td>\n",
       "      <td>-9.323590</td>\n",
       "      <td>-1.110193</td>\n",
       "      <td>-6.428081</td>\n",
       "      <td>-0.770292</td>\n",
       "      <td>-3.809209</td>\n",
       "      <td>-0.839959</td>\n",
       "      <td>1.847927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267113</td>\n",
       "      <td>0.069388</td>\n",
       "      <td>0.252205</td>\n",
       "      <td>1.291371</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>1076.918367</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.327514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>32.194061</td>\n",
       "      <td>4.328843</td>\n",
       "      <td>-12.861302</td>\n",
       "      <td>-8.997731</td>\n",
       "      <td>-0.403383</td>\n",
       "      <td>5.000128</td>\n",
       "      <td>3.642669</td>\n",
       "      <td>-3.721963</td>\n",
       "      <td>-6.917732</td>\n",
       "      <td>0.772333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>-0.000830</td>\n",
       "      <td>1.565127</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.401944</td>\n",
       "      <td>1381.936567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228464</td>\n",
       "      <td>0.229923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>-11.240644</td>\n",
       "      <td>38.191345</td>\n",
       "      <td>2.053651</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>-3.187149</td>\n",
       "      <td>0.353069</td>\n",
       "      <td>-3.564333</td>\n",
       "      <td>-4.061618</td>\n",
       "      <td>2.608191</td>\n",
       "      <td>-3.025811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151092</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.144829</td>\n",
       "      <td>1.375909</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.735556</td>\n",
       "      <td>1036.200000</td>\n",
       "      <td>0.243678</td>\n",
       "      <td>0.907834</td>\n",
       "      <td>0.274652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>-22.530234</td>\n",
       "      <td>31.830443</td>\n",
       "      <td>-5.155541</td>\n",
       "      <td>-6.073602</td>\n",
       "      <td>2.675920</td>\n",
       "      <td>1.076088</td>\n",
       "      <td>-2.166935</td>\n",
       "      <td>-1.123116</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>-0.718346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357938</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.342201</td>\n",
       "      <td>1.073918</td>\n",
       "      <td>0.371025</td>\n",
       "      <td>0.890833</td>\n",
       "      <td>960.448763</td>\n",
       "      <td>0.250883</td>\n",
       "      <td>0.687943</td>\n",
       "      <td>0.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>-25.681521</td>\n",
       "      <td>25.066416</td>\n",
       "      <td>-0.517062</td>\n",
       "      <td>-11.098346</td>\n",
       "      <td>0.320960</td>\n",
       "      <td>-0.722261</td>\n",
       "      <td>-0.316471</td>\n",
       "      <td>-1.407688</td>\n",
       "      <td>-1.067839</td>\n",
       "      <td>2.649285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388896</td>\n",
       "      <td>1.000470</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.928611</td>\n",
       "      <td>929.228070</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.638767</td>\n",
       "      <td>0.416551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>12.691205</td>\n",
       "      <td>1.361096</td>\n",
       "      <td>1.730498</td>\n",
       "      <td>-4.063565</td>\n",
       "      <td>2.698011</td>\n",
       "      <td>-0.263857</td>\n",
       "      <td>-2.108461</td>\n",
       "      <td>0.051035</td>\n",
       "      <td>-0.066095</td>\n",
       "      <td>0.649859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130152</td>\n",
       "      <td>0.049808</td>\n",
       "      <td>-0.131732</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.388194</td>\n",
       "      <td>1176.053640</td>\n",
       "      <td>0.249042</td>\n",
       "      <td>0.776923</td>\n",
       "      <td>0.375024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>26.208748</td>\n",
       "      <td>1.223819</td>\n",
       "      <td>-0.750656</td>\n",
       "      <td>-14.497578</td>\n",
       "      <td>21.059123</td>\n",
       "      <td>1.539668</td>\n",
       "      <td>3.829611</td>\n",
       "      <td>-2.383363</td>\n",
       "      <td>5.219389</td>\n",
       "      <td>4.505694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534002</td>\n",
       "      <td>0.313636</td>\n",
       "      <td>-0.490975</td>\n",
       "      <td>1.552178</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.184722</td>\n",
       "      <td>1452.404545</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.534247</td>\n",
       "      <td>0.227603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>25.898912</td>\n",
       "      <td>-0.875074</td>\n",
       "      <td>17.089111</td>\n",
       "      <td>-16.163478</td>\n",
       "      <td>-3.181579</td>\n",
       "      <td>-3.717163</td>\n",
       "      <td>-12.909274</td>\n",
       "      <td>0.747702</td>\n",
       "      <td>7.321352</td>\n",
       "      <td>-2.666413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376031</td>\n",
       "      <td>0.161049</td>\n",
       "      <td>-0.345681</td>\n",
       "      <td>1.414951</td>\n",
       "      <td>0.164794</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>1379.501873</td>\n",
       "      <td>0.250936</td>\n",
       "      <td>1.135338</td>\n",
       "      <td>0.263982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>5.944021</td>\n",
       "      <td>10.971361</td>\n",
       "      <td>-7.028612</td>\n",
       "      <td>-3.729516</td>\n",
       "      <td>16.252878</td>\n",
       "      <td>4.574362</td>\n",
       "      <td>-5.971668</td>\n",
       "      <td>6.157987</td>\n",
       "      <td>4.949524</td>\n",
       "      <td>5.259026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069949</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.066686</td>\n",
       "      <td>1.431018</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.595556</td>\n",
       "      <td>1206.288136</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>1.068085</td>\n",
       "      <td>0.281313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>26.131340</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>23.011535</td>\n",
       "      <td>-12.728496</td>\n",
       "      <td>8.075412</td>\n",
       "      <td>2.314124</td>\n",
       "      <td>10.166097</td>\n",
       "      <td>-1.258650</td>\n",
       "      <td>-5.445485</td>\n",
       "      <td>4.485016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555058</td>\n",
       "      <td>0.078261</td>\n",
       "      <td>0.507646</td>\n",
       "      <td>1.174809</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>1314.291304</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>0.359282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>16.477927</td>\n",
       "      <td>17.254603</td>\n",
       "      <td>15.624358</td>\n",
       "      <td>22.925548</td>\n",
       "      <td>-3.732207</td>\n",
       "      <td>-5.545847</td>\n",
       "      <td>-0.287058</td>\n",
       "      <td>2.493493</td>\n",
       "      <td>-2.856931</td>\n",
       "      <td>-1.472151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.873794</td>\n",
       "      <td>0.403008</td>\n",
       "      <td>-0.829380</td>\n",
       "      <td>1.724176</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.034167</td>\n",
       "      <td>1052.803008</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>1.692771</td>\n",
       "      <td>0.187160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>30.734405</td>\n",
       "      <td>0.195722</td>\n",
       "      <td>12.751227</td>\n",
       "      <td>-7.813799</td>\n",
       "      <td>6.543426</td>\n",
       "      <td>-2.407274</td>\n",
       "      <td>-2.644401</td>\n",
       "      <td>1.560591</td>\n",
       "      <td>-2.331377</td>\n",
       "      <td>13.273872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286722</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.257941</td>\n",
       "      <td>1.568366</td>\n",
       "      <td>0.281967</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>1335.285246</td>\n",
       "      <td>0.249180</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.216469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>11.901789</td>\n",
       "      <td>2.450261</td>\n",
       "      <td>-10.521368</td>\n",
       "      <td>-6.382197</td>\n",
       "      <td>-5.168251</td>\n",
       "      <td>-3.993819</td>\n",
       "      <td>-6.422926</td>\n",
       "      <td>3.484094</td>\n",
       "      <td>-3.876889</td>\n",
       "      <td>4.011051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410873</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>0.379774</td>\n",
       "      <td>1.355320</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>0.668611</td>\n",
       "      <td>1236.621359</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.287445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>28.762754</td>\n",
       "      <td>-1.056746</td>\n",
       "      <td>-3.490334</td>\n",
       "      <td>-7.397567</td>\n",
       "      <td>-6.997795</td>\n",
       "      <td>20.513098</td>\n",
       "      <td>6.709869</td>\n",
       "      <td>1.375369</td>\n",
       "      <td>-1.890947</td>\n",
       "      <td>-4.380331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569337</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>-0.519832</td>\n",
       "      <td>1.414330</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.056944</td>\n",
       "      <td>1375.405844</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>1.211726</td>\n",
       "      <td>0.250042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>0.742950</td>\n",
       "      <td>-2.627058</td>\n",
       "      <td>-3.835256</td>\n",
       "      <td>-6.145804</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>-3.033757</td>\n",
       "      <td>-0.100329</td>\n",
       "      <td>-0.043938</td>\n",
       "      <td>-1.506061</td>\n",
       "      <td>-1.085429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035166</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>1.577346</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.459306</td>\n",
       "      <td>1072.140000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.567839</td>\n",
       "      <td>0.235550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>22.258937</td>\n",
       "      <td>-0.865007</td>\n",
       "      <td>15.276752</td>\n",
       "      <td>-17.701465</td>\n",
       "      <td>14.155350</td>\n",
       "      <td>-4.693674</td>\n",
       "      <td>-2.443168</td>\n",
       "      <td>-2.805695</td>\n",
       "      <td>-2.804628</td>\n",
       "      <td>-2.417563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270898</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>-0.244673</td>\n",
       "      <td>1.444424</td>\n",
       "      <td>0.187793</td>\n",
       "      <td>0.311389</td>\n",
       "      <td>1383.929577</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>1.179245</td>\n",
       "      <td>0.269149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>25.401560</td>\n",
       "      <td>8.282808</td>\n",
       "      <td>-31.277136</td>\n",
       "      <td>13.145835</td>\n",
       "      <td>16.997314</td>\n",
       "      <td>-7.945109</td>\n",
       "      <td>-8.147648</td>\n",
       "      <td>-8.896687</td>\n",
       "      <td>-5.843419</td>\n",
       "      <td>-0.626392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232083</td>\n",
       "      <td>0.196774</td>\n",
       "      <td>0.206216</td>\n",
       "      <td>1.502625</td>\n",
       "      <td>0.374194</td>\n",
       "      <td>0.606944</td>\n",
       "      <td>1226.735484</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>1.291262</td>\n",
       "      <td>0.242310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>13.706690</td>\n",
       "      <td>7.791517</td>\n",
       "      <td>7.306411</td>\n",
       "      <td>10.541593</td>\n",
       "      <td>-9.516593</td>\n",
       "      <td>3.309520</td>\n",
       "      <td>-1.542335</td>\n",
       "      <td>4.351730</td>\n",
       "      <td>-3.554876</td>\n",
       "      <td>-5.031538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099858</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>-0.099977</td>\n",
       "      <td>0.877015</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.395417</td>\n",
       "      <td>1083.274038</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>0.573494</td>\n",
       "      <td>0.482329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>26.912932</td>\n",
       "      <td>-0.454641</td>\n",
       "      <td>14.015744</td>\n",
       "      <td>-9.998340</td>\n",
       "      <td>-8.905399</td>\n",
       "      <td>8.952896</td>\n",
       "      <td>-11.056320</td>\n",
       "      <td>6.633441</td>\n",
       "      <td>-5.127221</td>\n",
       "      <td>-1.530622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227136</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>-0.206427</td>\n",
       "      <td>0.998815</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>1320.083893</td>\n",
       "      <td>0.251678</td>\n",
       "      <td>0.690236</td>\n",
       "      <td>0.473807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>-30.570426</td>\n",
       "      <td>24.732419</td>\n",
       "      <td>-2.079454</td>\n",
       "      <td>-11.137881</td>\n",
       "      <td>1.518234</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.446444</td>\n",
       "      <td>-1.345274</td>\n",
       "      <td>1.313954</td>\n",
       "      <td>0.345504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559664</td>\n",
       "      <td>0.747592</td>\n",
       "      <td>0.563981</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>856.459716</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.496193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-5.828762</td>\n",
       "      <td>2.545959</td>\n",
       "      <td>-0.645852</td>\n",
       "      <td>1.055244</td>\n",
       "      <td>-12.794977</td>\n",
       "      <td>-0.612679</td>\n",
       "      <td>-6.507652</td>\n",
       "      <td>0.730545</td>\n",
       "      <td>-2.138035</td>\n",
       "      <td>-1.641358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>-0.006055</td>\n",
       "      <td>1.310332</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.496944</td>\n",
       "      <td>986.786713</td>\n",
       "      <td>0.251748</td>\n",
       "      <td>1.017544</td>\n",
       "      <td>0.306739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>22.675271</td>\n",
       "      <td>-1.925411</td>\n",
       "      <td>13.604115</td>\n",
       "      <td>-20.048106</td>\n",
       "      <td>-3.945546</td>\n",
       "      <td>2.531374</td>\n",
       "      <td>5.503931</td>\n",
       "      <td>-0.571000</td>\n",
       "      <td>0.317331</td>\n",
       "      <td>7.455540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079787</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>-0.073768</td>\n",
       "      <td>1.407055</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>1436.803828</td>\n",
       "      <td>0.248804</td>\n",
       "      <td>1.139423</td>\n",
       "      <td>0.290264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>-28.685449</td>\n",
       "      <td>28.085449</td>\n",
       "      <td>2.383516</td>\n",
       "      <td>-6.788596</td>\n",
       "      <td>2.347656</td>\n",
       "      <td>0.579053</td>\n",
       "      <td>0.203732</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-0.268027</td>\n",
       "      <td>-1.756875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416477</td>\n",
       "      <td>0.031873</td>\n",
       "      <td>0.403970</td>\n",
       "      <td>1.070504</td>\n",
       "      <td>0.466135</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>901.521912</td>\n",
       "      <td>0.239044</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.400232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>15.635537</td>\n",
       "      <td>-4.674515</td>\n",
       "      <td>-21.750005</td>\n",
       "      <td>-6.050642</td>\n",
       "      <td>-6.309088</td>\n",
       "      <td>-6.900510</td>\n",
       "      <td>-12.422318</td>\n",
       "      <td>-2.050268</td>\n",
       "      <td>-4.072678</td>\n",
       "      <td>3.729193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619993</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>-0.569125</td>\n",
       "      <td>1.468102</td>\n",
       "      <td>0.294372</td>\n",
       "      <td>0.048333</td>\n",
       "      <td>1323.142857</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>1.339130</td>\n",
       "      <td>0.239145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>31.667955</td>\n",
       "      <td>3.968138</td>\n",
       "      <td>-15.258337</td>\n",
       "      <td>-11.228208</td>\n",
       "      <td>-4.849364</td>\n",
       "      <td>-6.858703</td>\n",
       "      <td>2.468430</td>\n",
       "      <td>-1.729865</td>\n",
       "      <td>-0.044167</td>\n",
       "      <td>0.559501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594845</td>\n",
       "      <td>0.214789</td>\n",
       "      <td>-0.530439</td>\n",
       "      <td>1.379800</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>1479.183099</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.190813</td>\n",
       "      <td>0.253323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>5.693312</td>\n",
       "      <td>3.724910</td>\n",
       "      <td>0.837205</td>\n",
       "      <td>-2.108318</td>\n",
       "      <td>3.092695</td>\n",
       "      <td>0.562513</td>\n",
       "      <td>0.972588</td>\n",
       "      <td>1.002036</td>\n",
       "      <td>4.446282</td>\n",
       "      <td>0.449313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359014</td>\n",
       "      <td>0.110204</td>\n",
       "      <td>0.322635</td>\n",
       "      <td>1.302367</td>\n",
       "      <td>0.220408</td>\n",
       "      <td>0.696111</td>\n",
       "      <td>1116.616327</td>\n",
       "      <td>0.236735</td>\n",
       "      <td>0.897541</td>\n",
       "      <td>0.301858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>-3.423319</td>\n",
       "      <td>17.990912</td>\n",
       "      <td>-9.466775</td>\n",
       "      <td>-8.092630</td>\n",
       "      <td>0.987898</td>\n",
       "      <td>3.026215</td>\n",
       "      <td>-4.959364</td>\n",
       "      <td>-1.497612</td>\n",
       "      <td>7.117767</td>\n",
       "      <td>1.759992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004212</td>\n",
       "      <td>0.109312</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>1.537289</td>\n",
       "      <td>0.336032</td>\n",
       "      <td>0.601389</td>\n",
       "      <td>1153.404858</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>1.264228</td>\n",
       "      <td>0.248472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>-2.689915</td>\n",
       "      <td>5.327463</td>\n",
       "      <td>0.237086</td>\n",
       "      <td>8.228715</td>\n",
       "      <td>-0.121224</td>\n",
       "      <td>0.351049</td>\n",
       "      <td>-2.448782</td>\n",
       "      <td>-0.694674</td>\n",
       "      <td>-3.174310</td>\n",
       "      <td>-1.754148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115902</td>\n",
       "      <td>0.055718</td>\n",
       "      <td>-0.129394</td>\n",
       "      <td>1.087075</td>\n",
       "      <td>0.029326</td>\n",
       "      <td>0.446528</td>\n",
       "      <td>976.988270</td>\n",
       "      <td>0.249267</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.393951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>2.436708</td>\n",
       "      <td>2.565077</td>\n",
       "      <td>2.710996</td>\n",
       "      <td>2.542875</td>\n",
       "      <td>-0.756122</td>\n",
       "      <td>-1.675883</td>\n",
       "      <td>0.576098</td>\n",
       "      <td>-1.094740</td>\n",
       "      <td>1.337314</td>\n",
       "      <td>-3.562950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703093</td>\n",
       "      <td>0.164609</td>\n",
       "      <td>0.647279</td>\n",
       "      <td>1.152592</td>\n",
       "      <td>0.329218</td>\n",
       "      <td>0.849167</td>\n",
       "      <td>1036.823045</td>\n",
       "      <td>0.251029</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.354096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>-36.729628</td>\n",
       "      <td>36.734406</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-7.837553</td>\n",
       "      <td>-0.683447</td>\n",
       "      <td>1.128580</td>\n",
       "      <td>0.061302</td>\n",
       "      <td>-1.135128</td>\n",
       "      <td>0.110130</td>\n",
       "      <td>0.619779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473018</td>\n",
       "      <td>0.800311</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.987778</td>\n",
       "      <td>851.797101</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.477421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>15.530337</td>\n",
       "      <td>-4.464295</td>\n",
       "      <td>12.255507</td>\n",
       "      <td>-9.370663</td>\n",
       "      <td>-5.453862</td>\n",
       "      <td>1.494034</td>\n",
       "      <td>7.607356</td>\n",
       "      <td>-7.700408</td>\n",
       "      <td>-5.541105</td>\n",
       "      <td>9.505316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345386</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>-0.316729</td>\n",
       "      <td>1.405600</td>\n",
       "      <td>0.162055</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>1246.418972</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>1.146825</td>\n",
       "      <td>0.261010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>11.747589</td>\n",
       "      <td>45.567337</td>\n",
       "      <td>6.371927</td>\n",
       "      <td>23.781958</td>\n",
       "      <td>9.076805</td>\n",
       "      <td>-1.852480</td>\n",
       "      <td>-0.988570</td>\n",
       "      <td>1.938058</td>\n",
       "      <td>-2.953191</td>\n",
       "      <td>-0.210857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449579</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>0.416527</td>\n",
       "      <td>1.057506</td>\n",
       "      <td>0.228041</td>\n",
       "      <td>0.816944</td>\n",
       "      <td>1059.508446</td>\n",
       "      <td>0.241554</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.429151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          svd_1      svd_2      svd_3      svd_4      svd_5      svd_6  \\\n",
       "user                                                                     \n",
       "26    19.405226   1.056769 -21.038527   1.067305  26.873600   3.613976   \n",
       "199   24.701986   4.278532  -6.918118 -14.194389   4.933048   8.060955   \n",
       "202  -30.651479  27.972663  -1.808480 -11.094628   0.104153   0.539192   \n",
       "205    4.077090  27.859159  17.445291  -5.786915 -12.944545  -2.128206   \n",
       "231   34.053427   3.629981 -14.458684  -5.522036 -10.271650  -4.786838   \n",
       "284   19.995440  -3.963606  36.543998 -12.160488  -0.507532  10.580456   \n",
       "424   19.515725   1.946307 -11.297455 -12.750778  -1.955778   5.562889   \n",
       "459   19.229018  -4.886454  20.073144 -14.276159   7.799430   1.149055   \n",
       "469   15.833040  10.900338  17.698568 -12.457373   2.723195  -2.610725   \n",
       "561   12.384787  -1.564100  -9.387893  -5.248689   2.829056   3.688155   \n",
       "667   35.282715   1.605001   8.985617   3.324267 -25.776958   8.908518   \n",
       "699   22.899398  28.790211  -2.853864   0.518014  -5.737385  13.667635   \n",
       "730   10.109697  -0.534179   9.083594  -9.989619   5.325208   3.840458   \n",
       "786   28.727091   4.460696 -17.737821  -8.568014   2.184781  -4.353866   \n",
       "849   16.708753  -1.445953   4.006640 -10.407775  -1.372284   4.169791   \n",
       "1045 -14.589255  24.841091   0.592879 -12.930398   3.003011  -0.479167   \n",
       "1057  50.942770  10.806731 -19.721361   8.340857 -32.353401   0.202926   \n",
       "1066   6.660772  18.774476   2.211635 -13.141572 -12.306636   3.991652   \n",
       "1151   4.565552   2.001047  -4.991591  -2.488794   3.774549  -0.880130   \n",
       "1279  55.143343  12.673649  21.268550  27.536053  -8.691158   1.590301   \n",
       "1289  -3.964172  22.893215  -5.001387 -11.151228   4.800311   5.292133   \n",
       "1307   9.982597   1.233299   4.366212  -4.498651  -0.542160  -3.126854   \n",
       "1450  33.229563   5.877482 -19.276149  -9.445955  -9.482312  -5.610207   \n",
       "1455  -9.410127   1.405786  -4.863681   4.097648  -0.428274   0.271327   \n",
       "1561  18.630213   0.309461   4.104553 -12.629557  -7.013169  -0.524197   \n",
       "1563  46.562663   8.590602  -5.380619  17.140850  -0.568038   1.429418   \n",
       "1617  16.365574  -5.234236  18.390408 -18.399680  -0.653204  -1.960561   \n",
       "1640  48.347671   6.326245  26.561090   3.600284 -18.627871   4.337232   \n",
       "1682  -8.846367  22.965476  -9.353197  -9.323590  -1.110193  -6.428081   \n",
       "1722  32.194061   4.328843 -12.861302  -8.997731  -0.403383   5.000128   \n",
       "1818 -11.240644  38.191345   2.053651   0.160269  -3.187149   0.353069   \n",
       "1822 -22.530234  31.830443  -5.155541  -6.073602   2.675920   1.076088   \n",
       "1865 -25.681521  25.066416  -0.517062 -11.098346   0.320960  -0.722261   \n",
       "1903  12.691205   1.361096   1.730498  -4.063565   2.698011  -0.263857   \n",
       "2093  26.208748   1.223819  -0.750656 -14.497578  21.059123   1.539668   \n",
       "2120  25.898912  -0.875074  17.089111 -16.163478  -3.181579  -3.717163   \n",
       "2163   5.944021  10.971361  -7.028612  -3.729516  16.252878   4.574362   \n",
       "2225  26.131340   0.000671  23.011535 -12.728496   8.075412   2.314124   \n",
       "2328  16.477927  17.254603  15.624358  22.925548  -3.732207  -5.545847   \n",
       "2390  30.734405   0.195722  12.751227  -7.813799   6.543426  -2.407274   \n",
       "2458  11.901789   2.450261 -10.521368  -6.382197  -5.168251  -3.993819   \n",
       "2543  28.762754  -1.056746  -3.490334  -7.397567  -6.997795  20.513098   \n",
       "2548   0.742950  -2.627058  -3.835256  -6.145804   0.009457  -3.033757   \n",
       "2595  22.258937  -0.865007  15.276752 -17.701465  14.155350  -4.693674   \n",
       "2648  25.401560   8.282808 -31.277136  13.145835  16.997314  -7.945109   \n",
       "2767  13.706690   7.791517   7.306411  10.541593  -9.516593   3.309520   \n",
       "2778  26.912932  -0.454641  14.015744  -9.998340  -8.905399   8.952896   \n",
       "2782 -30.570426  24.732419  -2.079454 -11.137881   1.518234   0.011092   \n",
       "2800  -5.828762   2.545959  -0.645852   1.055244 -12.794977  -0.612679   \n",
       "2807  22.675271  -1.925411  13.604115 -20.048106  -3.945546   2.531374   \n",
       "2938 -28.685449  28.085449   2.383516  -6.788596   2.347656   0.579053   \n",
       "2980  15.635537  -4.674515 -21.750005  -6.050642  -6.309088  -6.900510   \n",
       "3082  31.667955   3.968138 -15.258337 -11.228208  -4.849364  -6.858703   \n",
       "3123   5.693312   3.724910   0.837205  -2.108318   3.092695   0.562513   \n",
       "3153  -3.423319  17.990912  -9.466775  -8.092630   0.987898   3.026215   \n",
       "3292  -2.689915   5.327463   0.237086   8.228715  -0.121224   0.351049   \n",
       "3401   2.436708   2.565077   2.710996   2.542875  -0.756122  -1.675883   \n",
       "3465 -36.729628  36.734406  -0.051635  -7.837553  -0.683447   1.128580   \n",
       "3499  15.530337  -4.464295  12.255507  -9.370663  -5.453862   1.494034   \n",
       "3576  11.747589  45.567337   6.371927  23.781958   9.076805  -1.852480   \n",
       "\n",
       "          svd_7      svd_8      svd_9     svd_10  ...  user_bias  \\\n",
       "user                                              ...              \n",
       "26    -2.008504  -1.611251   4.041583  -4.170670  ...  -0.439797   \n",
       "199    6.287393   4.449363  -1.056309  -0.967260  ...   0.109502   \n",
       "202   -0.447467  -1.427426  -0.590070   0.858616  ...   0.500438   \n",
       "205   -3.253641  -6.447440   2.451911  -3.451880  ...   0.171325   \n",
       "231    2.645228   9.534818  -3.295730   0.831030  ...  -0.691520   \n",
       "284   -0.204711   1.751987   4.764085  -3.733991  ...   0.915377   \n",
       "424    2.733605  -0.980205  -1.747588  -3.314120  ...   0.097125   \n",
       "459    9.521034  -0.200415  -3.550065   5.731454  ...  -0.441914   \n",
       "469    8.489573  -7.575831  -0.392949  -2.492455  ...   0.082205   \n",
       "561   -6.718119  -1.332926  -4.809891   2.177380  ...   0.171960   \n",
       "667   -4.520455   3.688603   1.437349   4.809446  ...  -0.393263   \n",
       "699    7.112067   7.904376  -0.047028   1.585041  ...   0.339959   \n",
       "730   -0.516786  -0.355747  -4.024504   4.291348  ...  -0.371623   \n",
       "786   -7.278990   8.086681  -4.452230  -3.061602  ...  -0.579775   \n",
       "849   -7.593610  -1.823122  -0.533504  -0.710331  ...   0.309073   \n",
       "1045   0.592950   1.268763   0.800157  -2.415303  ...   0.246324   \n",
       "1057   9.124116   0.243394  -3.626521   2.615711  ...   0.580949   \n",
       "1066  11.318564   2.828238   3.027072  -6.186215  ...   0.399783   \n",
       "1151  -1.752852  -1.050395   1.504125  -2.953061  ...   0.329951   \n",
       "1279   1.867865   2.002636 -10.896158   5.159406  ...  -0.164866   \n",
       "1289   4.868103   5.233376  -2.822094   1.064827  ...   0.385464   \n",
       "1307   5.185793   1.389276  -1.535660  -0.518658  ...  -0.145973   \n",
       "1450   2.965458  -3.737145  -8.958739   1.904718  ...  -0.277095   \n",
       "1455   6.355352  -0.929348  -0.500602  -0.158288  ...  -0.709848   \n",
       "1561  -2.458145   4.225193   4.917298  -0.953974  ...   0.049099   \n",
       "1563   4.274856  11.332335  -3.256507  -1.132952  ...  -0.405009   \n",
       "1617  -5.301496  -8.029610  -0.166289  -5.879510  ...  -0.613316   \n",
       "1640 -23.631518   0.098326  -5.827032   3.219304  ...   0.106968   \n",
       "1682  -0.770292  -3.809209  -0.839959   1.847927  ...   0.267113   \n",
       "1722   3.642669  -3.721963  -6.917732   0.772333  ...   0.007276   \n",
       "1818  -3.564333  -4.061618   2.608191  -3.025811  ...   0.151092   \n",
       "1822  -2.166935  -1.123116   0.019499  -0.718346  ...   0.357938   \n",
       "1865  -0.316471  -1.407688  -1.067839   2.649285  ...   0.400242   \n",
       "1903  -2.108461   0.051035  -0.066095   0.649859  ...  -0.130152   \n",
       "2093   3.829611  -2.383363   5.219389   4.505694  ...  -0.534002   \n",
       "2120 -12.909274   0.747702   7.321352  -2.666413  ...  -0.376031   \n",
       "2163  -5.971668   6.157987   4.949524   5.259026  ...   0.069949   \n",
       "2225  10.166097  -1.258650  -5.445485   4.485016  ...   0.555058   \n",
       "2328  -0.287058   2.493493  -2.856931  -1.472151  ...  -0.873794   \n",
       "2390  -2.644401   1.560591  -2.331377  13.273872  ...  -0.286722   \n",
       "2458  -6.422926   3.484094  -3.876889   4.011051  ...   0.410873   \n",
       "2543   6.709869   1.375369  -1.890947  -4.380331  ...  -0.569337   \n",
       "2548  -0.100329  -0.043938  -1.506061  -1.085429  ...  -0.035166   \n",
       "2595  -2.443168  -2.805695  -2.804628  -2.417563  ...  -0.270898   \n",
       "2648  -8.147648  -8.896687  -5.843419  -0.626392  ...   0.232083   \n",
       "2767  -1.542335   4.351730  -3.554876  -5.031538  ...  -0.099858   \n",
       "2778 -11.056320   6.633441  -5.127221  -1.530622  ...  -0.227136   \n",
       "2782   0.446444  -1.345274   1.313954   0.345504  ...   0.570789   \n",
       "2800  -6.507652   0.730545  -2.138035  -1.641358  ...   0.006366   \n",
       "2807   5.503931  -0.571000   0.317331   7.455540  ...  -0.079787   \n",
       "2938   0.203732  -0.804791  -0.268027  -1.756875  ...   0.416477   \n",
       "2980 -12.422318  -2.050268  -4.072678   3.729193  ...  -0.619993   \n",
       "3082   2.468430  -1.729865  -0.044167   0.559501  ...  -0.594845   \n",
       "3123   0.972588   1.002036   4.446282   0.449313  ...   0.359014   \n",
       "3153  -4.959364  -1.497612   7.117767   1.759992  ...  -0.004212   \n",
       "3292  -2.448782  -0.694674  -3.174310  -1.754148  ...  -0.115902   \n",
       "3401   0.576098  -1.094740   1.337314  -3.562950  ...   0.703093   \n",
       "3465   0.061302  -1.135128   0.110130   0.619779  ...   0.486003   \n",
       "3499   7.607356  -7.700408  -5.541105   9.505316  ...  -0.345386   \n",
       "3576  -0.988570   1.938058  -2.953191  -0.210857  ...   0.449579   \n",
       "\n",
       "      outlier_frac  mean_item_alignment  rating_entropy  extreme_ratio  \\\n",
       "user                                                                     \n",
       "26        0.143443            -0.399362        1.503556       0.151639   \n",
       "199       0.123223             0.100368        1.441563       0.189573   \n",
       "202       0.004673             0.477618        0.745018       0.471963   \n",
       "205       0.059946             0.170565        1.399122       0.234332   \n",
       "231       0.270950            -0.633656        1.384519       0.265363   \n",
       "284       0.152466             0.846178        0.858244       0.219731   \n",
       "424       0.060000             0.079770        1.255755       0.075000   \n",
       "459       0.239130            -0.395315        1.475948       0.256522   \n",
       "469       0.048327             0.079066        1.315856       0.148699   \n",
       "561       0.180488             0.143844        1.446732       0.214634   \n",
       "667       0.180180            -0.362430        1.339007       0.173423   \n",
       "699       0.104558             0.315420        1.338535       0.246649   \n",
       "730       0.068376            -0.352857        1.234931       0.047009   \n",
       "786       0.174216            -0.529118        1.389305       0.174216   \n",
       "849       0.056604             0.282600        1.200819       0.099057   \n",
       "1045      0.025532             0.238899        1.287192       0.387234   \n",
       "1057      0.092040             0.540825        1.136690       0.119403   \n",
       "1066      0.012146             0.381093        1.201517       0.214575   \n",
       "1151      0.105023             0.292262        1.300817       0.210046   \n",
       "1279      0.070312            -0.150875        1.158217       0.070312   \n",
       "1289      0.021008             0.362125        1.069240       0.256303   \n",
       "1307      0.078067            -0.148607        1.239811       0.078067   \n",
       "1450      0.155709            -0.260829        1.496491       0.186851   \n",
       "1455      0.181538            -0.687801        1.301151       0.092308   \n",
       "1561      0.160000             0.040100        1.513973       0.182222   \n",
       "1563      0.034417            -0.379207        1.188743       0.053537   \n",
       "1617      0.223214            -0.554063        1.378308       0.200893   \n",
       "1640      0.198276             0.101233        1.553445       0.273707   \n",
       "1682      0.069388             0.252205        1.291371       0.424490   \n",
       "1722      0.123134            -0.000830        1.565127       0.223881   \n",
       "1818      0.027586             0.144829        1.375909       0.310345   \n",
       "1822      0.014134             0.342201        1.073918       0.371025   \n",
       "1865      0.000000             0.388896        1.000470       0.412281   \n",
       "1903      0.049808            -0.131732        1.154581       0.053640   \n",
       "2093      0.313636            -0.490975        1.552178       0.345455   \n",
       "2120      0.161049            -0.345681        1.414951       0.164794   \n",
       "2163      0.080508             0.066686        1.431018       0.211864   \n",
       "2225      0.078261             0.507646        1.174809       0.182609   \n",
       "2328      0.403008            -0.829380        1.724176       0.345865   \n",
       "2390      0.200000            -0.257941        1.568366       0.281967   \n",
       "2458      0.165049             0.379774        1.355320       0.242718   \n",
       "2543      0.214286            -0.519832        1.414330       0.211039   \n",
       "2548      0.215000            -0.049011        1.577346       0.230000   \n",
       "2595      0.154930            -0.244673        1.444424       0.187793   \n",
       "2648      0.196774             0.206216        1.502625       0.374194   \n",
       "2767      0.028846            -0.099977        0.877015       0.002404   \n",
       "2778      0.063758            -0.206427        0.998815       0.063758   \n",
       "2782      0.000000             0.559664        0.747592       0.563981   \n",
       "2800      0.122378            -0.006055        1.310332       0.115385   \n",
       "2807      0.110048            -0.073768        1.407055       0.114833   \n",
       "2938      0.031873             0.403970        1.070504       0.466135   \n",
       "2980      0.246753            -0.569125        1.468102       0.294372   \n",
       "3082      0.214789            -0.530439        1.379800       0.225352   \n",
       "3123      0.110204             0.322635        1.302367       0.220408   \n",
       "3153      0.109312             0.000731        1.537289       0.336032   \n",
       "3292      0.055718            -0.129394        1.087075       0.029326   \n",
       "3401      0.164609             0.647279        1.152592       0.329218   \n",
       "3465      0.000000             0.473018        0.800311       0.489855   \n",
       "3499      0.169960            -0.316729        1.405600       0.162055   \n",
       "3576      0.016892             0.416527        1.057506       0.228041   \n",
       "\n",
       "      user_mean_rank  avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                                            \n",
       "26          0.248333          1296.213115         0.250000           1.395062   \n",
       "199         0.515833          1429.568720         0.251185           1.219048   \n",
       "202         0.987222           873.476636         0.252336           0.516432   \n",
       "205         0.651111          1128.376022         0.250681           1.019126   \n",
       "231         0.028056          1383.776536         0.251397           1.263305   \n",
       "284         0.859167          1249.322870         0.251121           0.540541   \n",
       "424         0.479167          1388.605000         0.250000           0.864322   \n",
       "459         0.178333          1343.843478         0.252174           1.371179   \n",
       "469         0.533056          1279.308550         0.249071           0.891791   \n",
       "561         0.543611          1214.692683         0.248780           1.171569   \n",
       "667         0.137222          1260.198198         0.250000           1.031603   \n",
       "699         0.706111          1237.621984         0.249330           0.930108   \n",
       "730         0.249583          1190.303419         0.252137           0.948498   \n",
       "786         0.098611          1406.651568         0.250871           1.220280   \n",
       "849         0.604167          1270.547170         0.250000           0.895735   \n",
       "1045        0.809722          1072.093617         0.246809           1.004274   \n",
       "1057        0.678333          1306.529851         0.251244           0.807980   \n",
       "1066        0.741111          1248.655870         0.251012           0.841463   \n",
       "1151        0.677778          1109.118721         0.251142           0.922018   \n",
       "1279        0.300278          1184.531250         0.250000           0.784038   \n",
       "1289        0.836667          1130.827731         0.252101           0.755274   \n",
       "1307        0.373472          1155.007435         0.245353           0.888060   \n",
       "1450        0.235556          1422.553633         0.245675           1.253472   \n",
       "1455        0.153889           919.012308         0.249231           1.046296   \n",
       "1561        0.436944          1292.951111         0.248889           1.187500   \n",
       "1563        0.162500          1250.078394         0.248566           0.768199   \n",
       "1617        0.083889          1359.825893         0.250000           1.228700   \n",
       "1640        0.433056          1268.467672         0.247845           1.382289   \n",
       "1682        0.792500          1076.918367         0.244898           0.991803   \n",
       "1722        0.401944          1381.936567         0.250000           1.228464   \n",
       "1818        0.735556          1036.200000         0.243678           0.907834   \n",
       "1822        0.890833           960.448763         0.250883           0.687943   \n",
       "1865        0.928611           929.228070         0.245614           0.638767   \n",
       "1903        0.388194          1176.053640         0.249042           0.776923   \n",
       "2093        0.184722          1452.404545         0.250000           1.534247   \n",
       "2120        0.192500          1379.501873         0.250936           1.135338   \n",
       "2163        0.595556          1206.288136         0.245763           1.068085   \n",
       "2225        0.729444          1314.291304         0.252174           0.786026   \n",
       "2328        0.034167          1052.803008         0.248120           1.692771   \n",
       "2390        0.243056          1335.285246         0.249180           1.312500   \n",
       "2458        0.668611          1236.621359         0.252427           0.975610   \n",
       "2543        0.056944          1375.405844         0.246753           1.211726   \n",
       "2548        0.459306          1072.140000         0.250000           1.567839   \n",
       "2595        0.311389          1383.929577         0.248826           1.179245   \n",
       "2648        0.606944          1226.735484         0.251613           1.291262   \n",
       "2767        0.395417          1083.274038         0.247596           0.573494   \n",
       "2778        0.253333          1320.083893         0.251678           0.690236   \n",
       "2782        0.998056           856.459716         0.251185           0.571429   \n",
       "2800        0.496944           986.786713         0.251748           1.017544   \n",
       "2807        0.317500          1436.803828         0.248804           1.139423   \n",
       "2938        0.923889           901.521912         0.239044           0.792000   \n",
       "2980        0.048333          1323.142857         0.251082           1.339130   \n",
       "3082        0.071111          1479.183099         0.250000           1.190813   \n",
       "3123        0.696111          1116.616327         0.236735           0.897541   \n",
       "3153        0.601389          1153.404858         0.251012           1.264228   \n",
       "3292        0.446528           976.988270         0.249267           0.785294   \n",
       "3401        0.849167          1036.823045         0.251029           0.867769   \n",
       "3465        0.987778           851.797101         0.246377           0.546512   \n",
       "3499        0.205556          1246.418972         0.249012           1.146825   \n",
       "3576        0.816944          1059.508446         0.241554           0.639594   \n",
       "\n",
       "      rating_concentration  \n",
       "user                        \n",
       "26                0.254065  \n",
       "199               0.276117  \n",
       "202               0.491833  \n",
       "205               0.279347  \n",
       "231               0.250882  \n",
       "284               0.511392  \n",
       "424               0.327250  \n",
       "459               0.242722  \n",
       "469               0.311618  \n",
       "561               0.263534  \n",
       "667               0.285965  \n",
       "699               0.297242  \n",
       "730               0.341369  \n",
       "786               0.269920  \n",
       "849               0.359381  \n",
       "1045              0.321865  \n",
       "1057              0.428195  \n",
       "1066              0.334065  \n",
       "1151              0.316862  \n",
       "1279              0.419419  \n",
       "1289              0.413424  \n",
       "1307              0.346416  \n",
       "1450              0.240550  \n",
       "1455              0.300317  \n",
       "1561              0.258746  \n",
       "1563              0.352295  \n",
       "1617              0.253866  \n",
       "1640              0.219986  \n",
       "1682              0.327514  \n",
       "1722              0.229923  \n",
       "1818              0.274652  \n",
       "1822              0.393200  \n",
       "1865              0.416551  \n",
       "1903              0.375024  \n",
       "2093              0.227603  \n",
       "2120              0.263982  \n",
       "2163              0.281313  \n",
       "2225              0.359282  \n",
       "2328              0.187160  \n",
       "2390              0.216469  \n",
       "2458              0.287445  \n",
       "2543              0.250042  \n",
       "2548              0.235550  \n",
       "2595              0.269149  \n",
       "2648              0.242310  \n",
       "2767              0.482329  \n",
       "2778              0.473807  \n",
       "2782              0.496193  \n",
       "2800              0.306739  \n",
       "2807              0.290264  \n",
       "2938              0.400232  \n",
       "2980              0.239145  \n",
       "3082              0.253323  \n",
       "3123              0.301858  \n",
       "3153              0.248472  \n",
       "3292              0.393951  \n",
       "3401              0.354096  \n",
       "3465              0.477421  \n",
       "3499              0.261010  \n",
       "3576              0.429151  \n",
       "\n",
       "[60 rows x 326 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user\n",
       "26      0\n",
       "199     1\n",
       "202     2\n",
       "205     2\n",
       "231     0\n",
       "284     2\n",
       "424     1\n",
       "459     0\n",
       "469     2\n",
       "561     1\n",
       "667     0\n",
       "699     2\n",
       "730     1\n",
       "786     0\n",
       "849     1\n",
       "1045    2\n",
       "1057    0\n",
       "1066    2\n",
       "1151    1\n",
       "1279    0\n",
       "1289    2\n",
       "1307    1\n",
       "1450    0\n",
       "1455    1\n",
       "1561    1\n",
       "1563    2\n",
       "1617    0\n",
       "1640    0\n",
       "1682    2\n",
       "1722    1\n",
       "1818    2\n",
       "1822    2\n",
       "1865    2\n",
       "1903    1\n",
       "2093    0\n",
       "2120    0\n",
       "2163    2\n",
       "2225    1\n",
       "2328    1\n",
       "2390    0\n",
       "2458    1\n",
       "2543    0\n",
       "2548    1\n",
       "2595    0\n",
       "2648    2\n",
       "2767    1\n",
       "2778    0\n",
       "2782    2\n",
       "2800    1\n",
       "2807    0\n",
       "2938    2\n",
       "2980    0\n",
       "3082    0\n",
       "3123    1\n",
       "3153    2\n",
       "3292    1\n",
       "3401    1\n",
       "3465    2\n",
       "3499    0\n",
       "3576    2\n",
       "Name: anomtype, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate the dataframes into input X and label y\n",
    "\n",
    "X_log = df_cla.drop(columns=[\"label\", \"anomtype\"])\n",
    "y_log = df_cla[\"anomtype\"]\n",
    "\n",
    "display(X_log)\n",
    "display(y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92dd4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features in X\n",
    "\n",
    "scaler_cla = StandardScaler().fit(X_log)\n",
    "X_log_std = scaler_cla.transform(X_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1e167",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d0153b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: {'estimator__C': 0.1, 'estimator__l1_ratio': 0.5}\n",
      "Best CV Accuracy: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>param_estimator__l1_ratio</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.113039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.084984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.081650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.124722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.113039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.074536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.040825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.040825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.084984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_estimator__C  param_estimator__l1_ratio  mean_test_score  \\\n",
       "0                 0.1                       0.25         0.700000   \n",
       "1                 0.1                       0.50         0.716667   \n",
       "2                 0.1                       0.75         0.683333   \n",
       "3                 1.0                       0.25         0.700000   \n",
       "4                 1.0                       0.50         0.716667   \n",
       "5                 1.0                       0.75         0.666667   \n",
       "6                10.0                       0.25         0.716667   \n",
       "7                10.0                       0.50         0.716667   \n",
       "8                10.0                       0.75         0.716667   \n",
       "\n",
       "   std_test_score  \n",
       "0        0.113039  \n",
       "1        0.084984  \n",
       "2        0.081650  \n",
       "3        0.124722  \n",
       "4        0.113039  \n",
       "5        0.074536  \n",
       "6        0.040825  \n",
       "7        0.040825  \n",
       "8        0.084984  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty=\"elasticnet\",\n",
    "    l1_ratio=0.5,\n",
    "    C=0.01,\n",
    "    solver=\"saga\",\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "# Wrap log_reg in OneVsRest classifier so the solver works for our multiclass classifier\n",
    "log_classifier = OneVsRestClassifier(log_reg)\n",
    "\n",
    "# Using StratifiedKFold, for n_splits=5, we train on 4 folds and validate on the remaining fold\n",
    "# then compute accuracy based on that fold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Code for GridSearchCV: to find the best value of C\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10], \n",
    "    'estimator__l1_ratio': [0.25, 0.5, 0.75]}\n",
    "grid = GridSearchCV(\n",
    "    estimator=log_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid.fit(X_log_std, y_log)\n",
    "\n",
    "print(\"Best C:\", grid.best_params_)\n",
    "print(\"Best CV Accuracy: %.4f\" % grid.best_score_)\n",
    "\n",
    "# Check the results of GridSearchCV on the parameters\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "display(cv_results[[\n",
    "    'param_estimator__C', \n",
    "    'param_estimator__l1_ratio', \n",
    "    'mean_test_score', \n",
    "    'std_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c570e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get final logistic regression model, we choose the best value of C\n",
    "# We don't use logistic regression as our model, so we comment this code out\n",
    "\n",
    "# log_reg = LogisticRegression(\n",
    "#     penalty=\"elasticnet\",\n",
    "#     C=grid.best_params_['estimator__C'],\n",
    "#     l1_ratio=grid.best_params_['estimator__l1_ratio'],\n",
    "#     solver=\"saga\",\n",
    "#     max_iter=10000\n",
    "# )\n",
    "\n",
    "# log_classifier = OneVsRestClassifier(log_reg)\n",
    "\n",
    "# final_model = log_classifier.fit(X_log_std, y_log)\n",
    "# final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44b608",
   "metadata": {},
   "source": [
    "We are able to get an overall maximum accuracy of 70% using this Logistic Regression model trained on ElasticNet.  \n",
    "We move on to see if unsupervised and semi-supervised methods can help us attain a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f74b54",
   "metadata": {},
   "source": [
    "### Unsupervised methods (KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed4f6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_rating</th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>202</td>\n",
       "      <td>0.470231</td>\n",
       "      <td>0.242574</td>\n",
       "      <td>0.534653</td>\n",
       "      <td>0.222772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750701</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>-0.676871</td>\n",
       "      <td>1.407269</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>1372.925743</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>1.273632</td>\n",
       "      <td>0.249240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>138</td>\n",
       "      <td>154</td>\n",
       "      <td>43</td>\n",
       "      <td>335</td>\n",
       "      <td>0.310541</td>\n",
       "      <td>0.128358</td>\n",
       "      <td>0.411940</td>\n",
       "      <td>0.459701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.434637</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>-0.400218</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>0.083582</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>1358.641791</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.880240</td>\n",
       "      <td>0.354564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>207</td>\n",
       "      <td>262</td>\n",
       "      <td>0.147537</td>\n",
       "      <td>0.790076</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665060</td>\n",
       "      <td>0.041985</td>\n",
       "      <td>0.606235</td>\n",
       "      <td>0.866574</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.777639</td>\n",
       "      <td>1373.125954</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.536398</td>\n",
       "      <td>0.507750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "      <td>213</td>\n",
       "      <td>302</td>\n",
       "      <td>0.284512</td>\n",
       "      <td>0.705298</td>\n",
       "      <td>0.119205</td>\n",
       "      <td>0.175497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550003</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.511695</td>\n",
       "      <td>1.374852</td>\n",
       "      <td>0.301325</td>\n",
       "      <td>0.732639</td>\n",
       "      <td>1223.599338</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>1.016611</td>\n",
       "      <td>0.300776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>329</td>\n",
       "      <td>342</td>\n",
       "      <td>0.143587</td>\n",
       "      <td>0.961988</td>\n",
       "      <td>0.020468</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437630</td>\n",
       "      <td>0.854248</td>\n",
       "      <td>0.467836</td>\n",
       "      <td>0.970278</td>\n",
       "      <td>867.903509</td>\n",
       "      <td>0.251462</td>\n",
       "      <td>0.671554</td>\n",
       "      <td>0.463784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_rating  max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                                     \n",
       "0              1           5            108             45          49   \n",
       "1              1           5            138            154          43   \n",
       "2              2           5              1             54         207   \n",
       "3              0           5             36             53         213   \n",
       "4              2           5              7              6         329   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0                    202        0.470231    0.242574       0.534653   \n",
       "1                    335        0.310541    0.128358       0.411940   \n",
       "2                    262        0.147537    0.790076       0.003817   \n",
       "3                    302        0.284512    0.705298       0.119205   \n",
       "4                    342        0.143587    0.961988       0.020468   \n",
       "\n",
       "      neutral_ratio  ...  user_bias  outlier_frac  mean_item_alignment  \\\n",
       "user                 ...                                                 \n",
       "0          0.222772  ...  -0.750701      0.272277            -0.676871   \n",
       "1          0.459701  ...  -0.434637      0.038806            -0.400218   \n",
       "2          0.206107  ...   0.665060      0.041985             0.606235   \n",
       "3          0.175497  ...   0.550003      0.211921             0.511695   \n",
       "4          0.017544  ...   0.443621      0.000000             0.437630   \n",
       "\n",
       "      rating_entropy  extreme_ratio  user_mean_rank  avg_item_popularity  \\\n",
       "user                                                                       \n",
       "0           1.407269       0.282178        0.023889          1372.925743   \n",
       "1           1.212858       0.083582        0.132500          1358.641791   \n",
       "2           0.866574       0.118321        0.777639          1373.125954   \n",
       "3           1.374852       0.301325        0.732639          1223.599338   \n",
       "4           0.854248       0.467836        0.970278           867.903509   \n",
       "\n",
       "      rare_item_ratio  rating_volatility  rating_concentration  \n",
       "user                                                            \n",
       "0            0.252475           1.273632              0.249240  \n",
       "1            0.250746           0.880240              0.354564  \n",
       "2            0.251908           0.536398              0.507750  \n",
       "3            0.251656           1.016611              0.300776  \n",
       "4            0.251462           0.671554              0.463784  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We try and cluster users just based off the created features\n",
    "# Create a dataframe without noise level nor anomtype\n",
    "\n",
    "df_cla_2 = engineer_features(X)\n",
    "df_cla_2 = df_cla_2.iloc[:, -23:]\n",
    "\n",
    "# # We convert all column names to strings so it does not throw an error later\n",
    "df_cla_2.columns = df_cla_2.columns.astype(str)\n",
    "df_cla_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b3f583c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4614703 ,  0.217773  ,  0.67637463, ...,  1.07189726,\n",
       "         0.97681459, -0.90660634],\n",
       "       [ 0.4614703 ,  0.217773  ,  1.17403758, ...,  0.55319392,\n",
       "        -0.49082773,  0.45819998],\n",
       "       [ 1.95141924,  0.217773  , -1.09862321, ...,  0.90183891,\n",
       "        -1.77360743,  2.44320143],\n",
       "       ...,\n",
       "       [-1.02847864,  0.217773  , -0.66731532, ..., -1.82728064,\n",
       "         0.19511647,  0.16412522],\n",
       "       [-1.02847864,  0.217773  ,  0.62660834, ...,  0.61021326,\n",
       "         1.37252393, -1.55390269],\n",
       "       [-1.02847864,  0.217773  , -0.73367038, ..., -0.31911795,\n",
       "        -0.49689691,  0.79808305]], shape=(3600, 23))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the features for KMeans (separate scaler for separate dataframe)\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "df_cla_2_scaled = scaler_features.fit_transform(df_cla_2)\n",
    "df_cla_2_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0c4a755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_rating</th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.676375</td>\n",
       "      <td>-0.753724</td>\n",
       "      <td>-1.307564</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>1.455813</td>\n",
       "      <td>-1.223091</td>\n",
       "      <td>2.072483</td>\n",
       "      <td>-0.360042</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.729360</td>\n",
       "      <td>1.555813</td>\n",
       "      <td>-1.677011</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>-1.649779</td>\n",
       "      <td>1.003710</td>\n",
       "      <td>1.071897</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>-0.906606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>1.174038</td>\n",
       "      <td>1.206414</td>\n",
       "      <td>-1.375877</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>-1.721433</td>\n",
       "      <td>1.295657</td>\n",
       "      <td>1.452570</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019037</td>\n",
       "      <td>-0.879531</td>\n",
       "      <td>-1.009237</td>\n",
       "      <td>-0.370275</td>\n",
       "      <td>-1.139736</td>\n",
       "      <td>-1.273539</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>0.553194</td>\n",
       "      <td>-0.490828</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.951419</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.098623</td>\n",
       "      <td>-0.591878</td>\n",
       "      <td>0.491361</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>-1.419891</td>\n",
       "      <td>1.165742</td>\n",
       "      <td>-1.287937</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452419</td>\n",
       "      <td>-0.846373</td>\n",
       "      <td>1.420099</td>\n",
       "      <td>-2.008956</td>\n",
       "      <td>-0.871282</td>\n",
       "      <td>0.961288</td>\n",
       "      <td>1.004850</td>\n",
       "      <td>0.901839</td>\n",
       "      <td>-1.773607</td>\n",
       "      <td>2.443201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.518016</td>\n",
       "      <td>-0.609861</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>-0.199227</td>\n",
       "      <td>0.795841</td>\n",
       "      <td>-0.557479</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193840</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>1.191901</td>\n",
       "      <td>0.396310</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>0.805404</td>\n",
       "      <td>0.153503</td>\n",
       "      <td>0.826007</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>-0.238795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.951419</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.999091</td>\n",
       "      <td>-1.455058</td>\n",
       "      <td>1.880405</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>-1.455088</td>\n",
       "      <td>1.915819</td>\n",
       "      <td>-1.182528</td>\n",
       "      <td>-1.930129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954757</td>\n",
       "      <td>-1.284317</td>\n",
       "      <td>1.013126</td>\n",
       "      <td>-2.067286</td>\n",
       "      <td>1.829722</td>\n",
       "      <td>1.628609</td>\n",
       "      <td>-1.871693</td>\n",
       "      <td>0.767914</td>\n",
       "      <td>-1.269377</td>\n",
       "      <td>1.873478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.435073</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>-0.430872</td>\n",
       "      <td>-0.607792</td>\n",
       "      <td>-0.193510</td>\n",
       "      <td>-0.117082</td>\n",
       "      <td>-0.290259</td>\n",
       "      <td>0.556078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081018</td>\n",
       "      <td>-0.339776</td>\n",
       "      <td>-0.094876</td>\n",
       "      <td>0.042066</td>\n",
       "      <td>-0.964181</td>\n",
       "      <td>-0.066876</td>\n",
       "      <td>-0.905806</td>\n",
       "      <td>0.919871</td>\n",
       "      <td>-0.117763</td>\n",
       "      <td>0.123364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.766848</td>\n",
       "      <td>-1.077417</td>\n",
       "      <td>0.855701</td>\n",
       "      <td>-0.294320</td>\n",
       "      <td>-0.451492</td>\n",
       "      <td>1.351945</td>\n",
       "      <td>-0.848897</td>\n",
       "      <td>-1.344620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674505</td>\n",
       "      <td>-0.811832</td>\n",
       "      <td>0.705402</td>\n",
       "      <td>-0.350036</td>\n",
       "      <td>1.283952</td>\n",
       "      <td>1.204257</td>\n",
       "      <td>-1.058761</td>\n",
       "      <td>0.590638</td>\n",
       "      <td>-0.239701</td>\n",
       "      <td>0.545020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.667315</td>\n",
       "      <td>-0.825656</td>\n",
       "      <td>0.924014</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>-0.227664</td>\n",
       "      <td>1.133765</td>\n",
       "      <td>-0.766024</td>\n",
       "      <td>-1.062213</td>\n",
       "      <td>...</td>\n",
       "      <td>1.187697</td>\n",
       "      <td>0.115373</td>\n",
       "      <td>1.216609</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.954901</td>\n",
       "      <td>1.038749</td>\n",
       "      <td>0.212611</td>\n",
       "      <td>-1.827281</td>\n",
       "      <td>0.195116</td>\n",
       "      <td>0.164125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.626608</td>\n",
       "      <td>-0.232219</td>\n",
       "      <td>-0.863525</td>\n",
       "      <td>-0.484303</td>\n",
       "      <td>1.729776</td>\n",
       "      <td>-0.843437</td>\n",
       "      <td>1.177393</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.137427</td>\n",
       "      <td>0.825328</td>\n",
       "      <td>-1.138540</td>\n",
       "      <td>1.882849</td>\n",
       "      <td>0.095666</td>\n",
       "      <td>-1.151333</td>\n",
       "      <td>1.660008</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>1.372524</td>\n",
       "      <td>-1.553903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.733670</td>\n",
       "      <td>0.343234</td>\n",
       "      <td>0.616603</td>\n",
       "      <td>0.275628</td>\n",
       "      <td>-0.473418</td>\n",
       "      <td>0.459635</td>\n",
       "      <td>-0.892503</td>\n",
       "      <td>0.272674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496048</td>\n",
       "      <td>-0.202136</td>\n",
       "      <td>0.440186</td>\n",
       "      <td>-0.501901</td>\n",
       "      <td>-0.872557</td>\n",
       "      <td>0.480644</td>\n",
       "      <td>-1.296826</td>\n",
       "      <td>-0.319118</td>\n",
       "      <td>-0.496897</td>\n",
       "      <td>0.798083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_rating  max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                                     \n",
       "0       0.461470    0.217773       0.676375      -0.753724   -1.307564   \n",
       "1       0.461470    0.217773       1.174038       1.206414   -1.375877   \n",
       "2       1.951419    0.217773      -1.098623      -0.591878    0.491361   \n",
       "3      -1.028479    0.217773      -0.518016      -0.609861    0.559675   \n",
       "4       1.951419    0.217773      -0.999091      -1.455058    1.880405   \n",
       "...          ...         ...            ...            ...         ...   \n",
       "3595   -1.028479    0.217773      -0.435073       0.001559   -0.430872   \n",
       "3596   -1.028479    0.217773      -0.766848      -1.077417    0.855701   \n",
       "3597   -1.028479    0.217773      -0.667315      -0.825656    0.924014   \n",
       "3598   -1.028479    0.217773       0.626608      -0.232219   -0.863525   \n",
       "3599   -1.028479    0.217773      -0.733670       0.343234    0.616603   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0              -1.101748        1.455813   -1.223091       2.072483   \n",
       "1               0.161638        0.032727   -1.721433       1.295657   \n",
       "2              -0.531799       -1.419891    1.165742      -1.287937   \n",
       "3              -0.151833       -0.199227    0.795841      -0.557479   \n",
       "4               0.228133       -1.455088    1.915819      -1.182528   \n",
       "...                  ...             ...         ...            ...   \n",
       "3595           -0.607792       -0.193510   -0.117082      -0.290259   \n",
       "3596           -0.294320       -0.451492    1.351945      -0.848897   \n",
       "3597           -0.047343       -0.227664    1.133765      -0.766024   \n",
       "3598           -0.484303        1.729776   -0.843437       1.177393   \n",
       "3599            0.275628       -0.473418    0.459635      -0.892503   \n",
       "\n",
       "      neutral_ratio  ...  user_bias  outlier_frac  mean_item_alignment  \\\n",
       "user                 ...                                                 \n",
       "0         -0.360042  ...  -1.729360      1.555813            -1.677011   \n",
       "1          1.452570  ...  -1.019037     -0.879531            -1.009237   \n",
       "2         -0.487540  ...   1.452419     -0.846373             1.420099   \n",
       "3         -0.721721  ...   1.193840      0.926231             1.191901   \n",
       "4         -1.930129  ...   0.954757     -1.284317             1.013126   \n",
       "...             ...  ...        ...           ...                  ...   \n",
       "3595       0.556078  ...  -0.081018     -0.339776            -0.094876   \n",
       "3596      -1.344620  ...   0.674505     -0.811832             0.705402   \n",
       "3597      -1.062213  ...   1.187697      0.115373             1.216609   \n",
       "3598       0.055999  ...  -1.137427      0.825328            -1.138540   \n",
       "3599       0.272674  ...   0.496048     -0.202136             0.440186   \n",
       "\n",
       "      rating_entropy  extreme_ratio  user_mean_rank  avg_item_popularity  \\\n",
       "user                                                                       \n",
       "0           0.549713       0.394985       -1.649779             1.003710   \n",
       "1          -0.370275      -1.139736       -1.273539             0.922383   \n",
       "2          -2.008956      -0.871282        0.961288             1.004850   \n",
       "3           0.396310       0.542944        0.805404             0.153503   \n",
       "4          -2.067286       1.829722        1.628609            -1.871693   \n",
       "...              ...            ...             ...                  ...   \n",
       "3595        0.042066      -0.964181       -0.066876            -0.905806   \n",
       "3596       -0.350036       1.283952        1.204257            -1.058761   \n",
       "3597        0.012196       0.954901        1.038749             0.212611   \n",
       "3598        1.882849       0.095666       -1.151333             1.660008   \n",
       "3599       -0.501901      -0.872557        0.480644            -1.296826   \n",
       "\n",
       "      rare_item_ratio  rating_volatility  rating_concentration  \n",
       "user                                                            \n",
       "0            1.071897           0.976815             -0.906606  \n",
       "1            0.553194          -0.490828              0.458200  \n",
       "2            0.901839          -1.773607              2.443201  \n",
       "3            0.826007           0.017939             -0.238795  \n",
       "4            0.767914          -1.269377              1.873478  \n",
       "...               ...                ...                   ...  \n",
       "3595         0.919871          -0.117763              0.123364  \n",
       "3596         0.590638          -0.239701              0.545020  \n",
       "3597        -1.827281           0.195116              0.164125  \n",
       "3598         0.610213           1.372524             -1.553903  \n",
       "3599        -0.319118          -0.496897              0.798083  \n",
       "\n",
       "[3600 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring the features back into a dataframe\n",
    "\n",
    "X_kmeans = pd.DataFrame(\n",
    "    df_cla_2_scaled,\n",
    "    columns=df_cla_2.columns,\n",
    "    index=df_cla_2.index\n",
    ")\n",
    "X_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b9a1346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "0       1\n",
       "1       1\n",
       "2       0\n",
       "3       2\n",
       "4       0\n",
       "       ..\n",
       "3595    2\n",
       "3596    0\n",
       "3597    0\n",
       "3598    1\n",
       "3599    2\n",
       "Name: cluster, Length: 3600, dtype: int32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply KMeans to sort the users into diffent clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=67)\n",
    "X_kmeans[\"cluster\"] = kmeans.fit_predict(X_kmeans)\n",
    "X_kmeans[\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37c9497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>cluster</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>424</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>459</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>469</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>561</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>699</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>730</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>849</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1045</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1057</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1066</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1151</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1279</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1289</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1307</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1450</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1455</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1561</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1563</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1640</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1682</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1818</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1822</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1865</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1903</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2093</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2163</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2225</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2328</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2390</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2458</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2543</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2548</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2595</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2648</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2767</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2778</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2782</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2800</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2807</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2938</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3082</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3123</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3153</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>3292</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3401</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3465</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3499</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3576</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user  cluster  anomtype\n",
       "0     26        1         0\n",
       "1    199        2         1\n",
       "2    202        0         2\n",
       "3    205        2         2\n",
       "4    231        1         0\n",
       "5    284        0         2\n",
       "6    424        2         1\n",
       "7    459        1         0\n",
       "8    469        2         2\n",
       "9    561        2         1\n",
       "10   667        1         0\n",
       "11   699        2         2\n",
       "12   730        2         1\n",
       "13   786        1         0\n",
       "14   849        2         1\n",
       "15  1045        0         2\n",
       "16  1057        0         0\n",
       "17  1066        2         2\n",
       "18  1151        2         1\n",
       "19  1279        2         0\n",
       "20  1289        0         2\n",
       "21  1307        2         1\n",
       "22  1450        1         0\n",
       "23  1455        1         1\n",
       "24  1561        2         1\n",
       "25  1563        1         2\n",
       "26  1617        1         0\n",
       "27  1640        1         0\n",
       "28  1682        0         2\n",
       "29  1722        1         1\n",
       "30  1818        2         2\n",
       "31  1822        0         2\n",
       "32  1865        0         2\n",
       "33  1903        2         1\n",
       "34  2093        1         0\n",
       "35  2120        1         0\n",
       "36  2163        2         2\n",
       "37  2225        2         1\n",
       "38  2328        1         1\n",
       "39  2390        1         0\n",
       "40  2458        2         1\n",
       "41  2543        1         0\n",
       "42  2548        1         1\n",
       "43  2595        1         0\n",
       "44  2648        2         2\n",
       "45  2767        2         1\n",
       "46  2778        2         0\n",
       "47  2782        0         2\n",
       "48  2800        2         1\n",
       "49  2807        2         0\n",
       "50  2938        0         2\n",
       "51  2980        1         0\n",
       "52  3082        1         0\n",
       "53  3123        2         1\n",
       "54  3153        2         2\n",
       "55  3292        2         1\n",
       "56  3401        0         1\n",
       "57  3465        0         2\n",
       "58  3499        1         0\n",
       "59  3576        0         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge with y_cat to check the difference between the predicted clusters and actual anomtype\n",
    "\n",
    "merged_df = pd.merge(X_kmeans, y_cat, on=\"user\", how=\"inner\")\n",
    "merged_df = merged_df[[\"user\", \"cluster\", \"anomtype\"]]\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76c5606a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create heatmap for easy visualisation\n",
    "\n",
    "ct = pd.crosstab(merged_df[\"anomtype\"], merged_df[\"cluster\"], normalize='index') * 100\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(ct, annot=True, cmap=\"Blues\", fmt=\".1f\")\n",
    "plt.title(\"Distribution of Cluster IDs within Each Anomaly Type (%)\")\n",
    "plt.ylabel(\"Anomaly Type\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa98208",
   "metadata": {},
   "source": [
    "From this heatmap, we see that the best possible accuracy we can achieve is 70%, if we do the following mapping:\n",
    "- Cluster ID 0 to anomaly type 1\n",
    "- Cluster ID 1 to anomaly type 0\n",
    "- Cluster ID 2 to anomaly type 2.  \n",
    "\n",
    "This is equal than the accuracy obtained from logistic regression (70%).\n",
    "\n",
    "Lastly, we try using PCA in addition to KMeans to see if it improves our performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ad541",
   "metadata": {},
   "source": [
    "### Unsupervised methods (PCA + KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa62a720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.749493</td>\n",
       "      <td>-1.280293</td>\n",
       "      <td>-1.857948</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.302685</td>\n",
       "      <td>3.140477</td>\n",
       "      <td>-0.289164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.136726</td>\n",
       "      <td>2.363126</td>\n",
       "      <td>-2.004187</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800030</td>\n",
       "      <td>-1.726738</td>\n",
       "      <td>-0.143201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.464794</td>\n",
       "      <td>-0.801436</td>\n",
       "      <td>0.172996</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.057448</td>\n",
       "      <td>0.765472</td>\n",
       "      <td>-0.206694</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>4.024893</td>\n",
       "      <td>-1.992253</td>\n",
       "      <td>0.440857</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>3.395013</td>\n",
       "      <td>-2.337388</td>\n",
       "      <td>0.636940</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>-4.394721</td>\n",
       "      <td>-1.642022</td>\n",
       "      <td>-0.806751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>2.654031</td>\n",
       "      <td>0.509978</td>\n",
       "      <td>1.466397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2       PC3  cluster\n",
       "user                                       \n",
       "0    -4.749493 -1.280293 -1.857948        1\n",
       "1    -3.302685  3.140477 -0.289164        0\n",
       "2     4.136726  2.363126 -2.004187        2\n",
       "3     1.800030 -1.726738 -0.143201        2\n",
       "4     6.464794 -0.801436  0.172996        2\n",
       "...        ...       ...       ...      ...\n",
       "3595  0.057448  0.765472 -0.206694        0\n",
       "3596  4.024893 -1.992253  0.440857        2\n",
       "3597  3.395013 -2.337388  0.636940        2\n",
       "3598 -4.394721 -1.642022 -0.806751        1\n",
       "3599  2.654031  0.509978  1.466397        2\n",
       "\n",
       "[3600 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply PCA to reduce features to 3 dimensions\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(df_cla_2_scaled)\n",
    "\n",
    "# Train KMeans on PCA-transformed features\n",
    "pca_kmeans = KMeans(n_clusters=3, random_state=67)\n",
    "pca_clusters = pca_kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Save clusters from numpy array into a dataframe\n",
    "df_pca_kmeans = pd.DataFrame(X_pca, index=df_cla_2.index, columns=[\n",
    "    \"PC1\", \"PC2\", \"PC3\"\n",
    "    ])\n",
    "df_pca_kmeans[\"cluster\"] = pca_clusters\n",
    "\n",
    "df_pca_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad41ca6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check how clusters align with true anomaly types\n",
    "pca_merged = pd.merge(df_pca_kmeans, y_cat, on=\"user\", how=\"inner\")\n",
    "\n",
    "# View alignment\n",
    "ct = pd.crosstab(pca_merged[\"anomtype\"], pca_merged[\"cluster\"], normalize=\"index\") * 100\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(ct, annot=True, cmap=\"Blues\", fmt=\".1f\")\n",
    "plt.title(\"Distribution of Cluster IDs within Each Anomaly Type (%)\")\n",
    "plt.ylabel(\"Anomaly Type\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89f228",
   "metadata": {},
   "source": [
    "We found that applying PCA to reduce our data does not improve our performance (still 70%).  \n",
    "We try another semi-supervised method called label propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e36c85",
   "metadata": {},
   "source": [
    "### Semi-supervised method (Label propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83cf66e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_rating</th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.676375</td>\n",
       "      <td>-0.753724</td>\n",
       "      <td>-1.307564</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>1.455813</td>\n",
       "      <td>-1.223091</td>\n",
       "      <td>2.072483</td>\n",
       "      <td>-0.360042</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.729360</td>\n",
       "      <td>1.555813</td>\n",
       "      <td>-1.677011</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>-1.649779</td>\n",
       "      <td>1.003710</td>\n",
       "      <td>1.071897</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>-0.906606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>1.174038</td>\n",
       "      <td>1.206414</td>\n",
       "      <td>-1.375877</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>-1.721433</td>\n",
       "      <td>1.295657</td>\n",
       "      <td>1.452570</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019037</td>\n",
       "      <td>-0.879531</td>\n",
       "      <td>-1.009237</td>\n",
       "      <td>-0.370275</td>\n",
       "      <td>-1.139736</td>\n",
       "      <td>-1.273539</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>0.553194</td>\n",
       "      <td>-0.490828</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.951419</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.098623</td>\n",
       "      <td>-0.591878</td>\n",
       "      <td>0.491361</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>-1.419891</td>\n",
       "      <td>1.165742</td>\n",
       "      <td>-1.287937</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452419</td>\n",
       "      <td>-0.846373</td>\n",
       "      <td>1.420099</td>\n",
       "      <td>-2.008956</td>\n",
       "      <td>-0.871282</td>\n",
       "      <td>0.961288</td>\n",
       "      <td>1.004850</td>\n",
       "      <td>0.901839</td>\n",
       "      <td>-1.773607</td>\n",
       "      <td>2.443201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.518016</td>\n",
       "      <td>-0.609861</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>-0.199227</td>\n",
       "      <td>0.795841</td>\n",
       "      <td>-0.557479</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193840</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>1.191901</td>\n",
       "      <td>0.396310</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>0.805404</td>\n",
       "      <td>0.153503</td>\n",
       "      <td>0.826007</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>-0.238795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.951419</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.999091</td>\n",
       "      <td>-1.455058</td>\n",
       "      <td>1.880405</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>-1.455088</td>\n",
       "      <td>1.915819</td>\n",
       "      <td>-1.182528</td>\n",
       "      <td>-1.930129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954757</td>\n",
       "      <td>-1.284317</td>\n",
       "      <td>1.013126</td>\n",
       "      <td>-2.067286</td>\n",
       "      <td>1.829722</td>\n",
       "      <td>1.628609</td>\n",
       "      <td>-1.871693</td>\n",
       "      <td>0.767914</td>\n",
       "      <td>-1.269377</td>\n",
       "      <td>1.873478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.435073</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>-0.430872</td>\n",
       "      <td>-0.607792</td>\n",
       "      <td>-0.193510</td>\n",
       "      <td>-0.117082</td>\n",
       "      <td>-0.290259</td>\n",
       "      <td>0.556078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081018</td>\n",
       "      <td>-0.339776</td>\n",
       "      <td>-0.094876</td>\n",
       "      <td>0.042066</td>\n",
       "      <td>-0.964181</td>\n",
       "      <td>-0.066876</td>\n",
       "      <td>-0.905806</td>\n",
       "      <td>0.919871</td>\n",
       "      <td>-0.117763</td>\n",
       "      <td>0.123364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.766848</td>\n",
       "      <td>-1.077417</td>\n",
       "      <td>0.855701</td>\n",
       "      <td>-0.294320</td>\n",
       "      <td>-0.451492</td>\n",
       "      <td>1.351945</td>\n",
       "      <td>-0.848897</td>\n",
       "      <td>-1.344620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674505</td>\n",
       "      <td>-0.811832</td>\n",
       "      <td>0.705402</td>\n",
       "      <td>-0.350036</td>\n",
       "      <td>1.283952</td>\n",
       "      <td>1.204257</td>\n",
       "      <td>-1.058761</td>\n",
       "      <td>0.590638</td>\n",
       "      <td>-0.239701</td>\n",
       "      <td>0.545020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.667315</td>\n",
       "      <td>-0.825656</td>\n",
       "      <td>0.924014</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>-0.227664</td>\n",
       "      <td>1.133765</td>\n",
       "      <td>-0.766024</td>\n",
       "      <td>-1.062213</td>\n",
       "      <td>...</td>\n",
       "      <td>1.187697</td>\n",
       "      <td>0.115373</td>\n",
       "      <td>1.216609</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.954901</td>\n",
       "      <td>1.038749</td>\n",
       "      <td>0.212611</td>\n",
       "      <td>-1.827281</td>\n",
       "      <td>0.195116</td>\n",
       "      <td>0.164125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.626608</td>\n",
       "      <td>-0.232219</td>\n",
       "      <td>-0.863525</td>\n",
       "      <td>-0.484303</td>\n",
       "      <td>1.729776</td>\n",
       "      <td>-0.843437</td>\n",
       "      <td>1.177393</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.137427</td>\n",
       "      <td>0.825328</td>\n",
       "      <td>-1.138540</td>\n",
       "      <td>1.882849</td>\n",
       "      <td>0.095666</td>\n",
       "      <td>-1.151333</td>\n",
       "      <td>1.660008</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>1.372524</td>\n",
       "      <td>-1.553903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.733670</td>\n",
       "      <td>0.343234</td>\n",
       "      <td>0.616603</td>\n",
       "      <td>0.275628</td>\n",
       "      <td>-0.473418</td>\n",
       "      <td>0.459635</td>\n",
       "      <td>-0.892503</td>\n",
       "      <td>0.272674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496048</td>\n",
       "      <td>-0.202136</td>\n",
       "      <td>0.440186</td>\n",
       "      <td>-0.501901</td>\n",
       "      <td>-0.872557</td>\n",
       "      <td>0.480644</td>\n",
       "      <td>-1.296826</td>\n",
       "      <td>-0.319118</td>\n",
       "      <td>-0.496897</td>\n",
       "      <td>0.798083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_rating  max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                                     \n",
       "0       0.461470    0.217773       0.676375      -0.753724   -1.307564   \n",
       "1       0.461470    0.217773       1.174038       1.206414   -1.375877   \n",
       "2       1.951419    0.217773      -1.098623      -0.591878    0.491361   \n",
       "3      -1.028479    0.217773      -0.518016      -0.609861    0.559675   \n",
       "4       1.951419    0.217773      -0.999091      -1.455058    1.880405   \n",
       "...          ...         ...            ...            ...         ...   \n",
       "3595   -1.028479    0.217773      -0.435073       0.001559   -0.430872   \n",
       "3596   -1.028479    0.217773      -0.766848      -1.077417    0.855701   \n",
       "3597   -1.028479    0.217773      -0.667315      -0.825656    0.924014   \n",
       "3598   -1.028479    0.217773       0.626608      -0.232219   -0.863525   \n",
       "3599   -1.028479    0.217773      -0.733670       0.343234    0.616603   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0              -1.101748        1.455813   -1.223091       2.072483   \n",
       "1               0.161638        0.032727   -1.721433       1.295657   \n",
       "2              -0.531799       -1.419891    1.165742      -1.287937   \n",
       "3              -0.151833       -0.199227    0.795841      -0.557479   \n",
       "4               0.228133       -1.455088    1.915819      -1.182528   \n",
       "...                  ...             ...         ...            ...   \n",
       "3595           -0.607792       -0.193510   -0.117082      -0.290259   \n",
       "3596           -0.294320       -0.451492    1.351945      -0.848897   \n",
       "3597           -0.047343       -0.227664    1.133765      -0.766024   \n",
       "3598           -0.484303        1.729776   -0.843437       1.177393   \n",
       "3599            0.275628       -0.473418    0.459635      -0.892503   \n",
       "\n",
       "      neutral_ratio  ...  user_bias  outlier_frac  mean_item_alignment  \\\n",
       "user                 ...                                                 \n",
       "0         -0.360042  ...  -1.729360      1.555813            -1.677011   \n",
       "1          1.452570  ...  -1.019037     -0.879531            -1.009237   \n",
       "2         -0.487540  ...   1.452419     -0.846373             1.420099   \n",
       "3         -0.721721  ...   1.193840      0.926231             1.191901   \n",
       "4         -1.930129  ...   0.954757     -1.284317             1.013126   \n",
       "...             ...  ...        ...           ...                  ...   \n",
       "3595       0.556078  ...  -0.081018     -0.339776            -0.094876   \n",
       "3596      -1.344620  ...   0.674505     -0.811832             0.705402   \n",
       "3597      -1.062213  ...   1.187697      0.115373             1.216609   \n",
       "3598       0.055999  ...  -1.137427      0.825328            -1.138540   \n",
       "3599       0.272674  ...   0.496048     -0.202136             0.440186   \n",
       "\n",
       "      rating_entropy  extreme_ratio  user_mean_rank  avg_item_popularity  \\\n",
       "user                                                                       \n",
       "0           0.549713       0.394985       -1.649779             1.003710   \n",
       "1          -0.370275      -1.139736       -1.273539             0.922383   \n",
       "2          -2.008956      -0.871282        0.961288             1.004850   \n",
       "3           0.396310       0.542944        0.805404             0.153503   \n",
       "4          -2.067286       1.829722        1.628609            -1.871693   \n",
       "...              ...            ...             ...                  ...   \n",
       "3595        0.042066      -0.964181       -0.066876            -0.905806   \n",
       "3596       -0.350036       1.283952        1.204257            -1.058761   \n",
       "3597        0.012196       0.954901        1.038749             0.212611   \n",
       "3598        1.882849       0.095666       -1.151333             1.660008   \n",
       "3599       -0.501901      -0.872557        0.480644            -1.296826   \n",
       "\n",
       "      rare_item_ratio  rating_volatility  rating_concentration  \n",
       "user                                                            \n",
       "0            1.071897           0.976815             -0.906606  \n",
       "1            0.553194          -0.490828              0.458200  \n",
       "2            0.901839          -1.773607              2.443201  \n",
       "3            0.826007           0.017939             -0.238795  \n",
       "4            0.767914          -1.269377              1.873478  \n",
       "...               ...                ...                   ...  \n",
       "3595         0.919871          -0.117763              0.123364  \n",
       "3596         0.590638          -0.239701              0.545020  \n",
       "3597        -1.827281           0.195116              0.164125  \n",
       "3598         0.610213           1.372524             -1.553903  \n",
       "3599        -0.319118          -0.496897              0.798083  \n",
       "\n",
       "[3600 rows x 23 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we prepare the dataframe\n",
    "# We join X_lp with y_cat again so we can fill the unlabeled rows with -1\n",
    "\n",
    "X_lp = pd.DataFrame(\n",
    "    df_cla_2_scaled,\n",
    "    columns=df_cla_2.columns,\n",
    "    index=df_cla_2.index\n",
    ")\n",
    "\n",
    "X_lp = pd.merge(X_lp, y_cat, on=\"user\", how=\"left\")\n",
    "X_lp[\"anomtype\"] = X_lp[\"anomtype\"].fillna(-1)\n",
    "X_full = X_lp.drop([\"label\", \"anomtype\"], axis=1).set_index(\"user\")\n",
    "\n",
    "# Check X_full\n",
    "X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af4f7e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "3595    False\n",
       "3596    False\n",
       "3597    False\n",
       "3598    False\n",
       "3599    False\n",
       "Name: anomtype, Length: 3600, dtype: bool"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask where False = unlabeled, True = labeled\n",
    "\n",
    "y_full = X_lp[\"anomtype\"]\n",
    "mask_labeled = (y_full != -1)\n",
    "mask_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12cc30ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies per fold: [0.8333333333333334, 0.5833333333333334, 0.8333333333333334, 0.8333333333333334, 0.5833333333333334]\n",
      "Mean CV accuracy on held-out labeled users: 0.7333\n"
     ]
    }
   ],
   "source": [
    "# Label Propagation\n",
    "# We test accuracy using Stratified K Fold\n",
    "\n",
    "# Here we mask the rows that already have labels\n",
    "labeled_idx = np.where(y_full != -1)[0]\n",
    "X_labeled = X_full.iloc[labeled_idx]\n",
    "y_labeled = y_full[labeled_idx]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=67)\n",
    "accs = []\n",
    "\n",
    "# print(labeled_idx)\n",
    "\n",
    "for train_idx, test_idx in skf.split(X_labeled, y_labeled):\n",
    "    # Create new partial labels (everything unlabeled)\n",
    "    y_partial = np.full_like(y_full, -1)\n",
    "\n",
    "    # SKF generates folds from index 0-59\n",
    "    # We need to convert them to the actual indices e.g. 26, 199, 202, ... in the full dataset\n",
    "    absolute_train_idx = labeled_idx[train_idx]\n",
    "    absolute_test_idx = labeled_idx[test_idx]\n",
    "\n",
    "    # Assign only the training fold labels\n",
    "    y_partial[absolute_train_idx] = y_labeled[absolute_train_idx]\n",
    "\n",
    "    # Fit the Label Spreading model\n",
    "    model = LabelSpreading(kernel='rbf', gamma=0.25, alpha=0.2, max_iter=50)\n",
    "    model.fit(X_full.values if isinstance(X_full, pd.DataFrame) else X_full, y_partial)\n",
    "\n",
    "    # Evaluate on held-out labeled users\n",
    "    y_pred = model.transduction_[absolute_test_idx]\n",
    "    acc = accuracy_score(y_labeled[absolute_test_idx], y_pred)\n",
    "    accs.append(acc)\n",
    "\n",
    "print(f\"Accuracies per fold: {accs}\")\n",
    "print(f\"Mean CV accuracy on held-out labeled users: {np.mean(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5d3ec",
   "metadata": {},
   "source": [
    "We get a more desirable accuracy of 76.7%, so we choose to use label propagation as our final classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1918e",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da5204",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e8f01f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>849</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3600</td>\n",
       "      <td>722</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3600</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3600</td>\n",
       "      <td>982</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600</td>\n",
       "      <td>749</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284949</th>\n",
       "      <td>4499</td>\n",
       "      <td>757</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284950</th>\n",
       "      <td>4499</td>\n",
       "      <td>752</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284951</th>\n",
       "      <td>4499</td>\n",
       "      <td>751</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284952</th>\n",
       "      <td>4499</td>\n",
       "      <td>778</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284953</th>\n",
       "      <td>4499</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284954 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  rating\n",
       "0       3600   849       5\n",
       "1       3600   722       5\n",
       "2       3600   462       4\n",
       "3       3600   982       4\n",
       "4       3600   749       4\n",
       "...      ...   ...     ...\n",
       "284949  4499   757       4\n",
       "284950  4499   752       4\n",
       "284951  4499   751       4\n",
       "284952  4499   778       4\n",
       "284953  4499     3       3\n",
       "\n",
       "[284954 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking what the test batch looks like again\n",
    "\n",
    "XX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea69a4c",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6aa0e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_50242/283105592.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-7.79989788e+00,  3.64651953e+01,  1.67627218e+01, ...,\n",
       "         2.55805825e+02,  9.95133820e-01,  3.21425205e-01],\n",
       "       [-1.24432240e+01,  1.66845320e+01,  9.28993270e+00, ...,\n",
       "         2.68673729e+02,  1.33191489e+00,  2.18435794e-01],\n",
       "       [-5.76038814e+00,  9.82102578e+00, -4.48504912e+00, ...,\n",
       "         2.44130699e+02,  6.37195122e-01,  4.77388420e-01],\n",
       "       ...,\n",
       "       [-3.38463312e+00,  2.14667896e+01,  1.12725913e+01, ...,\n",
       "         2.84154167e+02,  6.94560669e-01,  4.04618056e-01],\n",
       "       [ 1.45137212e+01,  1.43932120e+00,  1.49571290e-01, ...,\n",
       "         3.11236000e+02,  7.26907631e-01,  3.47040000e-01],\n",
       "       [ 1.88137033e+01, -4.68974389e+00,  1.61418788e+01, ...,\n",
       "         3.56677419e+02,  1.37037037e+00,  2.46575633e-01]],\n",
       "      shape=(900, 86))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 86)\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataframe for predictions\n",
    "\n",
    "XX_df = engineer_features(XX, n_svd_components=300)\n",
    "XX_df.columns = XX_df.columns.astype(str)\n",
    "\n",
    "XX_features_selected = selector_final_optimized.transform(XX_df.values)\n",
    "\n",
    "display(XX_features_selected)\n",
    "print(XX_features_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cb5a9",
   "metadata": {},
   "source": [
    "### Supervised learning prediction (LGBMRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56cb42f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (900,)\n",
      "Prediction range: [0.617, 0.993]\n",
      "(900,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using the trained LGBMRegressor model\n",
    "\n",
    "# XX_df_reg = scaler_full.transform(XX_df)\n",
    "\n",
    "yy_label_pred_optimized = lgbm_final.predict(XX_features_selected)\n",
    "print(f\"Predictions shape: {yy_label_pred_optimized.shape}\")\n",
    "print(f\"Prediction range: [{yy_label_pred_optimized.min():.3f}, {yy_label_pred_optimized.max():.3f}]\")\n",
    "print(yy_label_pred_optimized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7bacc",
   "metadata": {},
   "source": [
    "### Unsupervised learning prediction (Label propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "062f133f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_rating</th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.676375</td>\n",
       "      <td>-0.753724</td>\n",
       "      <td>-1.307564</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>1.455813</td>\n",
       "      <td>-1.223091</td>\n",
       "      <td>2.072483</td>\n",
       "      <td>-0.360042</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.729360</td>\n",
       "      <td>1.555813</td>\n",
       "      <td>-1.677011</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>-1.649779</td>\n",
       "      <td>1.003710</td>\n",
       "      <td>1.071897</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>-0.906606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>1.174038</td>\n",
       "      <td>1.206414</td>\n",
       "      <td>-1.375877</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>-1.721433</td>\n",
       "      <td>1.295657</td>\n",
       "      <td>1.452570</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019037</td>\n",
       "      <td>-0.879531</td>\n",
       "      <td>-1.009237</td>\n",
       "      <td>-0.370275</td>\n",
       "      <td>-1.139736</td>\n",
       "      <td>-1.273539</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>0.553194</td>\n",
       "      <td>-0.490828</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.951419</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.098623</td>\n",
       "      <td>-0.591878</td>\n",
       "      <td>0.491361</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>-1.419891</td>\n",
       "      <td>1.165742</td>\n",
       "      <td>-1.287937</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452419</td>\n",
       "      <td>-0.846373</td>\n",
       "      <td>1.420099</td>\n",
       "      <td>-2.008956</td>\n",
       "      <td>-0.871282</td>\n",
       "      <td>0.961288</td>\n",
       "      <td>1.004850</td>\n",
       "      <td>0.901839</td>\n",
       "      <td>-1.773607</td>\n",
       "      <td>2.443201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.518016</td>\n",
       "      <td>-0.609861</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>-0.199227</td>\n",
       "      <td>0.795841</td>\n",
       "      <td>-0.557479</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193840</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>1.191901</td>\n",
       "      <td>0.396310</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>0.805404</td>\n",
       "      <td>0.153503</td>\n",
       "      <td>0.826007</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>-0.238795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.951419</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.999091</td>\n",
       "      <td>-1.455058</td>\n",
       "      <td>1.880405</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>-1.455088</td>\n",
       "      <td>1.915819</td>\n",
       "      <td>-1.182528</td>\n",
       "      <td>-1.930129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954757</td>\n",
       "      <td>-1.284317</td>\n",
       "      <td>1.013126</td>\n",
       "      <td>-2.067286</td>\n",
       "      <td>1.829722</td>\n",
       "      <td>1.628609</td>\n",
       "      <td>-1.871693</td>\n",
       "      <td>0.767914</td>\n",
       "      <td>-1.269377</td>\n",
       "      <td>1.873478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_rating  max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                                     \n",
       "0       0.461470    0.217773       0.676375      -0.753724   -1.307564   \n",
       "1       0.461470    0.217773       1.174038       1.206414   -1.375877   \n",
       "2       1.951419    0.217773      -1.098623      -0.591878    0.491361   \n",
       "3      -1.028479    0.217773      -0.518016      -0.609861    0.559675   \n",
       "4       1.951419    0.217773      -0.999091      -1.455058    1.880405   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0              -1.101748        1.455813   -1.223091       2.072483   \n",
       "1               0.161638        0.032727   -1.721433       1.295657   \n",
       "2              -0.531799       -1.419891    1.165742      -1.287937   \n",
       "3              -0.151833       -0.199227    0.795841      -0.557479   \n",
       "4               0.228133       -1.455088    1.915819      -1.182528   \n",
       "\n",
       "      neutral_ratio  ...  user_bias  outlier_frac  mean_item_alignment  \\\n",
       "user                 ...                                                 \n",
       "0         -0.360042  ...  -1.729360      1.555813            -1.677011   \n",
       "1          1.452570  ...  -1.019037     -0.879531            -1.009237   \n",
       "2         -0.487540  ...   1.452419     -0.846373             1.420099   \n",
       "3         -0.721721  ...   1.193840      0.926231             1.191901   \n",
       "4         -1.930129  ...   0.954757     -1.284317             1.013126   \n",
       "\n",
       "      rating_entropy  extreme_ratio  user_mean_rank  avg_item_popularity  \\\n",
       "user                                                                       \n",
       "0           0.549713       0.394985       -1.649779             1.003710   \n",
       "1          -0.370275      -1.139736       -1.273539             0.922383   \n",
       "2          -2.008956      -0.871282        0.961288             1.004850   \n",
       "3           0.396310       0.542944        0.805404             0.153503   \n",
       "4          -2.067286       1.829722        1.628609            -1.871693   \n",
       "\n",
       "      rare_item_ratio  rating_volatility  rating_concentration  \n",
       "user                                                            \n",
       "0            1.071897           0.976815             -0.906606  \n",
       "1            0.553194          -0.490828              0.458200  \n",
       "2            0.901839          -1.773607              2.443201  \n",
       "3            0.826007           0.017939             -0.238795  \n",
       "4            0.767914          -1.269377              1.873478  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_rating</th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>0.461470</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.634138</td>\n",
       "      <td>0.145422</td>\n",
       "      <td>-0.909068</td>\n",
       "      <td>-1.044753</td>\n",
       "      <td>0.078842</td>\n",
       "      <td>-0.519438</td>\n",
       "      <td>-0.429492</td>\n",
       "      <td>1.429843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170875</td>\n",
       "      <td>-0.030588</td>\n",
       "      <td>0.151994</td>\n",
       "      <td>0.280634</td>\n",
       "      <td>-0.113755</td>\n",
       "      <td>-0.077461</td>\n",
       "      <td>-4.989630</td>\n",
       "      <td>-1.113025</td>\n",
       "      <td>0.640836</td>\n",
       "      <td>-0.129409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.683904</td>\n",
       "      <td>-1.239263</td>\n",
       "      <td>1.197269</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>-0.436394</td>\n",
       "      <td>1.468320</td>\n",
       "      <td>-0.786249</td>\n",
       "      <td>-1.624386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569302</td>\n",
       "      <td>-0.851080</td>\n",
       "      <td>0.594735</td>\n",
       "      <td>-0.539760</td>\n",
       "      <td>1.374624</td>\n",
       "      <td>1.319727</td>\n",
       "      <td>-5.428089</td>\n",
       "      <td>-10.453640</td>\n",
       "      <td>-0.259268</td>\n",
       "      <td>0.762769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>1.951419</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.065446</td>\n",
       "      <td>-1.113383</td>\n",
       "      <td>0.548289</td>\n",
       "      <td>-0.740780</td>\n",
       "      <td>-1.284779</td>\n",
       "      <td>1.572636</td>\n",
       "      <td>-1.232968</td>\n",
       "      <td>-1.267427</td>\n",
       "      <td>...</td>\n",
       "      <td>1.548023</td>\n",
       "      <td>-0.588915</td>\n",
       "      <td>1.554745</td>\n",
       "      <td>-1.338287</td>\n",
       "      <td>1.305493</td>\n",
       "      <td>1.492932</td>\n",
       "      <td>-5.195337</td>\n",
       "      <td>-0.920714</td>\n",
       "      <td>-1.183546</td>\n",
       "      <td>1.106801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.119886</td>\n",
       "      <td>0.702892</td>\n",
       "      <td>-1.136780</td>\n",
       "      <td>-0.645789</td>\n",
       "      <td>-0.103666</td>\n",
       "      <td>-1.164513</td>\n",
       "      <td>0.207202</td>\n",
       "      <td>1.791473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.739144</td>\n",
       "      <td>-0.366387</td>\n",
       "      <td>-0.759867</td>\n",
       "      <td>-0.170710</td>\n",
       "      <td>-1.260152</td>\n",
       "      <td>-0.785678</td>\n",
       "      <td>-5.041143</td>\n",
       "      <td>-1.470724</td>\n",
       "      <td>-1.062868</td>\n",
       "      <td>0.360697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>-1.028479</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.742730</td>\n",
       "      <td>-0.717758</td>\n",
       "      <td>-1.205093</td>\n",
       "      <td>-0.959261</td>\n",
       "      <td>1.313556</td>\n",
       "      <td>-1.115291</td>\n",
       "      <td>1.955215</td>\n",
       "      <td>-0.407340</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.603562</td>\n",
       "      <td>1.215283</td>\n",
       "      <td>-1.564363</td>\n",
       "      <td>0.714649</td>\n",
       "      <td>-0.040648</td>\n",
       "      <td>-1.597817</td>\n",
       "      <td>-4.782417</td>\n",
       "      <td>-0.016319</td>\n",
       "      <td>1.337720</td>\n",
       "      <td>-0.941135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_rating  max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                                     \n",
       "4495    0.461470    0.217773      -0.634138       0.145422   -0.909068   \n",
       "4496   -1.028479    0.217773      -0.683904      -1.239263    1.197269   \n",
       "4497    1.951419    0.217773      -1.065446      -1.113383    0.548289   \n",
       "4498   -1.028479    0.217773      -0.119886       0.702892   -1.136780   \n",
       "4499   -1.028479    0.217773       0.742730      -0.717758   -1.205093   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "4495           -1.044753        0.078842   -0.519438      -0.429492   \n",
       "4496           -0.047343       -0.436394    1.468320      -0.786249   \n",
       "4497           -0.740780       -1.284779    1.572636      -1.232968   \n",
       "4498           -0.645789       -0.103666   -1.164513       0.207202   \n",
       "4499           -0.959261        1.313556   -1.115291       1.955215   \n",
       "\n",
       "      neutral_ratio  ...  user_bias  outlier_frac  mean_item_alignment  \\\n",
       "user                 ...                                                 \n",
       "4495       1.429843  ...   0.170875     -0.030588             0.151994   \n",
       "4496      -1.624386  ...   0.569302     -0.851080             0.594735   \n",
       "4497      -1.267427  ...   1.548023     -0.588915             1.554745   \n",
       "4498       1.791473  ...  -0.739144     -0.366387            -0.759867   \n",
       "4499      -0.407340  ...  -1.603562      1.215283            -1.564363   \n",
       "\n",
       "      rating_entropy  extreme_ratio  user_mean_rank  avg_item_popularity  \\\n",
       "user                                                                       \n",
       "4495        0.280634      -0.113755       -0.077461            -4.989630   \n",
       "4496       -0.539760       1.374624        1.319727            -5.428089   \n",
       "4497       -1.338287       1.305493        1.492932            -5.195337   \n",
       "4498       -0.170710      -1.260152       -0.785678            -5.041143   \n",
       "4499        0.714649      -0.040648       -1.597817            -4.782417   \n",
       "\n",
       "      rare_item_ratio  rating_volatility  rating_concentration  \n",
       "user                                                            \n",
       "4495        -1.113025           0.640836             -0.129409  \n",
       "4496       -10.453640          -0.259268              0.762769  \n",
       "4497        -0.920714          -1.183546              1.106801  \n",
       "4498        -1.470724          -1.062868              0.360697  \n",
       "4499        -0.016319           1.337720             -0.941135  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for Label Propagation\n",
    "\n",
    "XX_lp_features = XX_df.iloc[:, -23:]\n",
    "# Use the same scaler the model was trained on \n",
    "XX_lp_features_scaled = scaler_features.transform(XX_lp_features)\n",
    "XX_lp_features = pd.DataFrame(\n",
    "    XX_lp_features_scaled,\n",
    "    columns=XX_lp_features.columns,\n",
    "    index=XX_lp_features.index\n",
    ")\n",
    "\n",
    "# Concatenate it with the previous training dataset\n",
    "lp_full = pd.concat([X_full, XX_lp_features], axis=0)\n",
    "\n",
    "display(lp_full.head(5))\n",
    "display(lp_full.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5a82358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4495   -1.0\n",
       "4496   -1.0\n",
       "4497   -1.0\n",
       "4498   -1.0\n",
       "4499   -1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 900 rows of -1 so we can predict on the test dataset\n",
    "y_full = pd.concat([y_full, pd.Series([-1] * 900)], ignore_index=True)\n",
    "y_full.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51a40ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2., 1., 2., 2., 1., 1., 2., 2., 1., 2., 2., 1., 1., 1., 0.,\n",
       "       2., 1., 2., 1., 2., 1., 2., 2., 1., 1., 1., 2., 1., 1., 2., 1., 1.,\n",
       "       1., 0., 0., 1., 2., 1., 2., 1., 1., 1., 1., 0., 2., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 2., 1., 1., 1., 2., 1., 2., 1., 2., 1.,\n",
       "       1., 2., 1., 0., 1., 1., 1., 0., 1., 1., 2., 2., 1., 2., 2., 2., 2.,\n",
       "       1., 0., 2., 1., 1., 2., 1., 1., 1., 1., 2., 1., 2., 1., 2., 1., 1.,\n",
       "       2., 2., 1., 1., 2., 1., 1., 2., 1., 2., 2., 1., 2., 2., 1., 2., 2.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 2., 2., 2., 0., 2., 1., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 2., 1., 2., 0., 1., 1., 2.,\n",
       "       2., 2., 2., 1., 0., 1., 2., 1., 1., 2., 1., 1., 1., 1., 2., 1., 2.,\n",
       "       1., 2., 1., 1., 2., 1., 1., 2., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 2., 1., 1., 1., 1., 0., 2., 2., 1., 1., 0., 2., 2.,\n",
       "       1., 1., 0., 2., 2., 1., 2., 1., 0., 1., 2., 2., 1., 2., 2., 1., 1.,\n",
       "       2., 1., 1., 2., 0., 1., 2., 2., 1., 0., 1., 2., 1., 1., 0., 0., 1.,\n",
       "       2., 1., 2., 2., 1., 2., 1., 0., 1., 2., 2., 2., 1., 1., 2., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 2., 1., 1., 1., 1., 2., 1., 1., 1., 0., 2., 2., 2.,\n",
       "       1., 1., 2., 2., 1., 2., 1., 1., 2., 2., 1., 1., 2., 1., 2., 1., 1.,\n",
       "       2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1.,\n",
       "       2., 1., 2., 1., 2., 2., 1., 1., 0., 1., 0., 1., 2., 2., 2., 1., 1.,\n",
       "       2., 2., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 2., 1., 0., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 0., 1., 1., 1., 2., 1., 1., 2., 1., 1., 2.,\n",
       "       1., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 1., 2., 1., 1., 2., 1.,\n",
       "       1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 1., 1., 2., 1., 1., 1., 1., 2., 1., 1., 2.,\n",
       "       2., 1., 1., 1., 2., 1., 1., 2., 2., 2., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 0., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 2., 1., 0., 1., 2., 2., 1., 2., 1., 1., 1., 1., 2., 1., 2., 1.,\n",
       "       1., 2., 0., 1., 2., 1., 2., 2., 2., 1., 0., 2., 0., 1., 1., 2., 1.,\n",
       "       2., 2., 0., 2., 2., 0., 1., 0., 2., 0., 2., 1., 1., 2., 2., 2., 1.,\n",
       "       1., 2., 1., 1., 0., 2., 2., 1., 1., 2., 1., 0., 2., 2., 1., 2., 1.,\n",
       "       1., 1., 2., 2., 1., 2., 1., 2., 2., 2., 0., 1., 1., 1., 2., 1., 2.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 0., 1., 0.,\n",
       "       2., 1., 1., 0., 1., 1., 1., 2., 1., 1., 0., 1., 2., 1., 2., 1., 2.,\n",
       "       2., 1., 2., 1., 0., 1., 1., 1., 0., 1., 2., 1., 1., 1., 2., 1., 1.,\n",
       "       1., 2., 2., 1., 2., 2., 2., 1., 1., 1., 1., 1., 2., 1., 1., 0., 2.,\n",
       "       0., 1., 1., 2., 1., 1., 0., 1., 1., 0., 1., 1., 0., 2., 1., 1., 2.,\n",
       "       1., 2., 2., 1., 0., 1., 1., 1., 1., 2., 0., 2., 1., 1., 0., 2., 1.,\n",
       "       1., 0., 2., 0., 2., 2., 1., 2., 2., 2., 2., 1., 1., 1., 1., 2., 2.,\n",
       "       1., 2., 0., 2., 0., 1., 2., 1., 1., 1., 2., 2., 2., 1., 0., 1., 0.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 2., 1., 2., 2., 1.,\n",
       "       2., 0., 1., 1., 2., 2., 2., 1., 2., 1., 1., 0., 1., 0., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 2., 1., 1., 1., 1., 0., 2., 2., 1., 1., 1., 2., 1.,\n",
       "       0., 1., 1., 2., 1., 2., 1., 2., 2., 1., 1., 2., 2., 0., 2., 2., 1.,\n",
       "       1., 2., 1., 2., 1., 1., 1., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 2., 1., 1., 2., 1., 1., 2., 1., 1., 1., 1., 0., 2., 2., 1.,\n",
       "       1., 1., 2., 0., 1., 2., 1., 1., 1., 2., 2., 2., 2., 1., 0., 0., 2.,\n",
       "       0., 2., 2., 2., 1., 2., 1., 2., 1., 2., 1., 2., 2., 1., 2., 2., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 2., 1., 2., 1., 2.,\n",
       "       2., 2., 2., 0., 1., 2., 1., 1., 1., 1., 2., 1., 1., 2., 1., 2., 0.,\n",
       "       2., 2., 2., 1., 1., 2., 2., 1., 1., 1., 2., 1., 2., 2., 1., 0.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data to label propagation\n",
    "\n",
    "model = LabelSpreading(kernel='rbf', gamma=0.25, alpha=0.2, max_iter=50)\n",
    "model.fit(lp_full, y_full)\n",
    "\n",
    "yy_label_cluster = model.transduction_[-900:]\n",
    "yy_label_cluster\n",
    "\n",
    "# Debug: class probabilities\n",
    "# y_test_proba = model.label_distributions_[-900:]\n",
    "# y_test_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7684144",
   "metadata": {},
   "source": [
    "### Saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc822f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>0.830512</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3601</td>\n",
       "      <td>0.856964</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3602</td>\n",
       "      <td>0.928235</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3603</td>\n",
       "      <td>0.775255</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>0.892044</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3605</td>\n",
       "      <td>0.744669</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3606</td>\n",
       "      <td>0.721323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3607</td>\n",
       "      <td>0.920685</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3608</td>\n",
       "      <td>0.964756</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3609</td>\n",
       "      <td>0.786650</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user     label  anomtype\n",
       "0  3600  0.830512       2.0\n",
       "1  3601  0.856964       1.0\n",
       "2  3602  0.928235       2.0\n",
       "3  3603  0.775255       1.0\n",
       "4  3604  0.892044       2.0\n",
       "5  3605  0.744669       2.0\n",
       "6  3606  0.721323       1.0\n",
       "7  3607  0.920685       1.0\n",
       "8  3608  0.964756       2.0\n",
       "9  3609  0.786650       2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine dataframe with predicted results\n",
    "\n",
    "result_df = XX_df.reset_index()\n",
    "result_df = result_df[[\"user\"]]\n",
    "result_df[\"label\"] = yy_label_pred_optimized # from LGBMRegressor\n",
    "result_df[\"anomtype\"] = yy_label_cluster # mapped from Label propagation\n",
    "display(result_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "037a6a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>0.830512</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3601</td>\n",
       "      <td>0.856964</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3602</td>\n",
       "      <td>0.928235</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3603</td>\n",
       "      <td>0.775255</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>0.892044</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>4495</td>\n",
       "      <td>0.738647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>4496</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>4497</td>\n",
       "      <td>0.862474</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>4498</td>\n",
       "      <td>0.770055</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>4499</td>\n",
       "      <td>0.885033</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user     label  anomtype\n",
       "0    3600  0.830512       2.0\n",
       "1    3601  0.856964       1.0\n",
       "2    3602  0.928235       2.0\n",
       "3    3603  0.775255       1.0\n",
       "4    3604  0.892044       2.0\n",
       "..    ...       ...       ...\n",
       "895  4495  0.738647       1.0\n",
       "896  4496  0.910332       2.0\n",
       "897  4497  0.862474       2.0\n",
       "898  4498  0.770055       1.0\n",
       "899  4499  0.885033       0.0\n",
       "\n",
       "[900 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up label predictions\n",
    "# We normalise label column as some predictions are <0 or >1\n",
    "\n",
    "result_df[\"label\"] = result_df[\"label\"].clip(lower=0, upper=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e379e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result successfully saved\n"
     ]
    }
   ],
   "source": [
    "# Save as csv\n",
    "\n",
    "result_df.to_csv('fifth_batch_output.csv',index=False)\n",
    "print(\"Result successfully saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ffb9e",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2984875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overfitting Analysis ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.0237\n",
      "Validation MAE: 0.0229\n",
      "Gap (Overfit indicator): -0.0007\n",
      "Relative Gap: -3.01%\n"
     ]
    }
   ],
   "source": [
    "# Add this analysis cell after training your final model\n",
    "print(\"=== Overfitting Analysis ===\")\n",
    "y_train_pred = lgbm_final.predict(X_train_opt_selected)\n",
    "y_val_pred = lgbm_final.predict(X_val_opt_selected)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_opt, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val_opt, y_val_pred)\n",
    "\n",
    "print(f\"Training MAE: {train_mae:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Gap (Overfit indicator): {val_mae - train_mae:.4f}\")\n",
    "print(f\"Relative Gap: {((val_mae - train_mae) / train_mae * 100):.2f}%\")\n",
    "\n",
    "# Rule of thumb:\n",
    "# - Gap < 0.005: Good generalization\n",
    "# - Gap 0.005-0.01: Acceptable\n",
    "# - Gap > 0.01: Likely overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83df07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 MAE: 0.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 MAE: 0.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 MAE: 0.0614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 MAE: 0.0602\n",
      "Fold 5 MAE: 0.0652\n",
      "\n",
      "Mean CV MAE: 0.0628\n",
      "Std CV MAE: 0.0026\n",
      "95% Confidence Interval: [0.0577, 0.0679]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "### Cross-Validation Check for Overfitting\n",
    "\n",
    "# Prepare full dataset\n",
    "X_full = df_reg_optimized.drop(columns=[\"label\"]).values\n",
    "y_full = df_reg_optimized[\"label\"].values\n",
    "X_full_selected = selector_final_optimized.transform(X_full)\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=67)\n",
    "cv_maes = []\n",
    "\n",
    "lgbm_cv = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=0.001\n",
    ")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full_selected), 1):\n",
    "    X_train_fold = X_full_selected[train_idx]\n",
    "    X_val_fold = X_full_selected[val_idx]\n",
    "    y_train_fold = y_full[train_idx]\n",
    "    y_val_fold = y_full[val_idx]\n",
    "    \n",
    "    lgbm_cv.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = lgbm_cv.predict(X_val_fold)\n",
    "    \n",
    "    mae_fold = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    cv_maes.append(mae_fold)\n",
    "    print(f\"Fold {fold} MAE: {mae_fold:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV MAE: {np.mean(cv_maes):.4f}\")\n",
    "print(f\"Std CV MAE: {np.std(cv_maes):.4f}\")\n",
    "print(f\"95% Confidence Interval: [{np.mean(cv_maes) - 2*np.std(cv_maes):.4f}, {np.mean(cv_maes) + 2*np.std(cv_maes):.4f}]\")\n",
    "\n",
    "# Rule of thumb:\n",
    "# - Low std (< 0.005): Model is stable\n",
    "# - High std (> 0.01): Model is sensitive to data splits (potential overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fd719b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV of feature importances: 0.1235\n",
      "Max CV of feature importances: 0.3420\n"
     ]
    }
   ],
   "source": [
    "### Feature Importance Stability\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "feature_importance_folds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full_selected), 1):\n",
    "    lgbm_temp = LGBMRegressor(\n",
    "        random_state=67,\n",
    "        max_depth=6,\n",
    "        num_leaves=64,\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.01,\n",
    "        reg_lambda=0.1,\n",
    "        reg_alpha=1.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbose=-1,\n",
    "        min_child_samples=20,\n",
    "        min_child_weight=0.001\n",
    "    )\n",
    "    \n",
    "    lgbm_temp.fit(X_full_selected[train_idx], y_full[train_idx])\n",
    "    feature_importance_folds.append(lgbm_temp.feature_importances_)\n",
    "\n",
    "# Calculate stability (coefficient of variation)\n",
    "importance_array = np.array(feature_importance_folds)\n",
    "importance_mean = importance_array.mean(axis=0)\n",
    "importance_std = importance_array.std(axis=0)\n",
    "importance_cv = importance_std / (importance_mean + 1e-10)  # Coefficient of variation\n",
    "\n",
    "print(f\"Mean CV of feature importances: {importance_cv.mean():.4f}\")\n",
    "print(f\"Max CV of feature importances: {importance_cv.max():.4f}\")\n",
    "\n",
    "# Rule of thumb:\n",
    "# - Mean CV < 0.5: Stable features (good)\n",
    "# - Mean CV > 1.0: Unstable features (potential overfitting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
