{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc61b84",
   "metadata": {},
   "source": [
    "# CS421 Project\n",
    "---\n",
    "\n",
    "Group: Empirical Risk Minimisers  \n",
    "Members:\n",
    "- Lai Wan Xuan Joanne (joanne.lai.2021)\n",
    "- Ryan Miguel Moralde Sia (ryansia.2022)\n",
    "- Dhruv Benegal (benegalda.2022)\n",
    "- Benedict Lee Zi Le (benedictlee.2022)\n",
    "\n",
    "In this notebook, we train our models on the four batches of data provided, and produce predictions on the fifth and final batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ade660",
   "metadata": {},
   "source": [
    "### 1. Background & Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce15d29",
   "metadata": {},
   "source": [
    "In this project, you will be working with data extracted from famous recommender systems type datasets: you are provided with a large set of interactions between users (persons) and items (movies). Whenever a user \"interacts\" with an item, it watches the movie and gives a \"rating\". There are 5 possible ratings expressed as a \"number of stars\": 1,2,3,4, or 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fab5e",
   "metadata": {},
   "source": [
    "In this exercise, we will **not** be performing the recommendation task per se. Instead, you will try to identify the amount of noise/corruption which was injected in each user. Indeed, for each of the users you have been given, an anomaly/noise generation procedure was applied to corrupt the sample. The noise generation procedure depends on two variables: the noise level $p\\in [0,1]$ and the noise type $X\\in\\{0,1,2\\}$.  Each user has been randomly assigned a noise level $p$ and anomaly/noise type $X$, and subsequently been corrupted with the associated noise generation procedure. \n",
    "\n",
    "You have two tasks: first, you must predict the noise level $p$ associated to each test user. This is a **supervised regression task**. Second, you must try to identify the noise generation type for each user. This is a classification task with three classes, with the possibility of including more classes later depending on class performance. This task will be semi-supervised: only a very small number of labels is provided. You will therefore need to combine supervised and unsupervised approaches for this component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c248f",
   "metadata": {},
   "source": [
    "### 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df689c",
   "metadata": {},
   "source": [
    "You are provided with three frames: the first one (\"X\") contains the interactions provided to you, and the second one (\"yy\") contains the continuous for the users. The third data frame \"yy_cat\" contains the anomaly/noise type for 15 users. The idea is to use these users to disambiguate the category types, but the task will mostly be unsupervised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc60513",
   "metadata": {},
   "source": [
    "As you can see, the three columns in \"X\" correspond to the user ID, the item ID and the rating (encoded into numerical form). Thus, each row of \"X\" contains a single interaction. For instance, if the row \"$142, 152, 5$\" is present, this means that the user with ID $142$ has given the movie $152$ a positive rating of $5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc9801",
   "metadata": {},
   "source": [
    "The dataframe \"yy\" has two columns. In the first column we have the user IDs, whilst the second column contains the continuous label. A label of $0.01$ indicates a very low anomaly level, whilst a label of $0.99$ indicates a very high amount of noise/corruption. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ef3e7",
   "metadata": {},
   "source": [
    "### 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ae8f7",
   "metadata": {},
   "source": [
    "Your task is to be able to regress the noise level $p$ for each new user, and predict the anomaly type $X$. The first (regression) task will be easier due to the larger amount of supervision, and will form the main basis of the evaluation. The second task will be more importance to showcase each team's creativity and differentiate between top performers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e059a16",
   "metadata": {},
   "source": [
    "THE **EVALUATION METRICs** are:  \n",
    "\n",
    "1. The Mean Absolute Error (MAE) for the regression task. \n",
    "2. The accuracy for the classiciation task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42d14f",
   "metadata": {},
   "source": [
    "Every few weeks, we will evaluate the performance of each team (on a *test set with unseen labels* that I will provide) in terms of both metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35f65c",
   "metadata": {},
   "source": [
    "The difficulty implied by **the generation procedure of the anomalies MAY CHANGE as the project evolves: depending on how well the teams are doing, I may generate easier or harder anomaly classes, which would change the number of labels in the classification task**. However, the regression task will still be the same (with a different distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065547d",
   "metadata": {},
   "source": [
    "### 4. Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac837a",
   "metadata": {},
   "source": [
    "Together with this file, you are provided with a first batch of examples \"`first_batch_regression_labelled.npz`\" which are labelled in terms of noise level. You are also provided with the test samples to rank by the next round (without labels) in the file \"`second_batch_regression_unlabelled.npz`\".\n",
    "\n",
    "The **first round** will take place after recess (week 9): you must hand in your scores for the second batch before the **Wednesday at NOON (15th of October)**. We will then look at the results together on the Friday.  \n",
    "\n",
    "We will check everyone's performance in this way every week (once on  week 10, once on week 11 and once on week 12). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527121a4",
   "metadata": {},
   "source": [
    "To summarise, the project deliverables are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab22a",
   "metadata": {},
   "source": [
    "- Before every checkpoint's deadline, you need to submit **a `.csv` file** containing a dataframe of size $\\text{number of test batch users} \\times 3$.\n",
    "    - The first column should be the user IDs of the test batch.\n",
    "    - The second column should contain the estimated noise level $p$ for each sample.\n",
    "    - The final column should contain the estimated class (it should be a natural number in \\{0,1,2\\}).\n",
    "- The order of rows should correspond to the user IDs. For example, if the test batch contains users 1100-2200, scores for user 1100 should be the first row (row 0), scores for user 1101 should be the second row (row 1), and so on.\n",
    "- On Week 12-13 (schedule to be decided), you need to present your work in class. The presentation duration is **10 minutes** with 5 minutes of QA.\n",
    "- On Week 12, you need to submit your **Jupyter Notebook** (with comments in Markdown) and the **slides** for your presentation. \n",
    "- On week 13 you need to submit your **final report**. The final report should be 2-3 pages long (consisting of problem statement, literature review, and motivation of algorithm design) with unlimited references/appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3015c",
   "metadata": {},
   "source": [
    "Whilst performance (expressed in terms of MAE and accuracy) at **each of the check points** (weeks 9 to 12 inclusive) is an **important component** of your **final grade**, the **final report** and the detail of the various methods you will have tried will **also** be very **important**. Ideally, to get perfect marks (A+), you should try at least **two supervised methods** and **two unsupervised methods**, as well as be ranked the **best team** in terms of performance. \n",
    "\n",
    "\n",
    "In addition, I will be especially interested in your **reasoning**. Especially high marks will be awarded to any team that is able to **qualitatively describe** the difference between the two anomaly types. You are also encouraged to compute statistics related to each class and describe what is different about them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab69c97",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b6c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import (\n",
    "    kurtosis,\n",
    "    skew,\n",
    "    entropy\n",
    ")\n",
    "from sklearn.decomposition import (\n",
    "    PCA,\n",
    "    TruncatedSVD\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    SGDRegressor\n",
    "    )\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeRegressor\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    BaggingRegressor\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5b0da-1a70-409b-8117-4e831ff7da13",
   "metadata": {},
   "source": [
    "## Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285ff9b",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "first_batch  = np.load(\"data/Week1/first_batch_regression_labelled.npz\")\n",
    "second_batch  = np.load(\"data/Week2/second_batch_regression_labelled.npz\")\n",
    "third_batch  = np.load(\"data/Week3/third_batch_regression_labelled.npz\")\n",
    "fourth_batch  = np.load(\"data/Week4/fourth_batch_regression_labelled.npz\")\n",
    "\n",
    "# Extract X, y, y_cat for each batch\n",
    "X1, y1, y1_cat = first_batch[\"X\"], first_batch[\"yy\"], first_batch[\"yy_cat\"]\n",
    "X2, y2, y2_cat = second_batch[\"X\"], second_batch[\"yy\"], second_batch[\"yy_cat\"]\n",
    "X3, y3, y3_cat = third_batch[\"X\"], third_batch[\"yy\"], third_batch[\"yy_cat\"]\n",
    "X4, y4, y4_cat = fourth_batch[\"X\"], fourth_batch[\"yy\"], fourth_batch[\"yy_cat\"]\n",
    "\n",
    "# Convert to DataFrames and rename columns for each batch\n",
    "X1     = pd.DataFrame(X1, columns=[\"user\", \"item\", \"rating\"])\n",
    "y1     = pd.DataFrame(y1, columns=[\"user\", \"label\"])\n",
    "y1_cat = pd.DataFrame(y1_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "X2     = pd.DataFrame(X2, columns=[\"user\", \"item\", \"rating\"])\n",
    "y2     = pd.DataFrame(y2, columns=[\"user\", \"label\"])\n",
    "y2_cat = pd.DataFrame(y2_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "X3     = pd.DataFrame(X3, columns=[\"user\", \"item\", \"rating\"])\n",
    "y3     = pd.DataFrame(y3, columns=[\"user\", \"label\"])\n",
    "y3_cat = pd.DataFrame(y3_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "X4     = pd.DataFrame(X4, columns=[\"user\", \"item\", \"rating\"])\n",
    "y4     = pd.DataFrame(y4, columns=[\"user\", \"label\"])\n",
    "y4_cat = pd.DataFrame(y4_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "\n",
    "# Combine the data \n",
    "X = pd.concat([X1, X2, X3, X4], ignore_index=True)\n",
    "y = pd.concat([y1, y2, y3, y4], ignore_index=True)\n",
    "y_cat = pd.concat([y1_cat, y2_cat, y3_cat, y4_cat], ignore_index=True)\n",
    "\n",
    "# Parse to correct types\n",
    "y     = y.astype({\"user\": int, \"label\": float})\n",
    "y_cat = y_cat.astype({\"user\": int, \"label\": float, \"anomtype\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf10da70-7e1f-416c-9ce2-f7cf4c78d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX    = np.load(\"data/Week4/fifth_batch_regression_unlabelled.npz\")['X']\n",
    "XX    = pd.DataFrame(XX, columns=[\"user\", \"item\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b3347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144737</th>\n",
       "      <td>3599</td>\n",
       "      <td>396</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144738</th>\n",
       "      <td>3599</td>\n",
       "      <td>183</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144739</th>\n",
       "      <td>3599</td>\n",
       "      <td>877</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144740</th>\n",
       "      <td>3599</td>\n",
       "      <td>961</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144741</th>\n",
       "      <td>3599</td>\n",
       "      <td>416</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1144742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user  item  rating\n",
       "0           0    94       2\n",
       "1           0    90       1\n",
       "2           0    97       2\n",
       "3           0   100       4\n",
       "4           0   101       2\n",
       "...       ...   ...     ...\n",
       "1144737  3599   396       4\n",
       "1144738  3599   183       3\n",
       "1144739  3599   877       3\n",
       "1144740  3599   961       5\n",
       "1144741  3599   416       4\n",
       "\n",
       "[1144742 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X contains interactions provided\n",
    "# has 1144742 rows\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b805f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---To check if number of users in X corresponds to number of rows in y---\n",
      "Number of unique users in X: 3600\n",
      "Number of rows in y: 3600\n"
     ]
    }
   ],
   "source": [
    "# y contains the noise level p\n",
    "# Has 3600 rows (900 users x 4 weeks) corresponding to users\n",
    "\n",
    "y\n",
    "\n",
    "print(\"---To check if number of users in X corresponds to number of rows in y---\")\n",
    "print(f\"Number of unique users in X: {X['user'].nunique()}\")\n",
    "print(f\"Number of rows in y: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8bed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>561</td>\n",
       "      <td>0.383316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>0.925028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>205</td>\n",
       "      <td>0.380860</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424</td>\n",
       "      <td>0.255181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>284</td>\n",
       "      <td>0.055162</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>667</td>\n",
       "      <td>0.558745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>730</td>\n",
       "      <td>0.311928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>469</td>\n",
       "      <td>0.233492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>199</td>\n",
       "      <td>0.165112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>699</td>\n",
       "      <td>0.261752</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>231</td>\n",
       "      <td>0.951103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>0.558222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>786</td>\n",
       "      <td>0.549116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>849</td>\n",
       "      <td>0.301816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>459</td>\n",
       "      <td>0.739300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1057</td>\n",
       "      <td>0.057549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1151</td>\n",
       "      <td>0.625202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1640</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1561</td>\n",
       "      <td>0.245883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1066</td>\n",
       "      <td>0.339369</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1455</td>\n",
       "      <td>0.979873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1450</td>\n",
       "      <td>0.426661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1722</td>\n",
       "      <td>0.150973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1045</td>\n",
       "      <td>0.638008</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1289</td>\n",
       "      <td>0.529647</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1279</td>\n",
       "      <td>0.099430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1617</td>\n",
       "      <td>0.903824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1563</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1682</td>\n",
       "      <td>0.587148</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1307</td>\n",
       "      <td>0.507218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2163</td>\n",
       "      <td>0.269215</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2120</td>\n",
       "      <td>0.671698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1865</td>\n",
       "      <td>0.873609</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2648</td>\n",
       "      <td>0.074802</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2548</td>\n",
       "      <td>0.698647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2458</td>\n",
       "      <td>0.441319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2093</td>\n",
       "      <td>0.805789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2328</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1903</td>\n",
       "      <td>0.520802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2595</td>\n",
       "      <td>0.440071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1818</td>\n",
       "      <td>0.580426</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2543</td>\n",
       "      <td>0.779798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1822</td>\n",
       "      <td>0.746696</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2225</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2390</td>\n",
       "      <td>0.567205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3499</td>\n",
       "      <td>0.590148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2782</td>\n",
       "      <td>0.976002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3153</td>\n",
       "      <td>0.449470</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3123</td>\n",
       "      <td>0.677369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2800</td>\n",
       "      <td>0.998705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3401</td>\n",
       "      <td>0.779507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3292</td>\n",
       "      <td>0.790033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2807</td>\n",
       "      <td>0.143894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2778</td>\n",
       "      <td>0.210702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2938</td>\n",
       "      <td>0.802037</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2980</td>\n",
       "      <td>0.779076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3576</td>\n",
       "      <td>0.403759</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3465</td>\n",
       "      <td>0.954013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2767</td>\n",
       "      <td>0.491294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3082</td>\n",
       "      <td>0.972140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user     label  anomtype\n",
       "0    561  0.383316         1\n",
       "1    202  0.925028         2\n",
       "2    205  0.380860         2\n",
       "3    424  0.255181         1\n",
       "4    284  0.055162         2\n",
       "5    667  0.558745         0\n",
       "6    730  0.311928         1\n",
       "7    469  0.233492         2\n",
       "8    199  0.165112         1\n",
       "9    699  0.261752         2\n",
       "10   231  0.951103         0\n",
       "11    26  0.558222         0\n",
       "12   786  0.549116         0\n",
       "13   849  0.301816         1\n",
       "14   459  0.739300         0\n",
       "15  1057  0.057549         0\n",
       "16  1151  0.625202         1\n",
       "17  1640  0.203252         0\n",
       "18  1561  0.245883         1\n",
       "19  1066  0.339369         2\n",
       "20  1455  0.979873         1\n",
       "21  1450  0.426661         0\n",
       "22  1722  0.150973         1\n",
       "23  1045  0.638008         2\n",
       "24  1289  0.529647         2\n",
       "25  1279  0.099430         0\n",
       "26  1617  0.903824         0\n",
       "27  1563  0.008819         2\n",
       "28  1682  0.587148         2\n",
       "29  1307  0.507218         1\n",
       "30  2163  0.269215         2\n",
       "31  2120  0.671698         0\n",
       "32  1865  0.873609         2\n",
       "33  2648  0.074802         2\n",
       "34  2548  0.698647         1\n",
       "35  2458  0.441319         1\n",
       "36  2093  0.805789         0\n",
       "37  2328  0.288185         1\n",
       "38  1903  0.520802         1\n",
       "39  2595  0.440071         0\n",
       "40  1818  0.580426         2\n",
       "41  2543  0.779798         0\n",
       "42  1822  0.746696         2\n",
       "43  2225  0.072081         1\n",
       "44  2390  0.567205         0\n",
       "45  3499  0.590148         0\n",
       "46  2782  0.976002         2\n",
       "47  3153  0.449470         2\n",
       "48  3123  0.677369         1\n",
       "49  2800  0.998705         1\n",
       "50  3401  0.779507         1\n",
       "51  3292  0.790033         1\n",
       "52  2807  0.143894         0\n",
       "53  2778  0.210702         0\n",
       "54  2938  0.802037         2\n",
       "55  2980  0.779076         0\n",
       "56  3576  0.403759         2\n",
       "57  3465  0.954013         2\n",
       "58  2767  0.491294         1\n",
       "59  3082  0.972140         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_cat contains the anomaly/noise type, which is in {0, 1, 2}\n",
    "# Only has 60 rows (15 users x 4 weeks)\n",
    "\n",
    "y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14ccea9-e508-4106-90eb-2ff8ac8dcd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>849</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3600</td>\n",
       "      <td>722</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3600</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3600</td>\n",
       "      <td>982</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600</td>\n",
       "      <td>749</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284949</th>\n",
       "      <td>4499</td>\n",
       "      <td>757</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284950</th>\n",
       "      <td>4499</td>\n",
       "      <td>752</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284951</th>\n",
       "      <td>4499</td>\n",
       "      <td>751</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284952</th>\n",
       "      <td>4499</td>\n",
       "      <td>778</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284953</th>\n",
       "      <td>4499</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284954 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  rating\n",
       "0       3600   849       5\n",
       "1       3600   722       5\n",
       "2       3600   462       4\n",
       "3       3600   982       4\n",
       "4       3600   749       4\n",
       "...      ...   ...     ...\n",
       "284949  4499   757       4\n",
       "284950  4499   752       4\n",
       "284951  4499   751       4\n",
       "284952  4499   778       4\n",
       "284953  4499     3       3\n",
       "\n",
       "[284954 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XX contains the second batch of data that we predict anomaly and noise on\n",
    "\n",
    "XX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6251994",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "For further explanation on how we decided on these features, refer to file `EDA.ipynb`.  \n",
    "*TODO: Add graphical explanations on why we chose these features in `EDA.ipynb`*   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7e3261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_components</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>cumulative_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.358422</td>\n",
       "      <td>0.358422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.436087</td>\n",
       "      <td>0.436087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.565867</td>\n",
       "      <td>0.565867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>0.670431</td>\n",
       "      <td>0.670431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>0.755216</td>\n",
       "      <td>0.755216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>0.824184</td>\n",
       "      <td>0.824184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>600</td>\n",
       "      <td>0.879620</td>\n",
       "      <td>0.879620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>650</td>\n",
       "      <td>0.902986</td>\n",
       "      <td>0.902986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>700</td>\n",
       "      <td>0.923665</td>\n",
       "      <td>0.923665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_components  explained_variance  cumulative_variance\n",
       "0            50            0.358422             0.358422\n",
       "1           100            0.436087             0.436087\n",
       "2           200            0.565867             0.565867\n",
       "3           300            0.670431             0.670431\n",
       "4           400            0.755216             0.755216\n",
       "5           500            0.824184             0.824184\n",
       "6           600            0.879620             0.879620\n",
       "7           650            0.902986             0.902986\n",
       "8           700            0.923665             0.923665"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended number of components: 600\n"
     ]
    }
   ],
   "source": [
    "# SVD Component Analysis - Find optimal number of components\n",
    "def analyze_svd_components(df_ratings, max_components=700):\n",
    "    \"\"\"\n",
    "    Analyze SVD components to find optimal dimensionality\n",
    "    Returns explained variance ratios for different component counts\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    component_range = [50, 100, 200, 300, 400, 500, 600, 650, 700]\n",
    "    \n",
    "    for n_comp in component_range:\n",
    "        if n_comp > min(df_ratings.shape):\n",
    "            continue\n",
    "            \n",
    "        svd = TruncatedSVD(n_components=n_comp, random_state=67)\n",
    "        svd.fit(df_ratings.values)\n",
    "        \n",
    "        explained_var = svd.explained_variance_ratio_.sum()\n",
    "        results.append({\n",
    "            'n_components': n_comp,\n",
    "            'explained_variance': explained_var,\n",
    "            'cumulative_variance': explained_var\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['n_components'], results_df['explained_variance'], 'o-', linewidth=2)\n",
    "    plt.axhline(y=0.85, color='r', linestyle='--', label='85% threshold')\n",
    "    plt.axhline(y=0.90, color='g', linestyle='--', label='90% threshold')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('SVD Component Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Test SVD optimization\n",
    "df_X_no_dupes = X.drop_duplicates(subset=[\"user\", \"item\"], keep=\"last\")\n",
    "df_ratings_test = df_X_no_dupes.pivot(index='user', columns='item', values='rating').fillna(-1)\n",
    "all_items = range(0, 1000)\n",
    "df_ratings_test = df_ratings_test.reindex(columns=all_items, fill_value=-1)\n",
    "\n",
    "svd_results = analyze_svd_components(df_ratings_test)\n",
    "display(svd_results)\n",
    "\n",
    "# Find optimal components (targeting ~85-90% variance explained)\n",
    "optimal_components = svd_results[svd_results['explained_variance'] >= 0.85]['n_components'].min()\n",
    "print(f\"\\nRecommended number of components: {optimal_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24361a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df_X, df_y=None, n_svd_components=300):\n",
    "    \"\"\"\n",
    "    Optimized feature engineering with configurable SVD components\n",
    "    Default reduced from 650 to 300 based on variance analysis\n",
    "    \"\"\"\n",
    "    df_X_no_dupes = df_X.drop_duplicates(subset=[\"user\", \"item\"], keep=\"last\")\n",
    "    df_ratings = df_X_no_dupes.pivot(index='user', columns='item', values='rating').fillna(-1)\n",
    "    all_items = range(0, 1000)\n",
    "    df_ratings = df_ratings.reindex(columns=all_items, fill_value=-1)\n",
    "\n",
    "    # Optimized SVD with fewer components\n",
    "    svd = TruncatedSVD(n_components=n_svd_components, random_state=67)\n",
    "    X_svd = svd.fit_transform(df_ratings.values)\n",
    "    svd_cols = [f\"svd_{i+1}\" for i in range(X_svd.shape[1])]\n",
    "    df_svd = pd.DataFrame(X_svd, columns=svd_cols, index=df_ratings.index)\n",
    "\n",
    "    # Basic user features\n",
    "    df_user_features = df_X.groupby(\"user\").agg(\n",
    "        mean_rating=(\"rating\", \"mean\"),\n",
    "        median_rating=(\"rating\", \"median\"),\n",
    "        std_rating=(\"rating\", \"std\"),\n",
    "        min_rating=(\"rating\", \"min\"),\n",
    "        max_rating=(\"rating\", \"max\"),\n",
    "        count_dislike=(\"rating\", lambda x: ((x == 0) | (x == 1) | (x == 2)).sum()),\n",
    "        count_neutral=(\"rating\", lambda x: (x == 3).sum()),\n",
    "        count_like=(\"rating\", lambda x: ((x == 4) | (x == 5)).sum()),\n",
    "        total_interactions=(\"rating\", \"count\"),\n",
    "    )\n",
    "\n",
    "    # Remove redundant rating_var (same as std_rating^2)\n",
    "    df_user_features[\"normalized_std\"] = (\n",
    "        df_user_features[\"std_rating\"] / (df_user_features[\"mean_rating\"] + 1e-5)\n",
    "    )\n",
    "\n",
    "    # Ratio features\n",
    "    df_user_features[\"like_ratio\"] = (\n",
    "        df_user_features[\"count_like\"] / df_user_features[\"total_interactions\"]\n",
    "    )\n",
    "    df_user_features[\"dislike_ratio\"] = (\n",
    "        df_user_features[\"count_dislike\"] / df_user_features[\"total_interactions\"]\n",
    "    )\n",
    "    df_user_features[\"neutral_ratio\"] = (\n",
    "        df_user_features[\"count_neutral\"] / df_user_features[\"total_interactions\"]\n",
    "    )\n",
    "\n",
    "    num_items = df_X[\"item\"].nunique()\n",
    "    df_user_features[\"interaction_ratio\"] = df_user_features[\"total_interactions\"] / num_items\n",
    "\n",
    "    # Weighted scores\n",
    "    df_user_features[\"weighted_score\"] = (\n",
    "        df_user_features[\"count_like\"] * 1.5 - df_user_features[\"count_dislike\"] * 1.5\n",
    "    )\n",
    "\n",
    "    # Distribution features\n",
    "    df_user_features[\"rating_kurtosis\"] = df_X.groupby(\"user\")[\"rating\"].apply(kurtosis)\n",
    "    df_user_features[\"rating_skewness\"] = df_X.groupby(\"user\")[\"rating\"].apply(skew)\n",
    "\n",
    "    # User bias (relative to item mean)\n",
    "    item_means = df_X.groupby(\"item\")[\"rating\"].mean().rename(\"item_mean\")\n",
    "    df_tmp = df_X.merge(item_means, on=\"item\", how=\"left\")\n",
    "    user_bias = df_tmp.groupby(\"user\").apply(\n",
    "        lambda df: (df[\"rating\"] - df[\"item_mean\"]).mean()\n",
    "    ).rename(\"user_bias\")\n",
    "    df_user_features = df_user_features.merge(user_bias, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Outliers\n",
    "    item_stats = df_X.groupby(\"item\")[\"rating\"].agg([\"mean\", \"std\"]).rename(\n",
    "        columns={\"mean\": \"item_mean\", \"std\": \"item_std\"}\n",
    "    )\n",
    "    df_tmp = df_X.merge(item_stats, on=\"item\", how=\"left\")\n",
    "    df_tmp[\"abs_dev\"] = abs(df_tmp[\"rating\"] - df_tmp[\"item_mean\"])\n",
    "    outlier_frac = (df_tmp[\"abs_dev\"] > 1.5 * df_tmp[\"item_std\"]).groupby(\n",
    "        df_tmp[\"user\"]\n",
    "    ).mean().rename(\"outlier_frac\")\n",
    "    df_user_features = df_user_features.merge(outlier_frac, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Mean item alignment\n",
    "    df_tmp[\"z_score_item\"] = (df_tmp[\"rating\"] - df_tmp[\"item_mean\"]) / (\n",
    "        df_tmp[\"item_std\"] + 1e-6\n",
    "    )\n",
    "    mean_item_alignment = df_tmp.groupby(\"user\")[\"z_score_item\"].mean().rename(\n",
    "        \"mean_item_alignment\"\n",
    "    )\n",
    "    df_user_features = df_user_features.merge(mean_item_alignment, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Entropy and extreme behaviour\n",
    "    user_entropy = df_X.groupby(\"user\")[\"rating\"].apply(\n",
    "        lambda x: entropy(x.value_counts(normalize=True))\n",
    "    ).rename(\"rating_entropy\")\n",
    "    extreme_ratio = df_X.groupby(\"user\").apply(\n",
    "        lambda df: ((df[\"rating\"] == 1) | (df[\"rating\"] == 5)).mean()\n",
    "    ).rename(\"extreme_ratio\")\n",
    "    df_user_features = df_user_features.merge(user_entropy, on=\"user\", how=\"left\")\n",
    "    df_user_features = df_user_features.merge(extreme_ratio, on=\"user\", how=\"left\")\n",
    "\n",
    "    # User mean rank\n",
    "    df_user_features[\"user_mean_rank\"] = df_user_features[\"mean_rating\"].rank(pct=True)\n",
    "\n",
    "    # Item popularity features\n",
    "    item_popularity = df_X.groupby(\"item\")[\"user\"].nunique().rename(\"item_popularity\")\n",
    "    item_rating_count = df_X.groupby(\"item\").size().rename(\"item_rating_count\")\n",
    "    df_tmp = df_X.merge(item_popularity, on=\"item\", how=\"left\")\n",
    "    df_tmp = df_tmp.merge(item_rating_count, on=\"item\", how=\"left\")\n",
    "    \n",
    "    user_avg_item_popularity = df_tmp.groupby(\"user\")[\"item_popularity\"].mean().rename(\n",
    "        \"avg_item_popularity\"\n",
    "    )\n",
    "    user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n",
    "        lambda x: (x[\"item_popularity\"] < x[\"item_popularity\"].quantile(0.25)).mean()\n",
    "    ).rename(\"rare_item_ratio\")\n",
    "    \n",
    "    df_user_features = df_user_features.merge(user_avg_item_popularity, on=\"user\", how=\"left\")\n",
    "    df_user_features = df_user_features.merge(user_rare_item_ratio, on=\"user\", how=\"left\")\n",
    "    \n",
    "    # Rating consistency features\n",
    "    user_rating_changes = df_X.sort_values([\"user\", \"item\"]).groupby(\"user\")[\"rating\"].apply(\n",
    "        lambda x: (x.diff().abs().mean()) if len(x) > 1 else 0\n",
    "    ).rename(\"rating_volatility\")\n",
    "    \n",
    "    df_user_features = df_user_features.merge(user_rating_changes, on=\"user\", how=\"left\")\n",
    "    \n",
    "    # Concentration features (Herfindahl index)\n",
    "    rating_concentration = df_X.groupby(\"user\")[\"rating\"].apply(\n",
    "        lambda x: (x.value_counts(normalize=True) ** 2).sum()\n",
    "    ).rename(\"rating_concentration\")\n",
    "    \n",
    "    df_user_features = df_user_features.merge(rating_concentration, on=\"user\", how=\"left\")\n",
    "\n",
    "    # Fill NaN values\n",
    "    df_user_features = df_user_features.fillna(0)\n",
    "\n",
    "    final_df = pd.merge(df_svd.reset_index(), df_user_features, on='user')\n",
    "    \n",
    "    if df_y is not None:\n",
    "        df_merged = pd.merge(final_df.reset_index(), df_y, on=\"user\", how=\"inner\")\n",
    "        return df_merged.drop(columns=[\"index\"]).set_index(\"user\")\n",
    "    else:\n",
    "        return final_df.set_index(\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c14227",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43bd5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print MAE for regression task\n",
    "def evaluate_linear_predictions(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# To print accuracy for classification task\n",
    "def evaluate_classification_accuracy(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed7e2b",
   "metadata": {},
   "source": [
    "## Regression Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f862e",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d26bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_1</th>\n",
       "      <th>svd_2</th>\n",
       "      <th>svd_3</th>\n",
       "      <th>svd_4</th>\n",
       "      <th>svd_5</th>\n",
       "      <th>svd_6</th>\n",
       "      <th>svd_7</th>\n",
       "      <th>svd_8</th>\n",
       "      <th>svd_9</th>\n",
       "      <th>svd_10</th>\n",
       "      <th>...</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.860749</td>\n",
       "      <td>-6.068257</td>\n",
       "      <td>-9.349063</td>\n",
       "      <td>-11.332684</td>\n",
       "      <td>3.569385</td>\n",
       "      <td>18.300837</td>\n",
       "      <td>1.165931</td>\n",
       "      <td>4.843467</td>\n",
       "      <td>5.262733</td>\n",
       "      <td>-3.331607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>-0.676871</td>\n",
       "      <td>1.407269</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>1372.925743</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>1.273632</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>0.962817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.009061</td>\n",
       "      <td>4.579251</td>\n",
       "      <td>4.523304</td>\n",
       "      <td>-10.496718</td>\n",
       "      <td>-10.419450</td>\n",
       "      <td>-9.257935</td>\n",
       "      <td>1.974163</td>\n",
       "      <td>-2.492988</td>\n",
       "      <td>-0.088541</td>\n",
       "      <td>1.393418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>-0.400218</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>0.083582</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>1358.641791</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.880240</td>\n",
       "      <td>0.354564</td>\n",
       "      <td>0.031248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.236045</td>\n",
       "      <td>3.335757</td>\n",
       "      <td>19.619654</td>\n",
       "      <td>-9.828392</td>\n",
       "      <td>11.517358</td>\n",
       "      <td>-6.641359</td>\n",
       "      <td>-2.178109</td>\n",
       "      <td>2.165167</td>\n",
       "      <td>-1.300472</td>\n",
       "      <td>6.411729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041985</td>\n",
       "      <td>0.606235</td>\n",
       "      <td>0.866574</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.777639</td>\n",
       "      <td>1373.125954</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.536398</td>\n",
       "      <td>0.507750</td>\n",
       "      <td>0.068668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.282289</td>\n",
       "      <td>7.201780</td>\n",
       "      <td>-3.125457</td>\n",
       "      <td>2.699728</td>\n",
       "      <td>-8.397937</td>\n",
       "      <td>7.214575</td>\n",
       "      <td>11.027766</td>\n",
       "      <td>-7.014426</td>\n",
       "      <td>-0.113976</td>\n",
       "      <td>-1.846197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.511695</td>\n",
       "      <td>1.374852</td>\n",
       "      <td>0.301325</td>\n",
       "      <td>0.732639</td>\n",
       "      <td>1223.599338</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>1.016611</td>\n",
       "      <td>0.300776</td>\n",
       "      <td>0.349012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35.818199</td>\n",
       "      <td>37.524453</td>\n",
       "      <td>0.308825</td>\n",
       "      <td>-7.912539</td>\n",
       "      <td>-0.218294</td>\n",
       "      <td>-0.704625</td>\n",
       "      <td>-0.070853</td>\n",
       "      <td>-0.875664</td>\n",
       "      <td>-0.148923</td>\n",
       "      <td>0.212953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437630</td>\n",
       "      <td>0.854248</td>\n",
       "      <td>0.467836</td>\n",
       "      <td>0.970278</td>\n",
       "      <td>867.903509</td>\n",
       "      <td>0.251462</td>\n",
       "      <td>0.671554</td>\n",
       "      <td>0.463784</td>\n",
       "      <td>0.917704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          svd_1      svd_2      svd_3      svd_4      svd_5      svd_6  \\\n",
       "user                                                                     \n",
       "0     14.860749  -6.068257  -9.349063 -11.332684   3.569385  18.300837   \n",
       "1     33.009061   4.579251   4.523304 -10.496718 -10.419450  -9.257935   \n",
       "2     36.236045   3.335757  19.619654  -9.828392  11.517358  -6.641359   \n",
       "3     23.282289   7.201780  -3.125457   2.699728  -8.397937   7.214575   \n",
       "4    -35.818199  37.524453   0.308825  -7.912539  -0.218294  -0.704625   \n",
       "\n",
       "          svd_7     svd_8     svd_9    svd_10  ...  outlier_frac  \\\n",
       "user                                           ...                 \n",
       "0      1.165931  4.843467  5.262733 -3.331607  ...      0.272277   \n",
       "1      1.974163 -2.492988 -0.088541  1.393418  ...      0.038806   \n",
       "2     -2.178109  2.165167 -1.300472  6.411729  ...      0.041985   \n",
       "3     11.027766 -7.014426 -0.113976 -1.846197  ...      0.211921   \n",
       "4     -0.070853 -0.875664 -0.148923  0.212953  ...      0.000000   \n",
       "\n",
       "      mean_item_alignment  rating_entropy  extreme_ratio  user_mean_rank  \\\n",
       "user                                                                       \n",
       "0               -0.676871        1.407269       0.282178        0.023889   \n",
       "1               -0.400218        1.212858       0.083582        0.132500   \n",
       "2                0.606235        0.866574       0.118321        0.777639   \n",
       "3                0.511695        1.374852       0.301325        0.732639   \n",
       "4                0.437630        0.854248       0.467836        0.970278   \n",
       "\n",
       "      avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                            \n",
       "0             1372.925743         0.252475           1.273632   \n",
       "1             1358.641791         0.250746           0.880240   \n",
       "2             1373.125954         0.251908           0.536398   \n",
       "3             1223.599338         0.251656           1.016611   \n",
       "4              867.903509         0.251462           0.671554   \n",
       "\n",
       "      rating_concentration     label  \n",
       "user                                  \n",
       "0                 0.249240  0.962817  \n",
       "1                 0.354564  0.031248  \n",
       "2                 0.507750  0.068668  \n",
       "3                 0.300776  0.349012  \n",
       "4                 0.463784  0.917704  \n",
       "\n",
       "[5 rows x 328 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the initial dataframe for linear regression, using X and y\n",
    "df_reg = engineer_features(X, y)\n",
    "df_reg.columns = df_reg.columns.astype(str)\n",
    "\n",
    "display(df_reg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007957e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (regression):\n",
      "(2880, 327) (2880,) (720, 327) (720,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare and split dataset into train and validation set\n",
    "\n",
    "# Step 1: Extract features and labels\n",
    "X_lr = df_reg.drop(columns=[\"label\"]).values # Features\n",
    "y_lr = df_reg[\"label\"].values # Labels\n",
    "\n",
    "# Step 2: Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_lr, y_lr, test_size = 0.2, random_state=67\n",
    ")\n",
    "\n",
    "print(\"Shapes (regression):\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cae6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise features (for regression)\n",
    "\n",
    "scaler_reg = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler_reg.transform(X_train)\n",
    "X_val_std = scaler_reg.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed99a0",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68aa9a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: mean\n",
      "Features selected: 86 / 327\n",
      "Validation MAE: 0.0607\n",
      "Training MAE: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: median\n",
      "Features selected: 174 / 327\n",
      "Validation MAE: 0.0619\n",
      "Training MAE: 0.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: 0.75*mean\n",
      "Features selected: 174 / 327\n",
      "Validation MAE: 0.0619\n",
      "Training MAE: 0.0212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>n_features</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>86</td>\n",
       "      <td>0.060657</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.038260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>174</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>0.040641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75*mean</td>\n",
       "      <td>174</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>0.040641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  n_features   val_mae  train_mae  overfitting\n",
       "0       mean          86  0.060657   0.022397     0.038260\n",
       "1     median         174  0.061883   0.021242     0.040641\n",
       "2  0.75*mean         174  0.061883   0.021242     0.040641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best threshold: mean\n",
      "\n",
      "Selected 86 features:\n",
      "['svd_1', 'svd_2', 'svd_3', 'svd_4', 'svd_5', 'svd_6', 'svd_7', 'svd_8', 'svd_9', 'svd_10', 'svd_11', 'svd_12', 'svd_13', 'svd_14', 'svd_15', 'svd_16', 'svd_17', 'svd_18', 'svd_19', 'svd_20'] ...\n"
     ]
    }
   ],
   "source": [
    "### Feature Selection using LGBM Importance\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Train LGBM to get feature importances\n",
    "lgbm_selector = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    learning_rate=0.05,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': df_reg.drop(columns=[\"label\"]).columns,\n",
    "    'importance': lgbm_selector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 30 features\n",
    "plt.figure(figsize=(10, 12))\n",
    "top_features = feature_importance.head(30)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 30 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select features using different thresholds\n",
    "thresholds = ['mean', 'median', '0.75*mean']\n",
    "results_comparison = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    selector = SelectFromModel(lgbm_selector, threshold=threshold, prefit=True)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Train model on selected features\n",
    "    lgbm_test = LGBMRegressor(\n",
    "        random_state=67,\n",
    "        max_depth=6,\n",
    "        num_leaves=64,\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.01,\n",
    "        reg_lambda=0.1,\n",
    "        reg_alpha=1.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbose=-1,\n",
    "        objective='mae',\n",
    "        metric='mae',\n",
    "        min_child_samples=20,\n",
    "        min_child_weight=0.001\n",
    "    )\n",
    "    \n",
    "    lgbm_test.fit(X_train_selected, y_train)\n",
    "    y_pred_val = lgbm_test.predict(X_val_selected)\n",
    "    y_pred_train = lgbm_test.predict(X_train_selected)\n",
    "    \n",
    "    mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'threshold': threshold,\n",
    "        'n_features': X_train_selected.shape[1],\n",
    "        'val_mae': mae_val,\n",
    "        'train_mae': mae_train,\n",
    "        'overfitting': mae_val - mae_train\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"Features selected: {X_train_selected.shape[1]} / {X_train.shape[1]}\")\n",
    "    print(f\"Validation MAE: {mae_val:.4f}\")\n",
    "    print(f\"Training MAE: {mae_train:.4f}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(results_comparison)\n",
    "display(comparison_df)\n",
    "\n",
    "# Choose best threshold\n",
    "best_threshold = comparison_df.loc[comparison_df['val_mae'].idxmin(), 'threshold']\n",
    "print(f\"\\nBest threshold: {best_threshold}\")\n",
    "\n",
    "# Create final selector\n",
    "selector_final = SelectFromModel(lgbm_selector, threshold=best_threshold, prefit=True)\n",
    "selected_features_mask = selector_final.get_support()\n",
    "selected_feature_names = df_reg.drop(columns=[\"label\"]).columns[selected_features_mask].tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_feature_names)} features:\")\n",
    "print(selected_feature_names[:20], \"...\")  # Show first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595e9b4",
   "metadata": {},
   "source": [
    "### Linear-based models (Linear Regression, Ridge, Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3f1adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.1072\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0910\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val_std)\n",
    "y_test = lr.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf20d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.1071\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0911\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train_std, y_train)\n",
    "y_pred = ridge.predict(X_val_std)\n",
    "y_test = ridge.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702ebd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.2079\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.2194\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_std, y_train)\n",
    "y_pred = lasso.predict(X_val_std)\n",
    "y_test = lasso.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3960ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.1183\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.1060\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (SGD) Regression\n",
    "\n",
    "sgd = SGDRegressor(random_state=67)\n",
    "sgd.fit(X_train_std, y_train)\n",
    "y_pred = sgd.predict(X_val_std)\n",
    "y_test = sgd.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01f96b",
   "metadata": {},
   "source": [
    "It appears that all linear-based models do not perform very well, which is expected as the data is not likely to follow a linear distribution.  \n",
    "We thus attempt using other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a099e",
   "metadata": {},
   "source": [
    "### Tree-based models (DecisionTree, XGBRegressor, LGBMRegressor, CatBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b885e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0946\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state=67)\n",
    "dtr.fit(X_train, y_train)\n",
    "y_pred = dtr.predict(X_val)\n",
    "y_test = dtr.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "257671c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0665\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0013\n"
     ]
    }
   ],
   "source": [
    "# XGB Regressor \n",
    "# XGB is an optimised software library that uses gradient boosted decision trees\n",
    "# We first try it without tweaking parameters\n",
    "\n",
    "xgbr = XGBRegressor(random_state=67)\n",
    "xgbr.fit(X_train, y_train)\n",
    "y_pred = xgbr.predict(X_val)\n",
    "y_test = xgbr.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecbfacd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0619\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Regressor\n",
    "# LightGBM is another well-known gradient boosting framework for efficient regression\n",
    "# We first try it without tweaking parameters\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=67)\n",
    "\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_pred = lgbm.predict(X_val)\n",
    "y_test = lgbm.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "039a612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.048391\n",
      "0:\tlearn: 0.2796492\ttotal: 81ms\tremaining: 1m 20s\n",
      "1:\tlearn: 0.2717341\ttotal: 92.7ms\tremaining: 46.3s\n",
      "2:\tlearn: 0.2638406\ttotal: 104ms\tremaining: 34.4s\n",
      "3:\tlearn: 0.2559448\ttotal: 112ms\tremaining: 28s\n",
      "4:\tlearn: 0.2484649\ttotal: 121ms\tremaining: 24.1s\n",
      "5:\tlearn: 0.2413592\ttotal: 130ms\tremaining: 21.5s\n",
      "6:\tlearn: 0.2348412\ttotal: 138ms\tremaining: 19.6s\n",
      "7:\tlearn: 0.2280789\ttotal: 150ms\tremaining: 18.5s\n",
      "8:\tlearn: 0.2220682\ttotal: 158ms\tremaining: 17.4s\n",
      "9:\tlearn: 0.2160295\ttotal: 166ms\tremaining: 16.5s\n",
      "10:\tlearn: 0.2106085\ttotal: 175ms\tremaining: 15.7s\n",
      "11:\tlearn: 0.2062345\ttotal: 185ms\tremaining: 15.2s\n",
      "12:\tlearn: 0.2015999\ttotal: 194ms\tremaining: 14.7s\n",
      "13:\tlearn: 0.1965207\ttotal: 203ms\tremaining: 14.3s\n",
      "14:\tlearn: 0.1929115\ttotal: 213ms\tremaining: 14s\n",
      "15:\tlearn: 0.1884374\ttotal: 221ms\tremaining: 13.6s\n",
      "16:\tlearn: 0.1838557\ttotal: 230ms\tremaining: 13.3s\n",
      "17:\tlearn: 0.1793986\ttotal: 238ms\tremaining: 13s\n",
      "18:\tlearn: 0.1760735\ttotal: 246ms\tremaining: 12.7s\n",
      "19:\tlearn: 0.1720324\ttotal: 255ms\tremaining: 12.5s\n",
      "20:\tlearn: 0.1688975\ttotal: 263ms\tremaining: 12.3s\n",
      "21:\tlearn: 0.1658511\ttotal: 271ms\tremaining: 12.1s\n",
      "22:\tlearn: 0.1625216\ttotal: 281ms\tremaining: 11.9s\n",
      "23:\tlearn: 0.1591889\ttotal: 290ms\tremaining: 11.8s\n",
      "24:\tlearn: 0.1562413\ttotal: 299ms\tremaining: 11.7s\n",
      "25:\tlearn: 0.1536990\ttotal: 308ms\tremaining: 11.5s\n",
      "26:\tlearn: 0.1516973\ttotal: 316ms\tremaining: 11.4s\n",
      "27:\tlearn: 0.1494671\ttotal: 324ms\tremaining: 11.3s\n",
      "28:\tlearn: 0.1471186\ttotal: 333ms\tremaining: 11.2s\n",
      "29:\tlearn: 0.1450654\ttotal: 342ms\tremaining: 11.1s\n",
      "30:\tlearn: 0.1429844\ttotal: 351ms\tremaining: 11s\n",
      "31:\tlearn: 0.1410475\ttotal: 359ms\tremaining: 10.9s\n",
      "32:\tlearn: 0.1390228\ttotal: 368ms\tremaining: 10.8s\n",
      "33:\tlearn: 0.1371954\ttotal: 377ms\tremaining: 10.7s\n",
      "34:\tlearn: 0.1351305\ttotal: 386ms\tremaining: 10.6s\n",
      "35:\tlearn: 0.1335440\ttotal: 396ms\tremaining: 10.6s\n",
      "36:\tlearn: 0.1318965\ttotal: 405ms\tremaining: 10.5s\n",
      "37:\tlearn: 0.1303552\ttotal: 414ms\tremaining: 10.5s\n",
      "38:\tlearn: 0.1291487\ttotal: 422ms\tremaining: 10.4s\n",
      "39:\tlearn: 0.1274438\ttotal: 431ms\tremaining: 10.4s\n",
      "40:\tlearn: 0.1260535\ttotal: 440ms\tremaining: 10.3s\n",
      "41:\tlearn: 0.1247716\ttotal: 449ms\tremaining: 10.2s\n",
      "42:\tlearn: 0.1236179\ttotal: 457ms\tremaining: 10.2s\n",
      "43:\tlearn: 0.1226402\ttotal: 466ms\tremaining: 10.1s\n",
      "44:\tlearn: 0.1215701\ttotal: 474ms\tremaining: 10.1s\n",
      "45:\tlearn: 0.1203675\ttotal: 483ms\tremaining: 10s\n",
      "46:\tlearn: 0.1195938\ttotal: 492ms\tremaining: 9.97s\n",
      "47:\tlearn: 0.1187127\ttotal: 503ms\tremaining: 9.98s\n",
      "48:\tlearn: 0.1174919\ttotal: 515ms\tremaining: 10s\n",
      "49:\tlearn: 0.1167512\ttotal: 525ms\tremaining: 9.98s\n",
      "50:\tlearn: 0.1158696\ttotal: 536ms\tremaining: 9.97s\n",
      "51:\tlearn: 0.1147826\ttotal: 545ms\tremaining: 9.93s\n",
      "52:\tlearn: 0.1139817\ttotal: 554ms\tremaining: 9.9s\n",
      "53:\tlearn: 0.1129191\ttotal: 562ms\tremaining: 9.85s\n",
      "54:\tlearn: 0.1121726\ttotal: 571ms\tremaining: 9.81s\n",
      "55:\tlearn: 0.1112932\ttotal: 579ms\tremaining: 9.76s\n",
      "56:\tlearn: 0.1106114\ttotal: 588ms\tremaining: 9.72s\n",
      "57:\tlearn: 0.1100918\ttotal: 597ms\tremaining: 9.7s\n",
      "58:\tlearn: 0.1096240\ttotal: 606ms\tremaining: 9.67s\n",
      "59:\tlearn: 0.1088753\ttotal: 615ms\tremaining: 9.63s\n",
      "60:\tlearn: 0.1081224\ttotal: 623ms\tremaining: 9.59s\n",
      "61:\tlearn: 0.1076059\ttotal: 631ms\tremaining: 9.55s\n",
      "62:\tlearn: 0.1069529\ttotal: 640ms\tremaining: 9.51s\n",
      "63:\tlearn: 0.1065328\ttotal: 648ms\tremaining: 9.48s\n",
      "64:\tlearn: 0.1059664\ttotal: 656ms\tremaining: 9.44s\n",
      "65:\tlearn: 0.1055840\ttotal: 665ms\tremaining: 9.41s\n",
      "66:\tlearn: 0.1051272\ttotal: 674ms\tremaining: 9.39s\n",
      "67:\tlearn: 0.1045202\ttotal: 687ms\tremaining: 9.41s\n",
      "68:\tlearn: 0.1041535\ttotal: 696ms\tremaining: 9.39s\n",
      "69:\tlearn: 0.1036290\ttotal: 705ms\tremaining: 9.37s\n",
      "70:\tlearn: 0.1031505\ttotal: 714ms\tremaining: 9.34s\n",
      "71:\tlearn: 0.1026906\ttotal: 723ms\tremaining: 9.31s\n",
      "72:\tlearn: 0.1021955\ttotal: 731ms\tremaining: 9.28s\n",
      "73:\tlearn: 0.1017806\ttotal: 740ms\tremaining: 9.26s\n",
      "74:\tlearn: 0.1013631\ttotal: 748ms\tremaining: 9.23s\n",
      "75:\tlearn: 0.1011278\ttotal: 757ms\tremaining: 9.2s\n",
      "76:\tlearn: 0.1008145\ttotal: 766ms\tremaining: 9.18s\n",
      "77:\tlearn: 0.1004587\ttotal: 774ms\tremaining: 9.15s\n",
      "78:\tlearn: 0.1001329\ttotal: 783ms\tremaining: 9.12s\n",
      "79:\tlearn: 0.0995448\ttotal: 792ms\tremaining: 9.1s\n",
      "80:\tlearn: 0.0991040\ttotal: 801ms\tremaining: 9.09s\n",
      "81:\tlearn: 0.0988536\ttotal: 810ms\tremaining: 9.06s\n",
      "82:\tlearn: 0.0986545\ttotal: 818ms\tremaining: 9.04s\n",
      "83:\tlearn: 0.0984068\ttotal: 827ms\tremaining: 9.01s\n",
      "84:\tlearn: 0.0981148\ttotal: 836ms\tremaining: 9s\n",
      "85:\tlearn: 0.0977632\ttotal: 844ms\tremaining: 8.97s\n",
      "86:\tlearn: 0.0974175\ttotal: 853ms\tremaining: 8.95s\n",
      "87:\tlearn: 0.0972023\ttotal: 862ms\tremaining: 8.93s\n",
      "88:\tlearn: 0.0969690\ttotal: 870ms\tremaining: 8.9s\n",
      "89:\tlearn: 0.0967236\ttotal: 879ms\tremaining: 8.88s\n",
      "90:\tlearn: 0.0963849\ttotal: 888ms\tremaining: 8.87s\n",
      "91:\tlearn: 0.0961837\ttotal: 897ms\tremaining: 8.86s\n",
      "92:\tlearn: 0.0960267\ttotal: 906ms\tremaining: 8.84s\n",
      "93:\tlearn: 0.0957978\ttotal: 915ms\tremaining: 8.82s\n",
      "94:\tlearn: 0.0954880\ttotal: 923ms\tremaining: 8.79s\n",
      "95:\tlearn: 0.0952443\ttotal: 932ms\tremaining: 8.77s\n",
      "96:\tlearn: 0.0950358\ttotal: 940ms\tremaining: 8.75s\n",
      "97:\tlearn: 0.0947829\ttotal: 948ms\tremaining: 8.73s\n",
      "98:\tlearn: 0.0944611\ttotal: 957ms\tremaining: 8.71s\n",
      "99:\tlearn: 0.0942527\ttotal: 965ms\tremaining: 8.69s\n",
      "100:\tlearn: 0.0940889\ttotal: 975ms\tremaining: 8.67s\n",
      "101:\tlearn: 0.0938984\ttotal: 984ms\tremaining: 8.66s\n",
      "102:\tlearn: 0.0936032\ttotal: 992ms\tremaining: 8.64s\n",
      "103:\tlearn: 0.0933751\ttotal: 1s\tremaining: 8.63s\n",
      "104:\tlearn: 0.0931447\ttotal: 1.01s\tremaining: 8.61s\n",
      "105:\tlearn: 0.0930266\ttotal: 1.02s\tremaining: 8.59s\n",
      "106:\tlearn: 0.0927367\ttotal: 1.03s\tremaining: 8.57s\n",
      "107:\tlearn: 0.0925582\ttotal: 1.03s\tremaining: 8.56s\n",
      "108:\tlearn: 0.0923028\ttotal: 1.04s\tremaining: 8.54s\n",
      "109:\tlearn: 0.0920960\ttotal: 1.05s\tremaining: 8.53s\n",
      "110:\tlearn: 0.0918336\ttotal: 1.06s\tremaining: 8.51s\n",
      "111:\tlearn: 0.0915857\ttotal: 1.07s\tremaining: 8.49s\n",
      "112:\tlearn: 0.0913993\ttotal: 1.08s\tremaining: 8.47s\n",
      "113:\tlearn: 0.0911858\ttotal: 1.09s\tremaining: 8.46s\n",
      "114:\tlearn: 0.0910319\ttotal: 1.1s\tremaining: 8.44s\n",
      "115:\tlearn: 0.0908366\ttotal: 1.1s\tremaining: 8.42s\n",
      "116:\tlearn: 0.0906689\ttotal: 1.11s\tremaining: 8.4s\n",
      "117:\tlearn: 0.0904364\ttotal: 1.12s\tremaining: 8.39s\n",
      "118:\tlearn: 0.0902511\ttotal: 1.13s\tremaining: 8.37s\n",
      "119:\tlearn: 0.0900967\ttotal: 1.14s\tremaining: 8.36s\n",
      "120:\tlearn: 0.0899656\ttotal: 1.15s\tremaining: 8.35s\n",
      "121:\tlearn: 0.0897661\ttotal: 1.16s\tremaining: 8.33s\n",
      "122:\tlearn: 0.0896061\ttotal: 1.17s\tremaining: 8.31s\n",
      "123:\tlearn: 0.0894291\ttotal: 1.17s\tremaining: 8.3s\n",
      "124:\tlearn: 0.0891684\ttotal: 1.18s\tremaining: 8.28s\n",
      "125:\tlearn: 0.0889802\ttotal: 1.19s\tremaining: 8.26s\n",
      "126:\tlearn: 0.0887620\ttotal: 1.2s\tremaining: 8.24s\n",
      "127:\tlearn: 0.0885929\ttotal: 1.21s\tremaining: 8.23s\n",
      "128:\tlearn: 0.0883154\ttotal: 1.22s\tremaining: 8.21s\n",
      "129:\tlearn: 0.0881547\ttotal: 1.23s\tremaining: 8.2s\n",
      "130:\tlearn: 0.0880435\ttotal: 1.23s\tremaining: 8.18s\n",
      "131:\tlearn: 0.0878051\ttotal: 1.24s\tremaining: 8.17s\n",
      "132:\tlearn: 0.0876229\ttotal: 1.25s\tremaining: 8.15s\n",
      "133:\tlearn: 0.0874391\ttotal: 1.26s\tremaining: 8.13s\n",
      "134:\tlearn: 0.0873096\ttotal: 1.27s\tremaining: 8.13s\n",
      "135:\tlearn: 0.0870377\ttotal: 1.28s\tremaining: 8.11s\n",
      "136:\tlearn: 0.0868233\ttotal: 1.28s\tremaining: 8.1s\n",
      "137:\tlearn: 0.0865891\ttotal: 1.29s\tremaining: 8.09s\n",
      "138:\tlearn: 0.0864559\ttotal: 1.3s\tremaining: 8.07s\n",
      "139:\tlearn: 0.0862644\ttotal: 1.31s\tremaining: 8.06s\n",
      "140:\tlearn: 0.0861084\ttotal: 1.32s\tremaining: 8.04s\n",
      "141:\tlearn: 0.0859859\ttotal: 1.33s\tremaining: 8.03s\n",
      "142:\tlearn: 0.0858417\ttotal: 1.34s\tremaining: 8.01s\n",
      "143:\tlearn: 0.0857148\ttotal: 1.34s\tremaining: 8s\n",
      "144:\tlearn: 0.0856205\ttotal: 1.35s\tremaining: 7.99s\n",
      "145:\tlearn: 0.0854388\ttotal: 1.36s\tremaining: 7.97s\n",
      "146:\tlearn: 0.0851934\ttotal: 1.37s\tremaining: 7.95s\n",
      "147:\tlearn: 0.0850249\ttotal: 1.38s\tremaining: 7.94s\n",
      "148:\tlearn: 0.0848923\ttotal: 1.39s\tremaining: 7.93s\n",
      "149:\tlearn: 0.0846865\ttotal: 1.4s\tremaining: 7.91s\n",
      "150:\tlearn: 0.0845618\ttotal: 1.41s\tremaining: 7.9s\n",
      "151:\tlearn: 0.0844065\ttotal: 1.41s\tremaining: 7.89s\n",
      "152:\tlearn: 0.0842115\ttotal: 1.42s\tremaining: 7.88s\n",
      "153:\tlearn: 0.0840968\ttotal: 1.43s\tremaining: 7.86s\n",
      "154:\tlearn: 0.0839669\ttotal: 1.44s\tremaining: 7.85s\n",
      "155:\tlearn: 0.0838084\ttotal: 1.45s\tremaining: 7.84s\n",
      "156:\tlearn: 0.0836408\ttotal: 1.46s\tremaining: 7.82s\n",
      "157:\tlearn: 0.0834439\ttotal: 1.47s\tremaining: 7.81s\n",
      "158:\tlearn: 0.0832634\ttotal: 1.47s\tremaining: 7.8s\n",
      "159:\tlearn: 0.0831459\ttotal: 1.48s\tremaining: 7.78s\n",
      "160:\tlearn: 0.0830052\ttotal: 1.49s\tremaining: 7.77s\n",
      "161:\tlearn: 0.0827769\ttotal: 1.5s\tremaining: 7.76s\n",
      "162:\tlearn: 0.0826320\ttotal: 1.51s\tremaining: 7.74s\n",
      "163:\tlearn: 0.0823653\ttotal: 1.52s\tremaining: 7.73s\n",
      "164:\tlearn: 0.0821728\ttotal: 1.52s\tremaining: 7.72s\n",
      "165:\tlearn: 0.0819960\ttotal: 1.53s\tremaining: 7.71s\n",
      "166:\tlearn: 0.0818196\ttotal: 1.54s\tremaining: 7.69s\n",
      "167:\tlearn: 0.0816426\ttotal: 1.55s\tremaining: 7.68s\n",
      "168:\tlearn: 0.0814380\ttotal: 1.56s\tremaining: 7.67s\n",
      "169:\tlearn: 0.0812477\ttotal: 1.57s\tremaining: 7.66s\n",
      "170:\tlearn: 0.0810263\ttotal: 1.58s\tremaining: 7.64s\n",
      "171:\tlearn: 0.0808316\ttotal: 1.59s\tremaining: 7.63s\n",
      "172:\tlearn: 0.0806583\ttotal: 1.59s\tremaining: 7.63s\n",
      "173:\tlearn: 0.0805060\ttotal: 1.6s\tremaining: 7.62s\n",
      "174:\tlearn: 0.0802841\ttotal: 1.61s\tremaining: 7.61s\n",
      "175:\tlearn: 0.0801757\ttotal: 1.62s\tremaining: 7.6s\n",
      "176:\tlearn: 0.0799908\ttotal: 1.63s\tremaining: 7.58s\n",
      "177:\tlearn: 0.0798171\ttotal: 1.64s\tremaining: 7.57s\n",
      "178:\tlearn: 0.0796255\ttotal: 1.65s\tremaining: 7.56s\n",
      "179:\tlearn: 0.0795151\ttotal: 1.66s\tremaining: 7.55s\n",
      "180:\tlearn: 0.0793540\ttotal: 1.67s\tremaining: 7.54s\n",
      "181:\tlearn: 0.0791531\ttotal: 1.67s\tremaining: 7.52s\n",
      "182:\tlearn: 0.0789768\ttotal: 1.68s\tremaining: 7.51s\n",
      "183:\tlearn: 0.0788446\ttotal: 1.69s\tremaining: 7.5s\n",
      "184:\tlearn: 0.0787069\ttotal: 1.7s\tremaining: 7.49s\n",
      "185:\tlearn: 0.0784924\ttotal: 1.71s\tremaining: 7.47s\n",
      "186:\tlearn: 0.0782883\ttotal: 1.72s\tremaining: 7.46s\n",
      "187:\tlearn: 0.0781682\ttotal: 1.73s\tremaining: 7.45s\n",
      "188:\tlearn: 0.0780378\ttotal: 1.74s\tremaining: 7.48s\n",
      "189:\tlearn: 0.0779546\ttotal: 1.75s\tremaining: 7.47s\n",
      "190:\tlearn: 0.0777995\ttotal: 1.76s\tremaining: 7.46s\n",
      "191:\tlearn: 0.0776697\ttotal: 1.77s\tremaining: 7.45s\n",
      "192:\tlearn: 0.0775151\ttotal: 1.78s\tremaining: 7.45s\n",
      "193:\tlearn: 0.0773354\ttotal: 1.79s\tremaining: 7.44s\n",
      "194:\tlearn: 0.0771734\ttotal: 1.8s\tremaining: 7.44s\n",
      "195:\tlearn: 0.0770474\ttotal: 1.81s\tremaining: 7.42s\n",
      "196:\tlearn: 0.0769037\ttotal: 1.82s\tremaining: 7.43s\n",
      "197:\tlearn: 0.0766956\ttotal: 1.83s\tremaining: 7.42s\n",
      "198:\tlearn: 0.0765542\ttotal: 1.84s\tremaining: 7.42s\n",
      "199:\tlearn: 0.0763517\ttotal: 1.85s\tremaining: 7.41s\n",
      "200:\tlearn: 0.0761617\ttotal: 1.86s\tremaining: 7.4s\n",
      "201:\tlearn: 0.0759968\ttotal: 1.87s\tremaining: 7.39s\n",
      "202:\tlearn: 0.0757816\ttotal: 1.88s\tremaining: 7.38s\n",
      "203:\tlearn: 0.0756666\ttotal: 1.89s\tremaining: 7.38s\n",
      "204:\tlearn: 0.0754969\ttotal: 1.9s\tremaining: 7.37s\n",
      "205:\tlearn: 0.0753381\ttotal: 1.91s\tremaining: 7.36s\n",
      "206:\tlearn: 0.0751641\ttotal: 1.92s\tremaining: 7.35s\n",
      "207:\tlearn: 0.0750043\ttotal: 1.93s\tremaining: 7.34s\n",
      "208:\tlearn: 0.0748550\ttotal: 1.94s\tremaining: 7.34s\n",
      "209:\tlearn: 0.0747334\ttotal: 1.95s\tremaining: 7.33s\n",
      "210:\tlearn: 0.0745912\ttotal: 1.96s\tremaining: 7.32s\n",
      "211:\tlearn: 0.0744341\ttotal: 1.97s\tremaining: 7.3s\n",
      "212:\tlearn: 0.0742215\ttotal: 1.97s\tremaining: 7.29s\n",
      "213:\tlearn: 0.0740666\ttotal: 1.98s\tremaining: 7.28s\n",
      "214:\tlearn: 0.0739350\ttotal: 1.99s\tremaining: 7.27s\n",
      "215:\tlearn: 0.0738180\ttotal: 2s\tremaining: 7.26s\n",
      "216:\tlearn: 0.0736779\ttotal: 2.01s\tremaining: 7.25s\n",
      "217:\tlearn: 0.0735287\ttotal: 2.02s\tremaining: 7.23s\n",
      "218:\tlearn: 0.0733762\ttotal: 2.02s\tremaining: 7.22s\n",
      "219:\tlearn: 0.0731942\ttotal: 2.03s\tremaining: 7.21s\n",
      "220:\tlearn: 0.0729664\ttotal: 2.04s\tremaining: 7.2s\n",
      "221:\tlearn: 0.0728332\ttotal: 2.05s\tremaining: 7.19s\n",
      "222:\tlearn: 0.0727046\ttotal: 2.06s\tremaining: 7.18s\n",
      "223:\tlearn: 0.0725655\ttotal: 2.07s\tremaining: 7.16s\n",
      "224:\tlearn: 0.0723735\ttotal: 2.08s\tremaining: 7.15s\n",
      "225:\tlearn: 0.0722357\ttotal: 2.08s\tremaining: 7.14s\n",
      "226:\tlearn: 0.0720153\ttotal: 2.09s\tremaining: 7.13s\n",
      "227:\tlearn: 0.0718983\ttotal: 2.1s\tremaining: 7.12s\n",
      "228:\tlearn: 0.0717422\ttotal: 2.11s\tremaining: 7.11s\n",
      "229:\tlearn: 0.0715853\ttotal: 2.12s\tremaining: 7.1s\n",
      "230:\tlearn: 0.0714357\ttotal: 2.13s\tremaining: 7.09s\n",
      "231:\tlearn: 0.0712698\ttotal: 2.14s\tremaining: 7.09s\n",
      "232:\tlearn: 0.0711378\ttotal: 2.15s\tremaining: 7.08s\n",
      "233:\tlearn: 0.0710483\ttotal: 2.16s\tremaining: 7.06s\n",
      "234:\tlearn: 0.0708979\ttotal: 2.17s\tremaining: 7.05s\n",
      "235:\tlearn: 0.0707446\ttotal: 2.18s\tremaining: 7.04s\n",
      "236:\tlearn: 0.0706294\ttotal: 2.19s\tremaining: 7.04s\n",
      "237:\tlearn: 0.0705281\ttotal: 2.19s\tremaining: 7.03s\n",
      "238:\tlearn: 0.0703842\ttotal: 2.2s\tremaining: 7.01s\n",
      "239:\tlearn: 0.0701957\ttotal: 2.21s\tremaining: 7s\n",
      "240:\tlearn: 0.0700664\ttotal: 2.22s\tremaining: 6.99s\n",
      "241:\tlearn: 0.0699138\ttotal: 2.23s\tremaining: 6.98s\n",
      "242:\tlearn: 0.0697625\ttotal: 2.24s\tremaining: 6.97s\n",
      "243:\tlearn: 0.0696305\ttotal: 2.25s\tremaining: 6.96s\n",
      "244:\tlearn: 0.0694953\ttotal: 2.25s\tremaining: 6.95s\n",
      "245:\tlearn: 0.0693611\ttotal: 2.26s\tremaining: 6.94s\n",
      "246:\tlearn: 0.0692407\ttotal: 2.27s\tremaining: 6.93s\n",
      "247:\tlearn: 0.0690954\ttotal: 2.28s\tremaining: 6.92s\n",
      "248:\tlearn: 0.0689201\ttotal: 2.29s\tremaining: 6.91s\n",
      "249:\tlearn: 0.0688048\ttotal: 2.3s\tremaining: 6.89s\n",
      "250:\tlearn: 0.0687032\ttotal: 2.31s\tremaining: 6.88s\n",
      "251:\tlearn: 0.0685846\ttotal: 2.31s\tremaining: 6.87s\n",
      "252:\tlearn: 0.0684279\ttotal: 2.32s\tremaining: 6.86s\n",
      "253:\tlearn: 0.0682790\ttotal: 2.33s\tremaining: 6.85s\n",
      "254:\tlearn: 0.0681709\ttotal: 2.34s\tremaining: 6.84s\n",
      "255:\tlearn: 0.0680451\ttotal: 2.35s\tremaining: 6.83s\n",
      "256:\tlearn: 0.0678794\ttotal: 2.36s\tremaining: 6.82s\n",
      "257:\tlearn: 0.0677308\ttotal: 2.37s\tremaining: 6.81s\n",
      "258:\tlearn: 0.0675627\ttotal: 2.38s\tremaining: 6.8s\n",
      "259:\tlearn: 0.0673906\ttotal: 2.38s\tremaining: 6.79s\n",
      "260:\tlearn: 0.0672542\ttotal: 2.39s\tremaining: 6.78s\n",
      "261:\tlearn: 0.0671371\ttotal: 2.4s\tremaining: 6.76s\n",
      "262:\tlearn: 0.0670232\ttotal: 2.41s\tremaining: 6.75s\n",
      "263:\tlearn: 0.0668970\ttotal: 2.42s\tremaining: 6.74s\n",
      "264:\tlearn: 0.0667463\ttotal: 2.43s\tremaining: 6.73s\n",
      "265:\tlearn: 0.0666400\ttotal: 2.43s\tremaining: 6.72s\n",
      "266:\tlearn: 0.0665317\ttotal: 2.44s\tremaining: 6.71s\n",
      "267:\tlearn: 0.0664052\ttotal: 2.45s\tremaining: 6.7s\n",
      "268:\tlearn: 0.0662862\ttotal: 2.46s\tremaining: 6.69s\n",
      "269:\tlearn: 0.0661192\ttotal: 2.47s\tremaining: 6.68s\n",
      "270:\tlearn: 0.0659574\ttotal: 2.48s\tremaining: 6.67s\n",
      "271:\tlearn: 0.0657994\ttotal: 2.49s\tremaining: 6.66s\n",
      "272:\tlearn: 0.0656888\ttotal: 2.5s\tremaining: 6.65s\n",
      "273:\tlearn: 0.0656006\ttotal: 2.5s\tremaining: 6.64s\n",
      "274:\tlearn: 0.0654522\ttotal: 2.51s\tremaining: 6.63s\n",
      "275:\tlearn: 0.0653422\ttotal: 2.52s\tremaining: 6.62s\n",
      "276:\tlearn: 0.0652290\ttotal: 2.53s\tremaining: 6.61s\n",
      "277:\tlearn: 0.0651181\ttotal: 2.54s\tremaining: 6.6s\n",
      "278:\tlearn: 0.0649928\ttotal: 2.55s\tremaining: 6.59s\n",
      "279:\tlearn: 0.0648324\ttotal: 2.56s\tremaining: 6.58s\n",
      "280:\tlearn: 0.0647534\ttotal: 2.56s\tremaining: 6.56s\n",
      "281:\tlearn: 0.0646442\ttotal: 2.57s\tremaining: 6.55s\n",
      "282:\tlearn: 0.0645662\ttotal: 2.58s\tremaining: 6.54s\n",
      "283:\tlearn: 0.0644558\ttotal: 2.59s\tremaining: 6.53s\n",
      "284:\tlearn: 0.0643272\ttotal: 2.6s\tremaining: 6.52s\n",
      "285:\tlearn: 0.0642303\ttotal: 2.61s\tremaining: 6.51s\n",
      "286:\tlearn: 0.0640986\ttotal: 2.62s\tremaining: 6.5s\n",
      "287:\tlearn: 0.0639975\ttotal: 2.63s\tremaining: 6.49s\n",
      "288:\tlearn: 0.0638324\ttotal: 2.63s\tremaining: 6.48s\n",
      "289:\tlearn: 0.0636942\ttotal: 2.64s\tremaining: 6.47s\n",
      "290:\tlearn: 0.0634889\ttotal: 2.65s\tremaining: 6.46s\n",
      "291:\tlearn: 0.0634029\ttotal: 2.66s\tremaining: 6.45s\n",
      "292:\tlearn: 0.0632404\ttotal: 2.67s\tremaining: 6.44s\n",
      "293:\tlearn: 0.0631257\ttotal: 2.68s\tremaining: 6.43s\n",
      "294:\tlearn: 0.0630403\ttotal: 2.69s\tremaining: 6.42s\n",
      "295:\tlearn: 0.0629572\ttotal: 2.7s\tremaining: 6.41s\n",
      "296:\tlearn: 0.0628158\ttotal: 2.71s\tremaining: 6.4s\n",
      "297:\tlearn: 0.0627283\ttotal: 2.71s\tremaining: 6.39s\n",
      "298:\tlearn: 0.0625768\ttotal: 2.72s\tremaining: 6.38s\n",
      "299:\tlearn: 0.0624135\ttotal: 2.73s\tremaining: 6.37s\n",
      "300:\tlearn: 0.0623041\ttotal: 2.74s\tremaining: 6.36s\n",
      "301:\tlearn: 0.0621560\ttotal: 2.75s\tremaining: 6.36s\n",
      "302:\tlearn: 0.0620523\ttotal: 2.76s\tremaining: 6.35s\n",
      "303:\tlearn: 0.0619657\ttotal: 2.77s\tremaining: 6.34s\n",
      "304:\tlearn: 0.0618318\ttotal: 2.78s\tremaining: 6.33s\n",
      "305:\tlearn: 0.0616937\ttotal: 2.79s\tremaining: 6.32s\n",
      "306:\tlearn: 0.0615457\ttotal: 2.79s\tremaining: 6.31s\n",
      "307:\tlearn: 0.0614152\ttotal: 2.81s\tremaining: 6.3s\n",
      "308:\tlearn: 0.0612512\ttotal: 2.81s\tremaining: 6.29s\n",
      "309:\tlearn: 0.0611516\ttotal: 2.82s\tremaining: 6.28s\n",
      "310:\tlearn: 0.0610001\ttotal: 2.83s\tremaining: 6.27s\n",
      "311:\tlearn: 0.0609312\ttotal: 2.84s\tremaining: 6.26s\n",
      "312:\tlearn: 0.0608473\ttotal: 2.85s\tremaining: 6.25s\n",
      "313:\tlearn: 0.0607506\ttotal: 2.86s\tremaining: 6.24s\n",
      "314:\tlearn: 0.0606359\ttotal: 2.87s\tremaining: 6.23s\n",
      "315:\tlearn: 0.0605288\ttotal: 2.87s\tremaining: 6.22s\n",
      "316:\tlearn: 0.0604445\ttotal: 2.88s\tremaining: 6.21s\n",
      "317:\tlearn: 0.0603662\ttotal: 2.89s\tremaining: 6.2s\n",
      "318:\tlearn: 0.0602482\ttotal: 2.9s\tremaining: 6.19s\n",
      "319:\tlearn: 0.0601161\ttotal: 2.91s\tremaining: 6.18s\n",
      "320:\tlearn: 0.0599727\ttotal: 2.92s\tremaining: 6.17s\n",
      "321:\tlearn: 0.0598481\ttotal: 2.92s\tremaining: 6.16s\n",
      "322:\tlearn: 0.0597226\ttotal: 2.93s\tremaining: 6.15s\n",
      "323:\tlearn: 0.0595809\ttotal: 2.94s\tremaining: 6.14s\n",
      "324:\tlearn: 0.0595036\ttotal: 2.95s\tremaining: 6.13s\n",
      "325:\tlearn: 0.0593887\ttotal: 2.96s\tremaining: 6.12s\n",
      "326:\tlearn: 0.0592472\ttotal: 2.97s\tremaining: 6.11s\n",
      "327:\tlearn: 0.0591728\ttotal: 2.98s\tremaining: 6.1s\n",
      "328:\tlearn: 0.0590850\ttotal: 2.98s\tremaining: 6.08s\n",
      "329:\tlearn: 0.0590020\ttotal: 2.99s\tremaining: 6.07s\n",
      "330:\tlearn: 0.0588940\ttotal: 3s\tremaining: 6.06s\n",
      "331:\tlearn: 0.0587645\ttotal: 3.01s\tremaining: 6.05s\n",
      "332:\tlearn: 0.0586040\ttotal: 3.02s\tremaining: 6.04s\n",
      "333:\tlearn: 0.0585149\ttotal: 3.02s\tremaining: 6.03s\n",
      "334:\tlearn: 0.0583928\ttotal: 3.03s\tremaining: 6.02s\n",
      "335:\tlearn: 0.0583318\ttotal: 3.04s\tremaining: 6.01s\n",
      "336:\tlearn: 0.0582323\ttotal: 3.05s\tremaining: 6s\n",
      "337:\tlearn: 0.0580990\ttotal: 3.06s\tremaining: 6s\n",
      "338:\tlearn: 0.0579668\ttotal: 3.07s\tremaining: 5.99s\n",
      "339:\tlearn: 0.0578512\ttotal: 3.08s\tremaining: 5.98s\n",
      "340:\tlearn: 0.0576962\ttotal: 3.09s\tremaining: 5.97s\n",
      "341:\tlearn: 0.0575828\ttotal: 3.1s\tremaining: 5.96s\n",
      "342:\tlearn: 0.0574579\ttotal: 3.1s\tremaining: 5.95s\n",
      "343:\tlearn: 0.0573320\ttotal: 3.11s\tremaining: 5.94s\n",
      "344:\tlearn: 0.0572168\ttotal: 3.12s\tremaining: 5.93s\n",
      "345:\tlearn: 0.0571179\ttotal: 3.13s\tremaining: 5.92s\n",
      "346:\tlearn: 0.0569650\ttotal: 3.14s\tremaining: 5.91s\n",
      "347:\tlearn: 0.0568495\ttotal: 3.15s\tremaining: 5.9s\n",
      "348:\tlearn: 0.0567541\ttotal: 3.16s\tremaining: 5.89s\n",
      "349:\tlearn: 0.0566543\ttotal: 3.16s\tremaining: 5.88s\n",
      "350:\tlearn: 0.0565375\ttotal: 3.17s\tremaining: 5.87s\n",
      "351:\tlearn: 0.0564373\ttotal: 3.18s\tremaining: 5.86s\n",
      "352:\tlearn: 0.0563386\ttotal: 3.19s\tremaining: 5.85s\n",
      "353:\tlearn: 0.0562578\ttotal: 3.2s\tremaining: 5.83s\n",
      "354:\tlearn: 0.0561411\ttotal: 3.21s\tremaining: 5.83s\n",
      "355:\tlearn: 0.0560409\ttotal: 3.21s\tremaining: 5.82s\n",
      "356:\tlearn: 0.0559620\ttotal: 3.22s\tremaining: 5.81s\n",
      "357:\tlearn: 0.0558927\ttotal: 3.23s\tremaining: 5.8s\n",
      "358:\tlearn: 0.0557768\ttotal: 3.24s\tremaining: 5.79s\n",
      "359:\tlearn: 0.0556954\ttotal: 3.25s\tremaining: 5.78s\n",
      "360:\tlearn: 0.0556020\ttotal: 3.26s\tremaining: 5.77s\n",
      "361:\tlearn: 0.0555074\ttotal: 3.27s\tremaining: 5.76s\n",
      "362:\tlearn: 0.0553914\ttotal: 3.27s\tremaining: 5.75s\n",
      "363:\tlearn: 0.0552589\ttotal: 3.28s\tremaining: 5.74s\n",
      "364:\tlearn: 0.0551328\ttotal: 3.29s\tremaining: 5.73s\n",
      "365:\tlearn: 0.0550387\ttotal: 3.3s\tremaining: 5.72s\n",
      "366:\tlearn: 0.0549585\ttotal: 3.31s\tremaining: 5.71s\n",
      "367:\tlearn: 0.0548415\ttotal: 3.32s\tremaining: 5.7s\n",
      "368:\tlearn: 0.0547242\ttotal: 3.33s\tremaining: 5.69s\n",
      "369:\tlearn: 0.0546165\ttotal: 3.33s\tremaining: 5.68s\n",
      "370:\tlearn: 0.0545353\ttotal: 3.34s\tremaining: 5.67s\n",
      "371:\tlearn: 0.0544729\ttotal: 3.35s\tremaining: 5.66s\n",
      "372:\tlearn: 0.0543939\ttotal: 3.36s\tremaining: 5.65s\n",
      "373:\tlearn: 0.0542866\ttotal: 3.37s\tremaining: 5.64s\n",
      "374:\tlearn: 0.0541854\ttotal: 3.38s\tremaining: 5.63s\n",
      "375:\tlearn: 0.0540588\ttotal: 3.39s\tremaining: 5.62s\n",
      "376:\tlearn: 0.0539932\ttotal: 3.4s\tremaining: 5.61s\n",
      "377:\tlearn: 0.0539268\ttotal: 3.4s\tremaining: 5.6s\n",
      "378:\tlearn: 0.0538838\ttotal: 3.41s\tremaining: 5.59s\n",
      "379:\tlearn: 0.0537886\ttotal: 3.42s\tremaining: 5.58s\n",
      "380:\tlearn: 0.0536932\ttotal: 3.43s\tremaining: 5.57s\n",
      "381:\tlearn: 0.0536183\ttotal: 3.44s\tremaining: 5.56s\n",
      "382:\tlearn: 0.0535242\ttotal: 3.44s\tremaining: 5.55s\n",
      "383:\tlearn: 0.0534270\ttotal: 3.45s\tremaining: 5.54s\n",
      "384:\tlearn: 0.0533084\ttotal: 3.46s\tremaining: 5.53s\n",
      "385:\tlearn: 0.0532205\ttotal: 3.47s\tremaining: 5.52s\n",
      "386:\tlearn: 0.0531483\ttotal: 3.48s\tremaining: 5.51s\n",
      "387:\tlearn: 0.0530188\ttotal: 3.49s\tremaining: 5.5s\n",
      "388:\tlearn: 0.0529262\ttotal: 3.5s\tremaining: 5.49s\n",
      "389:\tlearn: 0.0528042\ttotal: 3.5s\tremaining: 5.48s\n",
      "390:\tlearn: 0.0526988\ttotal: 3.51s\tremaining: 5.47s\n",
      "391:\tlearn: 0.0526461\ttotal: 3.52s\tremaining: 5.46s\n",
      "392:\tlearn: 0.0525910\ttotal: 3.53s\tremaining: 5.45s\n",
      "393:\tlearn: 0.0524956\ttotal: 3.54s\tremaining: 5.45s\n",
      "394:\tlearn: 0.0524070\ttotal: 3.55s\tremaining: 5.43s\n",
      "395:\tlearn: 0.0523254\ttotal: 3.56s\tremaining: 5.42s\n",
      "396:\tlearn: 0.0522280\ttotal: 3.56s\tremaining: 5.42s\n",
      "397:\tlearn: 0.0521494\ttotal: 3.57s\tremaining: 5.41s\n",
      "398:\tlearn: 0.0520588\ttotal: 3.58s\tremaining: 5.4s\n",
      "399:\tlearn: 0.0519688\ttotal: 3.59s\tremaining: 5.39s\n",
      "400:\tlearn: 0.0518967\ttotal: 3.6s\tremaining: 5.38s\n",
      "401:\tlearn: 0.0517975\ttotal: 3.61s\tremaining: 5.37s\n",
      "402:\tlearn: 0.0517180\ttotal: 3.62s\tremaining: 5.36s\n",
      "403:\tlearn: 0.0515990\ttotal: 3.63s\tremaining: 5.35s\n",
      "404:\tlearn: 0.0515146\ttotal: 3.63s\tremaining: 5.34s\n",
      "405:\tlearn: 0.0514140\ttotal: 3.64s\tremaining: 5.33s\n",
      "406:\tlearn: 0.0513128\ttotal: 3.65s\tremaining: 5.32s\n",
      "407:\tlearn: 0.0512042\ttotal: 3.66s\tremaining: 5.31s\n",
      "408:\tlearn: 0.0511292\ttotal: 3.67s\tremaining: 5.3s\n",
      "409:\tlearn: 0.0510161\ttotal: 3.68s\tremaining: 5.3s\n",
      "410:\tlearn: 0.0509142\ttotal: 3.69s\tremaining: 5.29s\n",
      "411:\tlearn: 0.0508694\ttotal: 3.7s\tremaining: 5.28s\n",
      "412:\tlearn: 0.0507661\ttotal: 3.71s\tremaining: 5.27s\n",
      "413:\tlearn: 0.0506793\ttotal: 3.71s\tremaining: 5.26s\n",
      "414:\tlearn: 0.0505958\ttotal: 3.72s\tremaining: 5.25s\n",
      "415:\tlearn: 0.0505073\ttotal: 3.73s\tremaining: 5.24s\n",
      "416:\tlearn: 0.0504345\ttotal: 3.74s\tremaining: 5.23s\n",
      "417:\tlearn: 0.0503805\ttotal: 3.75s\tremaining: 5.22s\n",
      "418:\tlearn: 0.0503020\ttotal: 3.76s\tremaining: 5.21s\n",
      "419:\tlearn: 0.0501991\ttotal: 3.77s\tremaining: 5.21s\n",
      "420:\tlearn: 0.0501037\ttotal: 3.78s\tremaining: 5.2s\n",
      "421:\tlearn: 0.0500097\ttotal: 3.79s\tremaining: 5.19s\n",
      "422:\tlearn: 0.0499326\ttotal: 3.79s\tremaining: 5.18s\n",
      "423:\tlearn: 0.0498320\ttotal: 3.8s\tremaining: 5.17s\n",
      "424:\tlearn: 0.0497608\ttotal: 3.81s\tremaining: 5.16s\n",
      "425:\tlearn: 0.0496783\ttotal: 3.82s\tremaining: 5.15s\n",
      "426:\tlearn: 0.0495563\ttotal: 3.83s\tremaining: 5.14s\n",
      "427:\tlearn: 0.0494611\ttotal: 3.84s\tremaining: 5.13s\n",
      "428:\tlearn: 0.0493881\ttotal: 3.85s\tremaining: 5.12s\n",
      "429:\tlearn: 0.0493293\ttotal: 3.85s\tremaining: 5.11s\n",
      "430:\tlearn: 0.0492373\ttotal: 3.86s\tremaining: 5.1s\n",
      "431:\tlearn: 0.0491461\ttotal: 3.87s\tremaining: 5.09s\n",
      "432:\tlearn: 0.0490518\ttotal: 3.88s\tremaining: 5.08s\n",
      "433:\tlearn: 0.0489603\ttotal: 3.89s\tremaining: 5.07s\n",
      "434:\tlearn: 0.0488698\ttotal: 3.9s\tremaining: 5.06s\n",
      "435:\tlearn: 0.0487908\ttotal: 3.9s\tremaining: 5.05s\n",
      "436:\tlearn: 0.0487067\ttotal: 3.91s\tremaining: 5.04s\n",
      "437:\tlearn: 0.0486105\ttotal: 3.92s\tremaining: 5.03s\n",
      "438:\tlearn: 0.0485173\ttotal: 3.93s\tremaining: 5.03s\n",
      "439:\tlearn: 0.0484435\ttotal: 3.95s\tremaining: 5.02s\n",
      "440:\tlearn: 0.0483495\ttotal: 3.96s\tremaining: 5.02s\n",
      "441:\tlearn: 0.0482608\ttotal: 3.96s\tremaining: 5.01s\n",
      "442:\tlearn: 0.0481457\ttotal: 3.97s\tremaining: 5s\n",
      "443:\tlearn: 0.0480790\ttotal: 3.98s\tremaining: 4.99s\n",
      "444:\tlearn: 0.0479681\ttotal: 3.99s\tremaining: 4.98s\n",
      "445:\tlearn: 0.0478721\ttotal: 4s\tremaining: 4.97s\n",
      "446:\tlearn: 0.0478019\ttotal: 4.01s\tremaining: 4.96s\n",
      "447:\tlearn: 0.0477303\ttotal: 4.02s\tremaining: 4.95s\n",
      "448:\tlearn: 0.0476673\ttotal: 4.03s\tremaining: 4.94s\n",
      "449:\tlearn: 0.0475885\ttotal: 4.03s\tremaining: 4.93s\n",
      "450:\tlearn: 0.0474703\ttotal: 4.04s\tremaining: 4.92s\n",
      "451:\tlearn: 0.0474112\ttotal: 4.05s\tremaining: 4.91s\n",
      "452:\tlearn: 0.0473013\ttotal: 4.06s\tremaining: 4.9s\n",
      "453:\tlearn: 0.0472265\ttotal: 4.07s\tremaining: 4.89s\n",
      "454:\tlearn: 0.0471580\ttotal: 4.08s\tremaining: 4.88s\n",
      "455:\tlearn: 0.0470514\ttotal: 4.08s\tremaining: 4.87s\n",
      "456:\tlearn: 0.0469727\ttotal: 4.09s\tremaining: 4.86s\n",
      "457:\tlearn: 0.0469122\ttotal: 4.1s\tremaining: 4.86s\n",
      "458:\tlearn: 0.0468415\ttotal: 4.11s\tremaining: 4.84s\n",
      "459:\tlearn: 0.0467747\ttotal: 4.12s\tremaining: 4.83s\n",
      "460:\tlearn: 0.0467074\ttotal: 4.13s\tremaining: 4.83s\n",
      "461:\tlearn: 0.0466123\ttotal: 4.14s\tremaining: 4.82s\n",
      "462:\tlearn: 0.0465399\ttotal: 4.15s\tremaining: 4.81s\n",
      "463:\tlearn: 0.0464407\ttotal: 4.16s\tremaining: 4.8s\n",
      "464:\tlearn: 0.0463394\ttotal: 4.17s\tremaining: 4.79s\n",
      "465:\tlearn: 0.0462782\ttotal: 4.17s\tremaining: 4.78s\n",
      "466:\tlearn: 0.0461665\ttotal: 4.18s\tremaining: 4.77s\n",
      "467:\tlearn: 0.0460349\ttotal: 4.19s\tremaining: 4.76s\n",
      "468:\tlearn: 0.0459467\ttotal: 4.2s\tremaining: 4.75s\n",
      "469:\tlearn: 0.0458267\ttotal: 4.21s\tremaining: 4.74s\n",
      "470:\tlearn: 0.0457396\ttotal: 4.22s\tremaining: 4.74s\n",
      "471:\tlearn: 0.0456721\ttotal: 4.22s\tremaining: 4.73s\n",
      "472:\tlearn: 0.0456102\ttotal: 4.23s\tremaining: 4.72s\n",
      "473:\tlearn: 0.0455348\ttotal: 4.24s\tremaining: 4.71s\n",
      "474:\tlearn: 0.0454378\ttotal: 4.25s\tremaining: 4.7s\n",
      "475:\tlearn: 0.0453388\ttotal: 4.26s\tremaining: 4.69s\n",
      "476:\tlearn: 0.0452532\ttotal: 4.27s\tremaining: 4.68s\n",
      "477:\tlearn: 0.0452197\ttotal: 4.28s\tremaining: 4.67s\n",
      "478:\tlearn: 0.0451179\ttotal: 4.29s\tremaining: 4.66s\n",
      "479:\tlearn: 0.0450575\ttotal: 4.29s\tremaining: 4.65s\n",
      "480:\tlearn: 0.0449719\ttotal: 4.3s\tremaining: 4.64s\n",
      "481:\tlearn: 0.0448814\ttotal: 4.31s\tremaining: 4.63s\n",
      "482:\tlearn: 0.0447857\ttotal: 4.32s\tremaining: 4.63s\n",
      "483:\tlearn: 0.0447034\ttotal: 4.33s\tremaining: 4.62s\n",
      "484:\tlearn: 0.0446215\ttotal: 4.34s\tremaining: 4.61s\n",
      "485:\tlearn: 0.0445282\ttotal: 4.35s\tremaining: 4.61s\n",
      "486:\tlearn: 0.0444599\ttotal: 4.36s\tremaining: 4.6s\n",
      "487:\tlearn: 0.0443818\ttotal: 4.37s\tremaining: 4.59s\n",
      "488:\tlearn: 0.0442954\ttotal: 4.38s\tremaining: 4.58s\n",
      "489:\tlearn: 0.0442374\ttotal: 4.39s\tremaining: 4.57s\n",
      "490:\tlearn: 0.0441702\ttotal: 4.4s\tremaining: 4.57s\n",
      "491:\tlearn: 0.0440821\ttotal: 4.41s\tremaining: 4.56s\n",
      "492:\tlearn: 0.0440080\ttotal: 4.42s\tremaining: 4.55s\n",
      "493:\tlearn: 0.0439693\ttotal: 4.43s\tremaining: 4.54s\n",
      "494:\tlearn: 0.0439040\ttotal: 4.44s\tremaining: 4.53s\n",
      "495:\tlearn: 0.0438296\ttotal: 4.45s\tremaining: 4.52s\n",
      "496:\tlearn: 0.0437593\ttotal: 4.46s\tremaining: 4.51s\n",
      "497:\tlearn: 0.0436915\ttotal: 4.47s\tremaining: 4.51s\n",
      "498:\tlearn: 0.0435998\ttotal: 4.48s\tremaining: 4.5s\n",
      "499:\tlearn: 0.0435176\ttotal: 4.49s\tremaining: 4.49s\n",
      "500:\tlearn: 0.0434349\ttotal: 4.5s\tremaining: 4.48s\n",
      "501:\tlearn: 0.0433769\ttotal: 4.51s\tremaining: 4.47s\n",
      "502:\tlearn: 0.0433072\ttotal: 4.52s\tremaining: 4.46s\n",
      "503:\tlearn: 0.0432257\ttotal: 4.53s\tremaining: 4.46s\n",
      "504:\tlearn: 0.0431368\ttotal: 4.54s\tremaining: 4.45s\n",
      "505:\tlearn: 0.0430332\ttotal: 4.55s\tremaining: 4.44s\n",
      "506:\tlearn: 0.0429528\ttotal: 4.56s\tremaining: 4.43s\n",
      "507:\tlearn: 0.0428935\ttotal: 4.56s\tremaining: 4.42s\n",
      "508:\tlearn: 0.0428323\ttotal: 4.57s\tremaining: 4.41s\n",
      "509:\tlearn: 0.0427497\ttotal: 4.58s\tremaining: 4.4s\n",
      "510:\tlearn: 0.0426884\ttotal: 4.59s\tremaining: 4.4s\n",
      "511:\tlearn: 0.0425938\ttotal: 4.6s\tremaining: 4.39s\n",
      "512:\tlearn: 0.0425184\ttotal: 4.61s\tremaining: 4.38s\n",
      "513:\tlearn: 0.0424377\ttotal: 4.62s\tremaining: 4.37s\n",
      "514:\tlearn: 0.0423184\ttotal: 4.63s\tremaining: 4.36s\n",
      "515:\tlearn: 0.0422525\ttotal: 4.64s\tremaining: 4.35s\n",
      "516:\tlearn: 0.0421603\ttotal: 4.65s\tremaining: 4.34s\n",
      "517:\tlearn: 0.0421056\ttotal: 4.66s\tremaining: 4.33s\n",
      "518:\tlearn: 0.0420208\ttotal: 4.67s\tremaining: 4.32s\n",
      "519:\tlearn: 0.0419637\ttotal: 4.67s\tremaining: 4.31s\n",
      "520:\tlearn: 0.0418970\ttotal: 4.68s\tremaining: 4.31s\n",
      "521:\tlearn: 0.0418213\ttotal: 4.69s\tremaining: 4.3s\n",
      "522:\tlearn: 0.0417550\ttotal: 4.7s\tremaining: 4.29s\n",
      "523:\tlearn: 0.0416477\ttotal: 4.73s\tremaining: 4.29s\n",
      "524:\tlearn: 0.0415625\ttotal: 4.74s\tremaining: 4.29s\n",
      "525:\tlearn: 0.0415101\ttotal: 4.76s\tremaining: 4.29s\n",
      "526:\tlearn: 0.0414400\ttotal: 4.78s\tremaining: 4.29s\n",
      "527:\tlearn: 0.0413734\ttotal: 4.8s\tremaining: 4.29s\n",
      "528:\tlearn: 0.0412935\ttotal: 4.81s\tremaining: 4.28s\n",
      "529:\tlearn: 0.0412246\ttotal: 4.82s\tremaining: 4.28s\n",
      "530:\tlearn: 0.0411578\ttotal: 4.83s\tremaining: 4.27s\n",
      "531:\tlearn: 0.0410747\ttotal: 4.84s\tremaining: 4.26s\n",
      "532:\tlearn: 0.0410403\ttotal: 4.86s\tremaining: 4.26s\n",
      "533:\tlearn: 0.0409678\ttotal: 4.87s\tremaining: 4.25s\n",
      "534:\tlearn: 0.0409122\ttotal: 4.88s\tremaining: 4.24s\n",
      "535:\tlearn: 0.0408266\ttotal: 4.89s\tremaining: 4.23s\n",
      "536:\tlearn: 0.0407471\ttotal: 4.9s\tremaining: 4.22s\n",
      "537:\tlearn: 0.0406858\ttotal: 4.91s\tremaining: 4.21s\n",
      "538:\tlearn: 0.0406287\ttotal: 4.91s\tremaining: 4.2s\n",
      "539:\tlearn: 0.0405706\ttotal: 4.92s\tremaining: 4.19s\n",
      "540:\tlearn: 0.0405114\ttotal: 4.93s\tremaining: 4.18s\n",
      "541:\tlearn: 0.0404651\ttotal: 4.94s\tremaining: 4.17s\n",
      "542:\tlearn: 0.0404008\ttotal: 4.95s\tremaining: 4.17s\n",
      "543:\tlearn: 0.0403404\ttotal: 4.96s\tremaining: 4.16s\n",
      "544:\tlearn: 0.0402633\ttotal: 4.97s\tremaining: 4.15s\n",
      "545:\tlearn: 0.0401670\ttotal: 4.97s\tremaining: 4.14s\n",
      "546:\tlearn: 0.0401032\ttotal: 4.98s\tremaining: 4.13s\n",
      "547:\tlearn: 0.0400074\ttotal: 4.99s\tremaining: 4.12s\n",
      "548:\tlearn: 0.0399520\ttotal: 5s\tremaining: 4.11s\n",
      "549:\tlearn: 0.0398866\ttotal: 5.01s\tremaining: 4.1s\n",
      "550:\tlearn: 0.0398065\ttotal: 5.02s\tremaining: 4.09s\n",
      "551:\tlearn: 0.0397165\ttotal: 5.03s\tremaining: 4.08s\n",
      "552:\tlearn: 0.0396383\ttotal: 5.03s\tremaining: 4.07s\n",
      "553:\tlearn: 0.0395923\ttotal: 5.04s\tremaining: 4.06s\n",
      "554:\tlearn: 0.0395259\ttotal: 5.05s\tremaining: 4.05s\n",
      "555:\tlearn: 0.0394777\ttotal: 5.06s\tremaining: 4.04s\n",
      "556:\tlearn: 0.0394114\ttotal: 5.07s\tremaining: 4.03s\n",
      "557:\tlearn: 0.0393168\ttotal: 5.08s\tremaining: 4.02s\n",
      "558:\tlearn: 0.0392348\ttotal: 5.09s\tremaining: 4.01s\n",
      "559:\tlearn: 0.0391644\ttotal: 5.09s\tremaining: 4s\n",
      "560:\tlearn: 0.0391209\ttotal: 5.1s\tremaining: 3.99s\n",
      "561:\tlearn: 0.0390789\ttotal: 5.11s\tremaining: 3.98s\n",
      "562:\tlearn: 0.0390060\ttotal: 5.12s\tremaining: 3.98s\n",
      "563:\tlearn: 0.0389377\ttotal: 5.13s\tremaining: 3.97s\n",
      "564:\tlearn: 0.0388710\ttotal: 5.14s\tremaining: 3.96s\n",
      "565:\tlearn: 0.0388144\ttotal: 5.15s\tremaining: 3.95s\n",
      "566:\tlearn: 0.0387489\ttotal: 5.16s\tremaining: 3.94s\n",
      "567:\tlearn: 0.0386796\ttotal: 5.17s\tremaining: 3.93s\n",
      "568:\tlearn: 0.0386065\ttotal: 5.17s\tremaining: 3.92s\n",
      "569:\tlearn: 0.0385426\ttotal: 5.18s\tremaining: 3.91s\n",
      "570:\tlearn: 0.0384604\ttotal: 5.19s\tremaining: 3.9s\n",
      "571:\tlearn: 0.0383880\ttotal: 5.2s\tremaining: 3.89s\n",
      "572:\tlearn: 0.0382989\ttotal: 5.21s\tremaining: 3.88s\n",
      "573:\tlearn: 0.0382180\ttotal: 5.22s\tremaining: 3.87s\n",
      "574:\tlearn: 0.0381749\ttotal: 5.23s\tremaining: 3.86s\n",
      "575:\tlearn: 0.0381205\ttotal: 5.24s\tremaining: 3.85s\n",
      "576:\tlearn: 0.0380664\ttotal: 5.24s\tremaining: 3.84s\n",
      "577:\tlearn: 0.0380000\ttotal: 5.25s\tremaining: 3.83s\n",
      "578:\tlearn: 0.0379225\ttotal: 5.26s\tremaining: 3.83s\n",
      "579:\tlearn: 0.0378442\ttotal: 5.27s\tremaining: 3.81s\n",
      "580:\tlearn: 0.0377391\ttotal: 5.28s\tremaining: 3.81s\n",
      "581:\tlearn: 0.0376608\ttotal: 5.29s\tremaining: 3.8s\n",
      "582:\tlearn: 0.0375642\ttotal: 5.29s\tremaining: 3.79s\n",
      "583:\tlearn: 0.0375240\ttotal: 5.3s\tremaining: 3.78s\n",
      "584:\tlearn: 0.0374388\ttotal: 5.31s\tremaining: 3.77s\n",
      "585:\tlearn: 0.0373556\ttotal: 5.32s\tremaining: 3.76s\n",
      "586:\tlearn: 0.0372669\ttotal: 5.33s\tremaining: 3.75s\n",
      "587:\tlearn: 0.0371899\ttotal: 5.34s\tremaining: 3.74s\n",
      "588:\tlearn: 0.0371153\ttotal: 5.35s\tremaining: 3.73s\n",
      "589:\tlearn: 0.0370315\ttotal: 5.36s\tremaining: 3.72s\n",
      "590:\tlearn: 0.0369873\ttotal: 5.37s\tremaining: 3.71s\n",
      "591:\tlearn: 0.0369174\ttotal: 5.38s\tremaining: 3.7s\n",
      "592:\tlearn: 0.0368632\ttotal: 5.38s\tremaining: 3.69s\n",
      "593:\tlearn: 0.0368133\ttotal: 5.39s\tremaining: 3.69s\n",
      "594:\tlearn: 0.0367367\ttotal: 5.4s\tremaining: 3.68s\n",
      "595:\tlearn: 0.0366756\ttotal: 5.41s\tremaining: 3.67s\n",
      "596:\tlearn: 0.0366375\ttotal: 5.42s\tremaining: 3.66s\n",
      "597:\tlearn: 0.0365753\ttotal: 5.43s\tremaining: 3.65s\n",
      "598:\tlearn: 0.0364952\ttotal: 5.44s\tremaining: 3.64s\n",
      "599:\tlearn: 0.0364164\ttotal: 5.45s\tremaining: 3.63s\n",
      "600:\tlearn: 0.0363551\ttotal: 5.46s\tremaining: 3.62s\n",
      "601:\tlearn: 0.0362908\ttotal: 5.46s\tremaining: 3.61s\n",
      "602:\tlearn: 0.0362368\ttotal: 5.47s\tremaining: 3.6s\n",
      "603:\tlearn: 0.0361592\ttotal: 5.48s\tremaining: 3.59s\n",
      "604:\tlearn: 0.0360850\ttotal: 5.49s\tremaining: 3.58s\n",
      "605:\tlearn: 0.0360332\ttotal: 5.5s\tremaining: 3.58s\n",
      "606:\tlearn: 0.0359742\ttotal: 5.51s\tremaining: 3.57s\n",
      "607:\tlearn: 0.0359197\ttotal: 5.52s\tremaining: 3.56s\n",
      "608:\tlearn: 0.0358710\ttotal: 5.53s\tremaining: 3.55s\n",
      "609:\tlearn: 0.0358124\ttotal: 5.53s\tremaining: 3.54s\n",
      "610:\tlearn: 0.0357405\ttotal: 5.54s\tremaining: 3.53s\n",
      "611:\tlearn: 0.0356538\ttotal: 5.55s\tremaining: 3.52s\n",
      "612:\tlearn: 0.0355945\ttotal: 5.56s\tremaining: 3.51s\n",
      "613:\tlearn: 0.0355374\ttotal: 5.57s\tremaining: 3.5s\n",
      "614:\tlearn: 0.0354806\ttotal: 5.58s\tremaining: 3.49s\n",
      "615:\tlearn: 0.0354409\ttotal: 5.58s\tremaining: 3.48s\n",
      "616:\tlearn: 0.0353785\ttotal: 5.59s\tremaining: 3.47s\n",
      "617:\tlearn: 0.0352978\ttotal: 5.6s\tremaining: 3.46s\n",
      "618:\tlearn: 0.0352164\ttotal: 5.61s\tremaining: 3.45s\n",
      "619:\tlearn: 0.0351618\ttotal: 5.62s\tremaining: 3.44s\n",
      "620:\tlearn: 0.0351336\ttotal: 5.63s\tremaining: 3.44s\n",
      "621:\tlearn: 0.0350834\ttotal: 5.64s\tremaining: 3.42s\n",
      "622:\tlearn: 0.0349883\ttotal: 5.64s\tremaining: 3.42s\n",
      "623:\tlearn: 0.0349262\ttotal: 5.65s\tremaining: 3.41s\n",
      "624:\tlearn: 0.0348313\ttotal: 5.66s\tremaining: 3.4s\n",
      "625:\tlearn: 0.0347870\ttotal: 5.67s\tremaining: 3.39s\n",
      "626:\tlearn: 0.0347186\ttotal: 5.68s\tremaining: 3.38s\n",
      "627:\tlearn: 0.0346715\ttotal: 5.69s\tremaining: 3.37s\n",
      "628:\tlearn: 0.0346011\ttotal: 5.7s\tremaining: 3.36s\n",
      "629:\tlearn: 0.0345204\ttotal: 5.71s\tremaining: 3.35s\n",
      "630:\tlearn: 0.0344523\ttotal: 5.72s\tremaining: 3.34s\n",
      "631:\tlearn: 0.0344000\ttotal: 5.73s\tremaining: 3.34s\n",
      "632:\tlearn: 0.0343533\ttotal: 5.74s\tremaining: 3.33s\n",
      "633:\tlearn: 0.0342978\ttotal: 5.75s\tremaining: 3.32s\n",
      "634:\tlearn: 0.0342428\ttotal: 5.76s\tremaining: 3.31s\n",
      "635:\tlearn: 0.0341975\ttotal: 5.77s\tremaining: 3.3s\n",
      "636:\tlearn: 0.0341505\ttotal: 5.78s\tremaining: 3.29s\n",
      "637:\tlearn: 0.0340951\ttotal: 5.78s\tremaining: 3.28s\n",
      "638:\tlearn: 0.0340610\ttotal: 5.79s\tremaining: 3.27s\n",
      "639:\tlearn: 0.0340052\ttotal: 5.8s\tremaining: 3.26s\n",
      "640:\tlearn: 0.0339363\ttotal: 5.81s\tremaining: 3.25s\n",
      "641:\tlearn: 0.0338793\ttotal: 5.82s\tremaining: 3.24s\n",
      "642:\tlearn: 0.0338425\ttotal: 5.83s\tremaining: 3.23s\n",
      "643:\tlearn: 0.0337992\ttotal: 5.84s\tremaining: 3.23s\n",
      "644:\tlearn: 0.0337597\ttotal: 5.85s\tremaining: 3.22s\n",
      "645:\tlearn: 0.0336959\ttotal: 5.86s\tremaining: 3.21s\n",
      "646:\tlearn: 0.0336486\ttotal: 5.87s\tremaining: 3.2s\n",
      "647:\tlearn: 0.0335867\ttotal: 5.87s\tremaining: 3.19s\n",
      "648:\tlearn: 0.0335463\ttotal: 5.88s\tremaining: 3.18s\n",
      "649:\tlearn: 0.0334869\ttotal: 5.89s\tremaining: 3.17s\n",
      "650:\tlearn: 0.0334352\ttotal: 5.9s\tremaining: 3.16s\n",
      "651:\tlearn: 0.0333622\ttotal: 5.91s\tremaining: 3.15s\n",
      "652:\tlearn: 0.0333245\ttotal: 5.92s\tremaining: 3.15s\n",
      "653:\tlearn: 0.0332756\ttotal: 5.93s\tremaining: 3.14s\n",
      "654:\tlearn: 0.0332232\ttotal: 5.94s\tremaining: 3.13s\n",
      "655:\tlearn: 0.0331526\ttotal: 5.95s\tremaining: 3.12s\n",
      "656:\tlearn: 0.0330876\ttotal: 5.96s\tremaining: 3.11s\n",
      "657:\tlearn: 0.0330187\ttotal: 5.97s\tremaining: 3.1s\n",
      "658:\tlearn: 0.0329628\ttotal: 5.98s\tremaining: 3.09s\n",
      "659:\tlearn: 0.0329197\ttotal: 5.99s\tremaining: 3.08s\n",
      "660:\tlearn: 0.0328636\ttotal: 6s\tremaining: 3.07s\n",
      "661:\tlearn: 0.0327968\ttotal: 6s\tremaining: 3.07s\n",
      "662:\tlearn: 0.0327266\ttotal: 6.01s\tremaining: 3.06s\n",
      "663:\tlearn: 0.0326828\ttotal: 6.02s\tremaining: 3.05s\n",
      "664:\tlearn: 0.0326063\ttotal: 6.03s\tremaining: 3.04s\n",
      "665:\tlearn: 0.0325550\ttotal: 6.04s\tremaining: 3.03s\n",
      "666:\tlearn: 0.0325112\ttotal: 6.05s\tremaining: 3.02s\n",
      "667:\tlearn: 0.0324519\ttotal: 6.06s\tremaining: 3.01s\n",
      "668:\tlearn: 0.0324058\ttotal: 6.07s\tremaining: 3s\n",
      "669:\tlearn: 0.0323586\ttotal: 6.07s\tremaining: 2.99s\n",
      "670:\tlearn: 0.0323140\ttotal: 6.08s\tremaining: 2.98s\n",
      "671:\tlearn: 0.0322654\ttotal: 6.09s\tremaining: 2.97s\n",
      "672:\tlearn: 0.0322122\ttotal: 6.1s\tremaining: 2.96s\n",
      "673:\tlearn: 0.0321596\ttotal: 6.11s\tremaining: 2.96s\n",
      "674:\tlearn: 0.0321233\ttotal: 6.12s\tremaining: 2.94s\n",
      "675:\tlearn: 0.0320713\ttotal: 6.13s\tremaining: 2.94s\n",
      "676:\tlearn: 0.0320180\ttotal: 6.13s\tremaining: 2.93s\n",
      "677:\tlearn: 0.0319613\ttotal: 6.14s\tremaining: 2.92s\n",
      "678:\tlearn: 0.0318941\ttotal: 6.15s\tremaining: 2.91s\n",
      "679:\tlearn: 0.0318348\ttotal: 6.17s\tremaining: 2.9s\n",
      "680:\tlearn: 0.0317918\ttotal: 6.18s\tremaining: 2.89s\n",
      "681:\tlearn: 0.0317421\ttotal: 6.19s\tremaining: 2.88s\n",
      "682:\tlearn: 0.0317081\ttotal: 6.2s\tremaining: 2.88s\n",
      "683:\tlearn: 0.0316685\ttotal: 6.21s\tremaining: 2.87s\n",
      "684:\tlearn: 0.0316220\ttotal: 6.21s\tremaining: 2.86s\n",
      "685:\tlearn: 0.0315724\ttotal: 6.22s\tremaining: 2.85s\n",
      "686:\tlearn: 0.0315123\ttotal: 6.23s\tremaining: 2.84s\n",
      "687:\tlearn: 0.0314572\ttotal: 6.24s\tremaining: 2.83s\n",
      "688:\tlearn: 0.0313938\ttotal: 6.25s\tremaining: 2.82s\n",
      "689:\tlearn: 0.0313603\ttotal: 6.26s\tremaining: 2.81s\n",
      "690:\tlearn: 0.0313277\ttotal: 6.27s\tremaining: 2.8s\n",
      "691:\tlearn: 0.0312744\ttotal: 6.27s\tremaining: 2.79s\n",
      "692:\tlearn: 0.0312335\ttotal: 6.28s\tremaining: 2.78s\n",
      "693:\tlearn: 0.0312087\ttotal: 6.29s\tremaining: 2.77s\n",
      "694:\tlearn: 0.0311576\ttotal: 6.3s\tremaining: 2.77s\n",
      "695:\tlearn: 0.0311201\ttotal: 6.31s\tremaining: 2.76s\n",
      "696:\tlearn: 0.0310673\ttotal: 6.32s\tremaining: 2.75s\n",
      "697:\tlearn: 0.0310240\ttotal: 6.33s\tremaining: 2.74s\n",
      "698:\tlearn: 0.0309464\ttotal: 6.34s\tremaining: 2.73s\n",
      "699:\tlearn: 0.0308921\ttotal: 6.35s\tremaining: 2.72s\n",
      "700:\tlearn: 0.0308301\ttotal: 6.36s\tremaining: 2.71s\n",
      "701:\tlearn: 0.0307716\ttotal: 6.36s\tremaining: 2.7s\n",
      "702:\tlearn: 0.0307370\ttotal: 6.37s\tremaining: 2.69s\n",
      "703:\tlearn: 0.0306825\ttotal: 6.38s\tremaining: 2.68s\n",
      "704:\tlearn: 0.0306262\ttotal: 6.39s\tremaining: 2.67s\n",
      "705:\tlearn: 0.0305702\ttotal: 6.4s\tremaining: 2.66s\n",
      "706:\tlearn: 0.0305043\ttotal: 6.41s\tremaining: 2.65s\n",
      "707:\tlearn: 0.0304344\ttotal: 6.42s\tremaining: 2.65s\n",
      "708:\tlearn: 0.0303900\ttotal: 6.42s\tremaining: 2.64s\n",
      "709:\tlearn: 0.0303255\ttotal: 6.43s\tremaining: 2.63s\n",
      "710:\tlearn: 0.0302764\ttotal: 6.44s\tremaining: 2.62s\n",
      "711:\tlearn: 0.0302293\ttotal: 6.45s\tremaining: 2.61s\n",
      "712:\tlearn: 0.0301556\ttotal: 6.46s\tremaining: 2.6s\n",
      "713:\tlearn: 0.0300892\ttotal: 6.47s\tremaining: 2.59s\n",
      "714:\tlearn: 0.0300384\ttotal: 6.48s\tremaining: 2.58s\n",
      "715:\tlearn: 0.0299878\ttotal: 6.49s\tremaining: 2.57s\n",
      "716:\tlearn: 0.0299238\ttotal: 6.5s\tremaining: 2.56s\n",
      "717:\tlearn: 0.0298676\ttotal: 6.5s\tremaining: 2.55s\n",
      "718:\tlearn: 0.0298361\ttotal: 6.51s\tremaining: 2.54s\n",
      "719:\tlearn: 0.0297731\ttotal: 6.52s\tremaining: 2.54s\n",
      "720:\tlearn: 0.0297255\ttotal: 6.53s\tremaining: 2.53s\n",
      "721:\tlearn: 0.0296860\ttotal: 6.54s\tremaining: 2.52s\n",
      "722:\tlearn: 0.0296181\ttotal: 6.55s\tremaining: 2.51s\n",
      "723:\tlearn: 0.0295637\ttotal: 6.56s\tremaining: 2.5s\n",
      "724:\tlearn: 0.0295147\ttotal: 6.56s\tremaining: 2.49s\n",
      "725:\tlearn: 0.0294723\ttotal: 6.57s\tremaining: 2.48s\n",
      "726:\tlearn: 0.0294402\ttotal: 6.58s\tremaining: 2.47s\n",
      "727:\tlearn: 0.0294068\ttotal: 6.59s\tremaining: 2.46s\n",
      "728:\tlearn: 0.0293586\ttotal: 6.6s\tremaining: 2.45s\n",
      "729:\tlearn: 0.0293110\ttotal: 6.61s\tremaining: 2.44s\n",
      "730:\tlearn: 0.0292554\ttotal: 6.62s\tremaining: 2.44s\n",
      "731:\tlearn: 0.0292126\ttotal: 6.63s\tremaining: 2.43s\n",
      "732:\tlearn: 0.0291639\ttotal: 6.64s\tremaining: 2.42s\n",
      "733:\tlearn: 0.0291301\ttotal: 6.65s\tremaining: 2.41s\n",
      "734:\tlearn: 0.0290866\ttotal: 6.66s\tremaining: 2.4s\n",
      "735:\tlearn: 0.0290373\ttotal: 6.66s\tremaining: 2.39s\n",
      "736:\tlearn: 0.0290060\ttotal: 6.67s\tremaining: 2.38s\n",
      "737:\tlearn: 0.0289577\ttotal: 6.68s\tremaining: 2.37s\n",
      "738:\tlearn: 0.0289115\ttotal: 6.69s\tremaining: 2.36s\n",
      "739:\tlearn: 0.0288811\ttotal: 6.7s\tremaining: 2.35s\n",
      "740:\tlearn: 0.0288300\ttotal: 6.71s\tremaining: 2.34s\n",
      "741:\tlearn: 0.0287624\ttotal: 6.71s\tremaining: 2.33s\n",
      "742:\tlearn: 0.0287221\ttotal: 6.72s\tremaining: 2.33s\n",
      "743:\tlearn: 0.0286593\ttotal: 6.73s\tremaining: 2.32s\n",
      "744:\tlearn: 0.0286179\ttotal: 6.74s\tremaining: 2.31s\n",
      "745:\tlearn: 0.0285907\ttotal: 6.75s\tremaining: 2.3s\n",
      "746:\tlearn: 0.0285460\ttotal: 6.76s\tremaining: 2.29s\n",
      "747:\tlearn: 0.0285149\ttotal: 6.77s\tremaining: 2.28s\n",
      "748:\tlearn: 0.0284630\ttotal: 6.78s\tremaining: 2.27s\n",
      "749:\tlearn: 0.0284223\ttotal: 6.78s\tremaining: 2.26s\n",
      "750:\tlearn: 0.0283726\ttotal: 6.79s\tremaining: 2.25s\n",
      "751:\tlearn: 0.0283404\ttotal: 6.8s\tremaining: 2.24s\n",
      "752:\tlearn: 0.0283043\ttotal: 6.81s\tremaining: 2.23s\n",
      "753:\tlearn: 0.0282575\ttotal: 6.82s\tremaining: 2.22s\n",
      "754:\tlearn: 0.0282247\ttotal: 6.83s\tremaining: 2.21s\n",
      "755:\tlearn: 0.0281828\ttotal: 6.83s\tremaining: 2.21s\n",
      "756:\tlearn: 0.0281341\ttotal: 6.84s\tremaining: 2.2s\n",
      "757:\tlearn: 0.0280839\ttotal: 6.85s\tremaining: 2.19s\n",
      "758:\tlearn: 0.0280538\ttotal: 6.86s\tremaining: 2.18s\n",
      "759:\tlearn: 0.0279987\ttotal: 6.87s\tremaining: 2.17s\n",
      "760:\tlearn: 0.0279512\ttotal: 6.88s\tremaining: 2.16s\n",
      "761:\tlearn: 0.0278890\ttotal: 6.89s\tremaining: 2.15s\n",
      "762:\tlearn: 0.0278525\ttotal: 6.9s\tremaining: 2.14s\n",
      "763:\tlearn: 0.0278208\ttotal: 6.91s\tremaining: 2.13s\n",
      "764:\tlearn: 0.0277904\ttotal: 6.91s\tremaining: 2.12s\n",
      "765:\tlearn: 0.0277625\ttotal: 6.92s\tremaining: 2.11s\n",
      "766:\tlearn: 0.0277294\ttotal: 6.93s\tremaining: 2.1s\n",
      "767:\tlearn: 0.0276838\ttotal: 6.94s\tremaining: 2.1s\n",
      "768:\tlearn: 0.0276456\ttotal: 6.95s\tremaining: 2.09s\n",
      "769:\tlearn: 0.0275973\ttotal: 6.96s\tremaining: 2.08s\n",
      "770:\tlearn: 0.0275502\ttotal: 6.96s\tremaining: 2.07s\n",
      "771:\tlearn: 0.0275210\ttotal: 6.97s\tremaining: 2.06s\n",
      "772:\tlearn: 0.0274612\ttotal: 6.98s\tremaining: 2.05s\n",
      "773:\tlearn: 0.0273917\ttotal: 6.99s\tremaining: 2.04s\n",
      "774:\tlearn: 0.0273505\ttotal: 7s\tremaining: 2.03s\n",
      "775:\tlearn: 0.0272952\ttotal: 7.01s\tremaining: 2.02s\n",
      "776:\tlearn: 0.0272407\ttotal: 7.02s\tremaining: 2.01s\n",
      "777:\tlearn: 0.0272148\ttotal: 7.03s\tremaining: 2s\n",
      "778:\tlearn: 0.0271814\ttotal: 7.03s\tremaining: 2s\n",
      "779:\tlearn: 0.0271439\ttotal: 7.04s\tremaining: 1.99s\n",
      "780:\tlearn: 0.0271057\ttotal: 7.05s\tremaining: 1.98s\n",
      "781:\tlearn: 0.0270471\ttotal: 7.06s\tremaining: 1.97s\n",
      "782:\tlearn: 0.0270132\ttotal: 7.07s\tremaining: 1.96s\n",
      "783:\tlearn: 0.0269699\ttotal: 7.08s\tremaining: 1.95s\n",
      "784:\tlearn: 0.0269334\ttotal: 7.09s\tremaining: 1.94s\n",
      "785:\tlearn: 0.0268657\ttotal: 7.09s\tremaining: 1.93s\n",
      "786:\tlearn: 0.0268425\ttotal: 7.11s\tremaining: 1.92s\n",
      "787:\tlearn: 0.0268072\ttotal: 7.11s\tremaining: 1.91s\n",
      "788:\tlearn: 0.0267573\ttotal: 7.12s\tremaining: 1.9s\n",
      "789:\tlearn: 0.0266991\ttotal: 7.13s\tremaining: 1.9s\n",
      "790:\tlearn: 0.0266596\ttotal: 7.14s\tremaining: 1.89s\n",
      "791:\tlearn: 0.0266148\ttotal: 7.15s\tremaining: 1.88s\n",
      "792:\tlearn: 0.0265653\ttotal: 7.16s\tremaining: 1.87s\n",
      "793:\tlearn: 0.0265252\ttotal: 7.16s\tremaining: 1.86s\n",
      "794:\tlearn: 0.0264808\ttotal: 7.17s\tremaining: 1.85s\n",
      "795:\tlearn: 0.0264417\ttotal: 7.18s\tremaining: 1.84s\n",
      "796:\tlearn: 0.0263818\ttotal: 7.19s\tremaining: 1.83s\n",
      "797:\tlearn: 0.0263253\ttotal: 7.2s\tremaining: 1.82s\n",
      "798:\tlearn: 0.0262616\ttotal: 7.21s\tremaining: 1.81s\n",
      "799:\tlearn: 0.0262177\ttotal: 7.22s\tremaining: 1.8s\n",
      "800:\tlearn: 0.0261724\ttotal: 7.23s\tremaining: 1.79s\n",
      "801:\tlearn: 0.0261325\ttotal: 7.24s\tremaining: 1.79s\n",
      "802:\tlearn: 0.0260743\ttotal: 7.24s\tremaining: 1.78s\n",
      "803:\tlearn: 0.0260257\ttotal: 7.25s\tremaining: 1.77s\n",
      "804:\tlearn: 0.0259860\ttotal: 7.26s\tremaining: 1.76s\n",
      "805:\tlearn: 0.0259401\ttotal: 7.27s\tremaining: 1.75s\n",
      "806:\tlearn: 0.0259048\ttotal: 7.28s\tremaining: 1.74s\n",
      "807:\tlearn: 0.0258637\ttotal: 7.29s\tremaining: 1.73s\n",
      "808:\tlearn: 0.0258110\ttotal: 7.3s\tremaining: 1.72s\n",
      "809:\tlearn: 0.0257514\ttotal: 7.3s\tremaining: 1.71s\n",
      "810:\tlearn: 0.0257169\ttotal: 7.31s\tremaining: 1.7s\n",
      "811:\tlearn: 0.0256697\ttotal: 7.32s\tremaining: 1.7s\n",
      "812:\tlearn: 0.0256229\ttotal: 7.33s\tremaining: 1.69s\n",
      "813:\tlearn: 0.0255848\ttotal: 7.34s\tremaining: 1.68s\n",
      "814:\tlearn: 0.0255378\ttotal: 7.35s\tremaining: 1.67s\n",
      "815:\tlearn: 0.0254957\ttotal: 7.36s\tremaining: 1.66s\n",
      "816:\tlearn: 0.0254304\ttotal: 7.37s\tremaining: 1.65s\n",
      "817:\tlearn: 0.0253834\ttotal: 7.37s\tremaining: 1.64s\n",
      "818:\tlearn: 0.0253403\ttotal: 7.38s\tremaining: 1.63s\n",
      "819:\tlearn: 0.0253035\ttotal: 7.39s\tremaining: 1.62s\n",
      "820:\tlearn: 0.0252635\ttotal: 7.4s\tremaining: 1.61s\n",
      "821:\tlearn: 0.0252256\ttotal: 7.41s\tremaining: 1.6s\n",
      "822:\tlearn: 0.0251590\ttotal: 7.42s\tremaining: 1.59s\n",
      "823:\tlearn: 0.0251298\ttotal: 7.43s\tremaining: 1.59s\n",
      "824:\tlearn: 0.0250961\ttotal: 7.43s\tremaining: 1.58s\n",
      "825:\tlearn: 0.0250519\ttotal: 7.44s\tremaining: 1.57s\n",
      "826:\tlearn: 0.0250150\ttotal: 7.45s\tremaining: 1.56s\n",
      "827:\tlearn: 0.0250030\ttotal: 7.46s\tremaining: 1.55s\n",
      "828:\tlearn: 0.0249478\ttotal: 7.47s\tremaining: 1.54s\n",
      "829:\tlearn: 0.0249059\ttotal: 7.48s\tremaining: 1.53s\n",
      "830:\tlearn: 0.0248710\ttotal: 7.49s\tremaining: 1.52s\n",
      "831:\tlearn: 0.0248261\ttotal: 7.5s\tremaining: 1.51s\n",
      "832:\tlearn: 0.0247842\ttotal: 7.51s\tremaining: 1.5s\n",
      "833:\tlearn: 0.0247235\ttotal: 7.52s\tremaining: 1.5s\n",
      "834:\tlearn: 0.0246880\ttotal: 7.53s\tremaining: 1.49s\n",
      "835:\tlearn: 0.0246589\ttotal: 7.53s\tremaining: 1.48s\n",
      "836:\tlearn: 0.0246209\ttotal: 7.54s\tremaining: 1.47s\n",
      "837:\tlearn: 0.0245998\ttotal: 7.55s\tremaining: 1.46s\n",
      "838:\tlearn: 0.0245670\ttotal: 7.56s\tremaining: 1.45s\n",
      "839:\tlearn: 0.0245251\ttotal: 7.57s\tremaining: 1.44s\n",
      "840:\tlearn: 0.0244875\ttotal: 7.58s\tremaining: 1.43s\n",
      "841:\tlearn: 0.0244428\ttotal: 7.59s\tremaining: 1.42s\n",
      "842:\tlearn: 0.0244046\ttotal: 7.59s\tremaining: 1.41s\n",
      "843:\tlearn: 0.0243503\ttotal: 7.6s\tremaining: 1.41s\n",
      "844:\tlearn: 0.0243295\ttotal: 7.61s\tremaining: 1.4s\n",
      "845:\tlearn: 0.0242987\ttotal: 7.62s\tremaining: 1.39s\n",
      "846:\tlearn: 0.0242508\ttotal: 7.63s\tremaining: 1.38s\n",
      "847:\tlearn: 0.0242079\ttotal: 7.64s\tremaining: 1.37s\n",
      "848:\tlearn: 0.0241821\ttotal: 7.65s\tremaining: 1.36s\n",
      "849:\tlearn: 0.0241467\ttotal: 7.66s\tremaining: 1.35s\n",
      "850:\tlearn: 0.0241077\ttotal: 7.66s\tremaining: 1.34s\n",
      "851:\tlearn: 0.0240684\ttotal: 7.67s\tremaining: 1.33s\n",
      "852:\tlearn: 0.0240386\ttotal: 7.68s\tremaining: 1.32s\n",
      "853:\tlearn: 0.0240059\ttotal: 7.69s\tremaining: 1.31s\n",
      "854:\tlearn: 0.0239734\ttotal: 7.7s\tremaining: 1.3s\n",
      "855:\tlearn: 0.0239378\ttotal: 7.71s\tremaining: 1.3s\n",
      "856:\tlearn: 0.0238955\ttotal: 7.72s\tremaining: 1.29s\n",
      "857:\tlearn: 0.0238721\ttotal: 7.72s\tremaining: 1.28s\n",
      "858:\tlearn: 0.0238368\ttotal: 7.73s\tremaining: 1.27s\n",
      "859:\tlearn: 0.0237955\ttotal: 7.74s\tremaining: 1.26s\n",
      "860:\tlearn: 0.0237480\ttotal: 7.75s\tremaining: 1.25s\n",
      "861:\tlearn: 0.0237018\ttotal: 7.76s\tremaining: 1.24s\n",
      "862:\tlearn: 0.0236576\ttotal: 7.77s\tremaining: 1.23s\n",
      "863:\tlearn: 0.0236182\ttotal: 7.78s\tremaining: 1.22s\n",
      "864:\tlearn: 0.0235728\ttotal: 7.79s\tremaining: 1.22s\n",
      "865:\tlearn: 0.0235341\ttotal: 7.79s\tremaining: 1.21s\n",
      "866:\tlearn: 0.0234787\ttotal: 7.8s\tremaining: 1.2s\n",
      "867:\tlearn: 0.0234563\ttotal: 7.81s\tremaining: 1.19s\n",
      "868:\tlearn: 0.0234133\ttotal: 7.82s\tremaining: 1.18s\n",
      "869:\tlearn: 0.0233743\ttotal: 7.83s\tremaining: 1.17s\n",
      "870:\tlearn: 0.0233393\ttotal: 7.84s\tremaining: 1.16s\n",
      "871:\tlearn: 0.0232931\ttotal: 7.85s\tremaining: 1.15s\n",
      "872:\tlearn: 0.0232699\ttotal: 7.86s\tremaining: 1.14s\n",
      "873:\tlearn: 0.0232399\ttotal: 7.86s\tremaining: 1.13s\n",
      "874:\tlearn: 0.0232229\ttotal: 7.87s\tremaining: 1.12s\n",
      "875:\tlearn: 0.0231917\ttotal: 7.88s\tremaining: 1.11s\n",
      "876:\tlearn: 0.0231445\ttotal: 7.89s\tremaining: 1.11s\n",
      "877:\tlearn: 0.0231190\ttotal: 7.9s\tremaining: 1.1s\n",
      "878:\tlearn: 0.0230769\ttotal: 7.91s\tremaining: 1.09s\n",
      "879:\tlearn: 0.0230317\ttotal: 7.92s\tremaining: 1.08s\n",
      "880:\tlearn: 0.0229887\ttotal: 7.92s\tremaining: 1.07s\n",
      "881:\tlearn: 0.0229475\ttotal: 7.93s\tremaining: 1.06s\n",
      "882:\tlearn: 0.0229084\ttotal: 7.94s\tremaining: 1.05s\n",
      "883:\tlearn: 0.0228725\ttotal: 7.95s\tremaining: 1.04s\n",
      "884:\tlearn: 0.0228336\ttotal: 7.96s\tremaining: 1.03s\n",
      "885:\tlearn: 0.0228008\ttotal: 7.97s\tremaining: 1.03s\n",
      "886:\tlearn: 0.0227773\ttotal: 7.98s\tremaining: 1.02s\n",
      "887:\tlearn: 0.0227542\ttotal: 7.99s\tremaining: 1.01s\n",
      "888:\tlearn: 0.0227254\ttotal: 8s\tremaining: 999ms\n",
      "889:\tlearn: 0.0226924\ttotal: 8.01s\tremaining: 990ms\n",
      "890:\tlearn: 0.0226741\ttotal: 8.02s\tremaining: 981ms\n",
      "891:\tlearn: 0.0226409\ttotal: 8.03s\tremaining: 972ms\n",
      "892:\tlearn: 0.0226013\ttotal: 8.04s\tremaining: 963ms\n",
      "893:\tlearn: 0.0225764\ttotal: 8.04s\tremaining: 954ms\n",
      "894:\tlearn: 0.0225458\ttotal: 8.05s\tremaining: 945ms\n",
      "895:\tlearn: 0.0225249\ttotal: 8.06s\tremaining: 936ms\n",
      "896:\tlearn: 0.0224752\ttotal: 8.07s\tremaining: 927ms\n",
      "897:\tlearn: 0.0224428\ttotal: 8.09s\tremaining: 918ms\n",
      "898:\tlearn: 0.0224010\ttotal: 8.09s\tremaining: 909ms\n",
      "899:\tlearn: 0.0223549\ttotal: 8.1s\tremaining: 900ms\n",
      "900:\tlearn: 0.0223351\ttotal: 8.11s\tremaining: 891ms\n",
      "901:\tlearn: 0.0222942\ttotal: 8.12s\tremaining: 882ms\n",
      "902:\tlearn: 0.0222614\ttotal: 8.13s\tremaining: 873ms\n",
      "903:\tlearn: 0.0222428\ttotal: 8.14s\tremaining: 864ms\n",
      "904:\tlearn: 0.0222240\ttotal: 8.15s\tremaining: 855ms\n",
      "905:\tlearn: 0.0221932\ttotal: 8.16s\tremaining: 846ms\n",
      "906:\tlearn: 0.0221690\ttotal: 8.16s\tremaining: 837ms\n",
      "907:\tlearn: 0.0221266\ttotal: 8.17s\tremaining: 828ms\n",
      "908:\tlearn: 0.0220970\ttotal: 8.18s\tremaining: 819ms\n",
      "909:\tlearn: 0.0220526\ttotal: 8.19s\tremaining: 810ms\n",
      "910:\tlearn: 0.0220136\ttotal: 8.2s\tremaining: 801ms\n",
      "911:\tlearn: 0.0219765\ttotal: 8.21s\tremaining: 792ms\n",
      "912:\tlearn: 0.0219531\ttotal: 8.22s\tremaining: 783ms\n",
      "913:\tlearn: 0.0219239\ttotal: 8.23s\tremaining: 774ms\n",
      "914:\tlearn: 0.0218872\ttotal: 8.23s\tremaining: 765ms\n",
      "915:\tlearn: 0.0218628\ttotal: 8.24s\tremaining: 756ms\n",
      "916:\tlearn: 0.0218466\ttotal: 8.25s\tremaining: 747ms\n",
      "917:\tlearn: 0.0218066\ttotal: 8.26s\tremaining: 738ms\n",
      "918:\tlearn: 0.0217829\ttotal: 8.27s\tremaining: 729ms\n",
      "919:\tlearn: 0.0217352\ttotal: 8.28s\tremaining: 720ms\n",
      "920:\tlearn: 0.0217016\ttotal: 8.29s\tremaining: 711ms\n",
      "921:\tlearn: 0.0216646\ttotal: 8.29s\tremaining: 702ms\n",
      "922:\tlearn: 0.0216445\ttotal: 8.3s\tremaining: 693ms\n",
      "923:\tlearn: 0.0216074\ttotal: 8.31s\tremaining: 684ms\n",
      "924:\tlearn: 0.0215678\ttotal: 8.32s\tremaining: 675ms\n",
      "925:\tlearn: 0.0215383\ttotal: 8.33s\tremaining: 666ms\n",
      "926:\tlearn: 0.0214990\ttotal: 8.34s\tremaining: 657ms\n",
      "927:\tlearn: 0.0214646\ttotal: 8.35s\tremaining: 648ms\n",
      "928:\tlearn: 0.0214294\ttotal: 8.36s\tremaining: 639ms\n",
      "929:\tlearn: 0.0213917\ttotal: 8.37s\tremaining: 630ms\n",
      "930:\tlearn: 0.0213624\ttotal: 8.37s\tremaining: 621ms\n",
      "931:\tlearn: 0.0213280\ttotal: 8.38s\tremaining: 612ms\n",
      "932:\tlearn: 0.0212944\ttotal: 8.39s\tremaining: 603ms\n",
      "933:\tlearn: 0.0212596\ttotal: 8.4s\tremaining: 594ms\n",
      "934:\tlearn: 0.0212319\ttotal: 8.41s\tremaining: 585ms\n",
      "935:\tlearn: 0.0212185\ttotal: 8.43s\tremaining: 576ms\n",
      "936:\tlearn: 0.0211920\ttotal: 8.44s\tremaining: 567ms\n",
      "937:\tlearn: 0.0211553\ttotal: 8.45s\tremaining: 559ms\n",
      "938:\tlearn: 0.0211293\ttotal: 8.46s\tremaining: 550ms\n",
      "939:\tlearn: 0.0210877\ttotal: 8.47s\tremaining: 541ms\n",
      "940:\tlearn: 0.0210665\ttotal: 8.48s\tremaining: 532ms\n",
      "941:\tlearn: 0.0210405\ttotal: 8.49s\tremaining: 523ms\n",
      "942:\tlearn: 0.0210120\ttotal: 8.49s\tremaining: 514ms\n",
      "943:\tlearn: 0.0209991\ttotal: 8.5s\tremaining: 504ms\n",
      "944:\tlearn: 0.0209753\ttotal: 8.51s\tremaining: 495ms\n",
      "945:\tlearn: 0.0209515\ttotal: 8.52s\tremaining: 486ms\n",
      "946:\tlearn: 0.0209296\ttotal: 8.53s\tremaining: 477ms\n",
      "947:\tlearn: 0.0208947\ttotal: 8.54s\tremaining: 468ms\n",
      "948:\tlearn: 0.0208689\ttotal: 8.55s\tremaining: 459ms\n",
      "949:\tlearn: 0.0208349\ttotal: 8.56s\tremaining: 450ms\n",
      "950:\tlearn: 0.0208137\ttotal: 8.57s\tremaining: 441ms\n",
      "951:\tlearn: 0.0207867\ttotal: 8.57s\tremaining: 432ms\n",
      "952:\tlearn: 0.0207368\ttotal: 8.58s\tremaining: 423ms\n",
      "953:\tlearn: 0.0207087\ttotal: 8.59s\tremaining: 414ms\n",
      "954:\tlearn: 0.0206817\ttotal: 8.6s\tremaining: 405ms\n",
      "955:\tlearn: 0.0206683\ttotal: 8.61s\tremaining: 396ms\n",
      "956:\tlearn: 0.0206439\ttotal: 8.62s\tremaining: 387ms\n",
      "957:\tlearn: 0.0206262\ttotal: 8.63s\tremaining: 378ms\n",
      "958:\tlearn: 0.0205744\ttotal: 8.64s\tremaining: 369ms\n",
      "959:\tlearn: 0.0205641\ttotal: 8.64s\tremaining: 360ms\n",
      "960:\tlearn: 0.0205355\ttotal: 8.65s\tremaining: 351ms\n",
      "961:\tlearn: 0.0205095\ttotal: 8.66s\tremaining: 342ms\n",
      "962:\tlearn: 0.0204768\ttotal: 8.67s\tremaining: 333ms\n",
      "963:\tlearn: 0.0204500\ttotal: 8.68s\tremaining: 324ms\n",
      "964:\tlearn: 0.0204262\ttotal: 8.69s\tremaining: 315ms\n",
      "965:\tlearn: 0.0203890\ttotal: 8.7s\tremaining: 306ms\n",
      "966:\tlearn: 0.0203632\ttotal: 8.71s\tremaining: 297ms\n",
      "967:\tlearn: 0.0203265\ttotal: 8.72s\tremaining: 288ms\n",
      "968:\tlearn: 0.0203037\ttotal: 8.73s\tremaining: 279ms\n",
      "969:\tlearn: 0.0202759\ttotal: 8.73s\tremaining: 270ms\n",
      "970:\tlearn: 0.0202449\ttotal: 8.74s\tremaining: 261ms\n",
      "971:\tlearn: 0.0202159\ttotal: 8.75s\tremaining: 252ms\n",
      "972:\tlearn: 0.0201768\ttotal: 8.76s\tremaining: 243ms\n",
      "973:\tlearn: 0.0201304\ttotal: 8.77s\tremaining: 234ms\n",
      "974:\tlearn: 0.0200804\ttotal: 8.78s\tremaining: 225ms\n",
      "975:\tlearn: 0.0200402\ttotal: 8.79s\tremaining: 216ms\n",
      "976:\tlearn: 0.0200143\ttotal: 8.8s\tremaining: 207ms\n",
      "977:\tlearn: 0.0199866\ttotal: 8.8s\tremaining: 198ms\n",
      "978:\tlearn: 0.0199527\ttotal: 8.81s\tremaining: 189ms\n",
      "979:\tlearn: 0.0199324\ttotal: 8.82s\tremaining: 180ms\n",
      "980:\tlearn: 0.0199009\ttotal: 8.83s\tremaining: 171ms\n",
      "981:\tlearn: 0.0198836\ttotal: 8.84s\tremaining: 162ms\n",
      "982:\tlearn: 0.0198395\ttotal: 8.85s\tremaining: 153ms\n",
      "983:\tlearn: 0.0198105\ttotal: 8.86s\tremaining: 144ms\n",
      "984:\tlearn: 0.0197987\ttotal: 8.87s\tremaining: 135ms\n",
      "985:\tlearn: 0.0197704\ttotal: 8.87s\tremaining: 126ms\n",
      "986:\tlearn: 0.0197458\ttotal: 8.88s\tremaining: 117ms\n",
      "987:\tlearn: 0.0196951\ttotal: 8.89s\tremaining: 108ms\n",
      "988:\tlearn: 0.0196579\ttotal: 8.9s\tremaining: 99ms\n",
      "989:\tlearn: 0.0196282\ttotal: 8.91s\tremaining: 90ms\n",
      "990:\tlearn: 0.0196088\ttotal: 8.92s\tremaining: 81ms\n",
      "991:\tlearn: 0.0195798\ttotal: 8.93s\tremaining: 72ms\n",
      "992:\tlearn: 0.0195476\ttotal: 8.94s\tremaining: 63ms\n",
      "993:\tlearn: 0.0195093\ttotal: 8.94s\tremaining: 54ms\n",
      "994:\tlearn: 0.0194774\ttotal: 8.95s\tremaining: 45ms\n",
      "995:\tlearn: 0.0194484\ttotal: 8.96s\tremaining: 36ms\n",
      "996:\tlearn: 0.0194110\ttotal: 8.97s\tremaining: 27ms\n",
      "997:\tlearn: 0.0193984\ttotal: 8.98s\tremaining: 18ms\n",
      "998:\tlearn: 0.0193651\ttotal: 8.99s\tremaining: 9ms\n",
      "999:\tlearn: 0.0193349\ttotal: 9s\tremaining: 0us\n",
      "Validation result:\n",
      "MAE: 0.0653\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0151\n"
     ]
    }
   ],
   "source": [
    "# CatBoostRegressor\n",
    "# CatBoost is the last of the well-known gradient boosting frameworks, known for its \n",
    "# high accuracy and ability to handle categorical features directly\n",
    "# We first try it without tweaking parameters\n",
    "\n",
    "cbr = CatBoostRegressor(random_state=67)\n",
    "\n",
    "cbr.fit(X_train_std, y_train)\n",
    "y_pred = cbr.predict(X_val_std)\n",
    "y_test = cbr.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b96d4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0619\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.6467\n"
     ]
    }
   ],
   "source": [
    "# LGBMRegressor appears to have the best performance without overfitting that much\n",
    "# We try to tune LGBMRegressor even more to deliver the best performance\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=0.001\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "target_transformer = QuantileTransformer(output_distribution='normal', random_state=67)\n",
    "y_train_transformed = target_transformer.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "lgbm.fit(X_train, y_train_transformed)\n",
    "y_pred_transformed = lgbm.predict(X_val)\n",
    "y_pred = target_transformer.inverse_transform(y_pred_transformed.reshape(-1, 1)).ravel()\n",
    "y_test = lgbm.predict(X_train)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c1e8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code used to find LGBMRegressor parameters\n",
    "\n",
    "# Define ranges to sweep\n",
    "# depths = [3, 4, 5, 6, 7, 8]\n",
    "# lambdas = [0.0, 0.1, 0.5, 1.0]\n",
    "# learning_rates = [0.005, 0.008, 0.01]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Sweep over hyperparameters\n",
    "# for depth in depths:\n",
    "#     for lam in lambdas:\n",
    "#         for lr in learning_rates:\n",
    "#             model = LGBMRegressor(\n",
    "#                 random_state=67,\n",
    "#                 max_depth=depth,\n",
    "#                 num_leaves=2**depth,       # roughly matched to depth\n",
    "#                 n_estimators=3500,\n",
    "#                 learning_rate=lr,\n",
    "#                 reg_lambda=lam,\n",
    "#                 reg_alpha=0.1,\n",
    "#                 subsample=0.9,\n",
    "#                 colsample_bytree=0.9,\n",
    "#                 verbose=-1,\n",
    "#             )\n",
    "\n",
    "#             model.fit(X_train_std, y_train)\n",
    "#             y_train_pred = model.predict(X_train_std)\n",
    "#             y_val_pred = model.predict(X_val_std)\n",
    "\n",
    "#             train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "#             val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "#             results.append((depth, lam, lr, train_mae, val_mae))\n",
    "\n",
    "# # Convert to numpy for easy slicing\n",
    "# results = np.array(results, dtype=object)\n",
    "\n",
    "# # Pick the lowest validation MAE combo\n",
    "# best_idx = np.argmin(results[:, 4].astype(float))\n",
    "# best = results[best_idx]\n",
    "# print(f\"\\nBest config: depth={best[0]}, lambda={best[1]}, lr={best[2]} \"\n",
    "#       f\"→ train MAE={best[3]:.5f}, val MAE={best[4]:.5f}\")\n",
    "\n",
    "# # Plot curves by depth for the best lambda/lr combination\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# for lam in lambdas:\n",
    "#     subset = results[results[:, 1] == lam]\n",
    "#     plt.plot(subset[:, 0], subset[:, 3].astype(float), 'o--', label=f\"Train λ={lam}\")\n",
    "#     plt.plot(subset[:, 0], subset[:, 4].astype(float), 'o-', label=f\"Val λ={lam}\")\n",
    "\n",
    "# plt.xlabel(\"max_depth\")\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.title(\"Train vs Validation MAE across model complexity\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f616e2b",
   "metadata": {},
   "source": [
    "We find that Tree-based models perform much better than Linear-based models, which is understandable as the data likely does not follow a linear distribution.  \n",
    "In particular, the LGBM Regressor gave an exceptionally good performance out of the 3 most well-known gradient boosting frameworks.  \n",
    "This is likely because LGBM grows leaf-wise: it chooses the single leaf with the highest loss reduction to split next. Since our engineered features capture strong but localised signals, LightGBM's leaf-wise search can find them efficiently.  \n",
    "Based off this, we further fine-tuned LGBMRegressor to deliver the best performance we could.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8978c",
   "metadata": {},
   "source": [
    "### Ensemble-based models (RandomForest, BaggingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8185f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0670\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0265\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "rfr = RandomForestRegressor(random_state=67)\n",
    "rfr.fit(X_train_std, y_train)\n",
    "y_pred = rfr.predict(X_val_std)\n",
    "y_test = rfr.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae4f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0653\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0319\n"
     ]
    }
   ],
   "source": [
    "# Since Random Forest Regressor delivered great performance, we tried tuning it further\n",
    "# The performance did not increase much despite our best efforts\n",
    "\n",
    "rfr2 = RandomForestRegressor(\n",
    "    random_state=67,\n",
    "    n_estimators=1200,\n",
    "    n_jobs=-1,\n",
    "    max_depth=17,\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=10,\n",
    "    max_features=0.7,\n",
    "    )\n",
    "\n",
    "rfr2.fit(X_train_std, y_train)\n",
    "y_pred = rfr2.predict(X_val_std)\n",
    "y_test = rfr2.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d06328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0709\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0304\n"
     ]
    }
   ],
   "source": [
    "# Bagging Regressor\n",
    "\n",
    "br = BaggingRegressor(random_state=67)\n",
    "br.fit(X_train_std, y_train)\n",
    "y_pred = br.predict(X_val_std)\n",
    "y_test = br.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cb786",
   "metadata": {},
   "source": [
    "Ensemble-based methods also provide good performance out-of-the-box, but we find that Tree-based regressors provide the best performance out of all the supervised learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d35a6",
   "metadata": {},
   "source": [
    "### Neural Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c95a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result:\n",
      "MAE: 0.0924\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0436\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer perceptron (MLP) Regressor with 4 layers\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.1,\n",
    "    learning_rate_init=0.003,\n",
    "    max_iter=5000,\n",
    "    random_state=67,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_std, y_train)\n",
    "y_pred = mlp.predict(X_val_std)\n",
    "y_test = mlp.predict(X_train_std)\n",
    "\n",
    "print(\"Validation result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ad59f",
   "metadata": {},
   "source": [
    "After some fine tuning, the MLP Regressor is able to get decent performance on the validation dataset, but still not as good as the other types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe1d66",
   "metadata": {},
   "source": [
    "### Unsupervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "933c6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual result:\n",
      "MAE: 0.1426\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0489\n"
     ]
    }
   ],
   "source": [
    "# Trying out PCA + LGBMRegressor\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_val_pca = pca.transform(X_val_std)\n",
    "\n",
    "lgbm2 = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae'\n",
    ")\n",
    "\n",
    "lgbm2.fit(X_train_pca, y_train)\n",
    "y_pred = lgbm2.predict(X_val_pca)\n",
    "y_test = lgbm2.predict(X_train_pca)\n",
    "\n",
    "print(\"Actual result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b8862",
   "metadata": {},
   "source": [
    "Reducing the dimensionality of the data does not seem to work very well overall, even when we vary the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2177a12",
   "metadata": {},
   "source": [
    "### Stacking uncorrelated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62c4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.57490687 0.30806671 0.2973029  0.31336321]\n",
      " [0.57490687 1.         0.47802302 0.4604701  0.48888055]\n",
      " [0.30806671 0.47802302 1.         0.9871216  0.92777653]\n",
      " [0.2973029  0.4604701  0.9871216  1.         0.92272941]\n",
      " [0.31336321 0.48888055 0.92777653 0.92272941 1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We check the correlations between the predictions of the best models\n",
    "# If the models are too closely related, there is no point in stacking them\n",
    "\n",
    "pred_xgb = xgbr.predict(X_val_std)\n",
    "pred_lgbm = lgbm.predict(X_val_std)\n",
    "pred_cbr = cbr.predict(X_val_std)\n",
    "pred_rfr = rfr2.predict(X_val_std)\n",
    "pred_mlp = mlp.predict(X_val_std)\n",
    "\n",
    "corr_matrix = np.corrcoef([pred_xgb, pred_lgbm, pred_cbr, pred_rfr, pred_mlp])\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d504776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual result:\n",
      "MAE: 0.0620\n",
      "Result on training set (test for overfitting):\n",
      "MAE: 0.0263\n"
     ]
    }
   ],
   "source": [
    "# We try stacking models that are not strongly correlated (corr < 0.95)\n",
    "# This can potentially improve our performance, as the MLP can capture smooth nonlinear patterns missed by tree models\n",
    "# The stacking regressor can combine these predictions to reduce both bias and variance, improving overall MAE\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"lgbm\", LGBMRegressor(\n",
    "            random_state=67,\n",
    "            max_depth=6,\n",
    "            num_leaves=64,\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.01,\n",
    "            reg_lambda=0.1,\n",
    "            reg_alpha=1.0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            verbose=-1,\n",
    "            objective='mae',\n",
    "            metric='mae'\n",
    "        )),\n",
    "        (\"mlp\", MLPRegressor(\n",
    "            hidden_layer_sizes=(128, 64),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.1,\n",
    "            learning_rate_init=0.003,\n",
    "            max_iter=5000,\n",
    "            random_state=67,\n",
    "            batch_size=128\n",
    "        ))\n",
    "    ],\n",
    "    final_estimator=Ridge(alpha=0.1)\n",
    ")\n",
    "\n",
    "stack.fit(X_train_std, y_train)\n",
    "y_pred = stack.predict(X_val_std)\n",
    "y_test = stack.predict(X_train_std)\n",
    "\n",
    "print(\"Actual result:\")\n",
    "evaluate_linear_predictions(y_val, y_pred)\n",
    "print(\"Result on training set (test for overfitting):\")\n",
    "evaluate_linear_predictions(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf1ead",
   "metadata": {},
   "source": [
    "Our hypothesis that stacking the two regressors increases performance does not prove true; the tuned LGBMRegressor still leads to roughly equivalent performance, and the slight improvement observed might just be due to noise.  \n",
    "We thus then retrain the single LGBMRegressor model with tuned parameters on the full dataset, to prepare it to predict on the final batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d6626",
   "metadata": {},
   "source": [
    "### Preparing final regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3fa774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training final model on full dataset\n",
    "\n",
    "# scaler_full = StandardScaler()\n",
    "# X_all_std = scaler_full.fit_transform(X_lr)\n",
    "\n",
    "# lgbm_final = LGBMRegressor(\n",
    "#     random_state=67,\n",
    "#     max_depth=6,\n",
    "#     num_leaves=64,\n",
    "#     n_estimators=5000,\n",
    "#     learning_rate=0.01,\n",
    "#     reg_lambda=0.1,\n",
    "#     reg_alpha=1.0,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     verbose=-1,\n",
    "#     objective='mae',\n",
    "#     metric='mae'\n",
    "# )\n",
    "\n",
    "# lgbm_final.fit(X_all_std, y_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "448f3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating features with optimized parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 627\n",
      "\n",
      "=== Retraining Feature Selector on Optimized Features ===\n",
      "\n",
      "Top 20 most important features:\n",
      "                  feature  importance\n",
      "623   avg_item_popularity         632\n",
      "0                   svd_1         376\n",
      "1                   svd_2         363\n",
      "615       rating_kurtosis         243\n",
      "2                   svd_3         221\n",
      "607            count_like         183\n",
      "600           mean_rating         171\n",
      "3                   svd_4         141\n",
      "620        rating_entropy         126\n",
      "618          outlier_frac         123\n",
      "5                   svd_6         115\n",
      "602            std_rating         113\n",
      "4                   svd_5         111\n",
      "608    total_interactions         109\n",
      "7                   svd_8         107\n",
      "610            like_ratio         105\n",
      "616       rating_skewness          93\n",
      "611         dislike_ratio          85\n",
      "626  rating_concentration          83\n",
      "609        normalized_std          80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: mean\n",
      "Features selected: 151 / 627\n",
      "Validation MAE: 0.0625\n",
      "Training MAE: 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: median\n",
      "Features selected: 337 / 627\n",
      "Validation MAE: 0.0632\n",
      "Training MAE: 0.0207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: 0.75*mean\n",
      "Features selected: 337 / 627\n",
      "Validation MAE: 0.0632\n",
      "Training MAE: 0.0207\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>n_features</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>151</td>\n",
       "      <td>0.062457</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.040545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>337</td>\n",
       "      <td>0.063171</td>\n",
       "      <td>0.020672</td>\n",
       "      <td>0.042499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75*mean</td>\n",
       "      <td>337</td>\n",
       "      <td>0.063171</td>\n",
       "      <td>0.020672</td>\n",
       "      <td>0.042499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  n_features   val_mae  train_mae  overfitting\n",
       "0       mean         151  0.062457   0.021912     0.040545\n",
       "1     median         337  0.063171   0.020672     0.042499\n",
       "2  0.75*mean         337  0.063171   0.020672     0.042499"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best threshold: mean\n",
      "\n",
      "Selected features: 151 / 627\n",
      "\n",
      "=== Training Final Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Model Performance:\n",
      "Validation MAE: 0.0625\n",
      "\n",
      "Final model trained on full dataset with optimized features!\n"
     ]
    }
   ],
   "source": [
    "### Training Final Model with Optimized Features\n",
    "\n",
    "# Regenerate features with optimal SVD components\n",
    "print(\"Regenerating features with optimized parameters...\")\n",
    "df_reg_optimized = engineer_features(X, y, n_svd_components=600)\n",
    "df_reg_optimized.columns = df_reg_optimized.columns.astype(str)\n",
    "\n",
    "# Extract features and labels\n",
    "X_lr_optimized = df_reg_optimized.drop(columns=[\"label\"]).values\n",
    "y_lr_optimized = df_reg_optimized[\"label\"].values\n",
    "\n",
    "# Split for validation\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_lr_optimized, y_lr_optimized, test_size=0.2, random_state=67\n",
    ")\n",
    "\n",
    "print(f\"Original features: {X_train_opt.shape[1]}\")\n",
    "\n",
    "# ==========================================\n",
    "# RETRAIN FEATURE SELECTOR ON NEW FEATURES\n",
    "# ==========================================\n",
    "print(\"\\n=== Retraining Feature Selector on Optimized Features ===\")\n",
    "\n",
    "# Train new LGBM for feature importance\n",
    "lgbm_selector_optimized = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    learning_rate=0.05,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_selector_optimized.fit(X_train_opt, y_train_opt)\n",
    "\n",
    "# Get feature importances for optimized features\n",
    "feature_importance_opt = pd.DataFrame({\n",
    "    'feature': df_reg_optimized.drop(columns=[\"label\"]).columns,\n",
    "    'importance': lgbm_selector_optimized.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(feature_importance_opt.head(20))\n",
    "\n",
    "# Try different thresholds to find best one\n",
    "thresholds = ['mean', 'median', '0.75*mean']\n",
    "results_comparison_opt = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    \n",
    "    selector_temp = SelectFromModel(lgbm_selector_optimized, threshold=threshold, prefit=True)\n",
    "    X_train_selected_temp = selector_temp.transform(X_train_opt)\n",
    "    X_val_selected_temp = selector_temp.transform(X_val_opt)\n",
    "    \n",
    "    # Train model on selected features\n",
    "    lgbm_test = LGBMRegressor(\n",
    "        random_state=67,\n",
    "        max_depth=6,\n",
    "        num_leaves=64,\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.01,\n",
    "        reg_lambda=0.1,\n",
    "        reg_alpha=1.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbose=-1,\n",
    "        objective='mae',\n",
    "        metric='mae',\n",
    "        min_child_samples=20,\n",
    "        min_child_weight=0.001\n",
    "    )\n",
    "    \n",
    "    lgbm_test.fit(X_train_selected_temp, y_train_opt)\n",
    "    y_pred_val_temp = lgbm_test.predict(X_val_selected_temp)\n",
    "    y_pred_train_temp = lgbm_test.predict(X_train_selected_temp)\n",
    "    \n",
    "    mae_val_temp = mean_absolute_error(y_val_opt, y_pred_val_temp)\n",
    "    mae_train_temp = mean_absolute_error(y_train_opt, y_pred_train_temp)\n",
    "    \n",
    "    results_comparison_opt.append({\n",
    "        'threshold': threshold,\n",
    "        'n_features': X_train_selected_temp.shape[1],\n",
    "        'val_mae': mae_val_temp,\n",
    "        'train_mae': mae_train_temp,\n",
    "        'overfitting': mae_val_temp - mae_train_temp\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"Features selected: {X_train_selected_temp.shape[1]} / {X_train_opt.shape[1]}\")\n",
    "    print(f\"Validation MAE: {mae_val_temp:.4f}\")\n",
    "    print(f\"Training MAE: {mae_train_temp:.4f}\")\n",
    "\n",
    "comparison_df_opt = pd.DataFrame(results_comparison_opt)\n",
    "display(comparison_df_opt)\n",
    "\n",
    "# Choose best threshold\n",
    "best_threshold_opt = comparison_df_opt.loc[comparison_df_opt['val_mae'].idxmin(), 'threshold']\n",
    "print(f\"\\nBest threshold: {best_threshold_opt}\")\n",
    "\n",
    "# Create FINAL selector for optimized features\n",
    "selector_final_optimized = SelectFromModel(\n",
    "    lgbm_selector_optimized, \n",
    "    threshold=best_threshold_opt, \n",
    "    prefit=True\n",
    ")\n",
    "\n",
    "# Apply feature selection\n",
    "X_train_opt_selected = selector_final_optimized.transform(X_train_opt)\n",
    "X_val_opt_selected = selector_final_optimized.transform(X_val_opt)\n",
    "\n",
    "print(f\"\\nSelected features: {X_train_opt_selected.shape[1]} / {X_train_opt.shape[1]}\")\n",
    "\n",
    "# ==========================================\n",
    "# TRAIN FINAL MODEL\n",
    "# ==========================================\n",
    "print(\"\\n=== Training Final Model ===\")\n",
    "\n",
    "lgbm_final = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=0.001\n",
    ")\n",
    "\n",
    "lgbm_final.fit(X_train_opt_selected, y_train_opt)\n",
    "y_pred_final = lgbm_final.predict(X_val_opt_selected)\n",
    "\n",
    "print(\"\\nOptimized Model Performance:\")\n",
    "print(f\"Validation MAE: {mean_absolute_error(y_val_opt, y_pred_final):.4f}\")\n",
    "\n",
    "# Train on full dataset\n",
    "X_all_opt_selected = selector_final_optimized.transform(X_lr_optimized)\n",
    "lgbm_final.fit(X_all_opt_selected, y_lr_optimized)\n",
    "\n",
    "print(\"\\nFinal model trained on full dataset with optimized features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1656f",
   "metadata": {},
   "source": [
    "## Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9357c7",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca475fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_1</th>\n",
       "      <th>svd_2</th>\n",
       "      <th>svd_3</th>\n",
       "      <th>svd_4</th>\n",
       "      <th>svd_5</th>\n",
       "      <th>svd_6</th>\n",
       "      <th>svd_7</th>\n",
       "      <th>svd_8</th>\n",
       "      <th>svd_9</th>\n",
       "      <th>svd_10</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.405226</td>\n",
       "      <td>1.056769</td>\n",
       "      <td>-21.038527</td>\n",
       "      <td>1.067305</td>\n",
       "      <td>26.873600</td>\n",
       "      <td>3.613976</td>\n",
       "      <td>-2.008504</td>\n",
       "      <td>-1.611251</td>\n",
       "      <td>4.041583</td>\n",
       "      <td>-4.170670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399362</td>\n",
       "      <td>1.503556</td>\n",
       "      <td>0.151639</td>\n",
       "      <td>0.248333</td>\n",
       "      <td>1296.213115</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.395062</td>\n",
       "      <td>0.254065</td>\n",
       "      <td>0.558222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>24.701986</td>\n",
       "      <td>4.278532</td>\n",
       "      <td>-6.918118</td>\n",
       "      <td>-14.194389</td>\n",
       "      <td>4.933048</td>\n",
       "      <td>8.060955</td>\n",
       "      <td>6.287393</td>\n",
       "      <td>4.449363</td>\n",
       "      <td>-1.056309</td>\n",
       "      <td>-0.967260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100368</td>\n",
       "      <td>1.441563</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>0.515833</td>\n",
       "      <td>1429.568720</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>1.219048</td>\n",
       "      <td>0.276117</td>\n",
       "      <td>0.165112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-30.651479</td>\n",
       "      <td>27.972663</td>\n",
       "      <td>-1.808480</td>\n",
       "      <td>-11.094628</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.539192</td>\n",
       "      <td>-0.447467</td>\n",
       "      <td>-1.427426</td>\n",
       "      <td>-0.590070</td>\n",
       "      <td>0.858616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477618</td>\n",
       "      <td>0.745018</td>\n",
       "      <td>0.471963</td>\n",
       "      <td>0.987222</td>\n",
       "      <td>873.476636</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.516432</td>\n",
       "      <td>0.491833</td>\n",
       "      <td>0.925028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>4.077090</td>\n",
       "      <td>27.859159</td>\n",
       "      <td>17.445291</td>\n",
       "      <td>-5.786915</td>\n",
       "      <td>-12.944545</td>\n",
       "      <td>-2.128206</td>\n",
       "      <td>-3.253641</td>\n",
       "      <td>-6.447440</td>\n",
       "      <td>2.451911</td>\n",
       "      <td>-3.451880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170565</td>\n",
       "      <td>1.399122</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>0.651111</td>\n",
       "      <td>1128.376022</td>\n",
       "      <td>0.250681</td>\n",
       "      <td>1.019126</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.380860</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>34.053427</td>\n",
       "      <td>3.629981</td>\n",
       "      <td>-14.458684</td>\n",
       "      <td>-5.522036</td>\n",
       "      <td>-10.271650</td>\n",
       "      <td>-4.786838</td>\n",
       "      <td>2.645228</td>\n",
       "      <td>9.534818</td>\n",
       "      <td>-3.295730</td>\n",
       "      <td>0.831030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.633656</td>\n",
       "      <td>1.384519</td>\n",
       "      <td>0.265363</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>1383.776536</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>1.263305</td>\n",
       "      <td>0.250882</td>\n",
       "      <td>0.951103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>19.995440</td>\n",
       "      <td>-3.963606</td>\n",
       "      <td>36.543998</td>\n",
       "      <td>-12.160488</td>\n",
       "      <td>-0.507532</td>\n",
       "      <td>10.580456</td>\n",
       "      <td>-0.204711</td>\n",
       "      <td>1.751987</td>\n",
       "      <td>4.764085</td>\n",
       "      <td>-3.733991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846178</td>\n",
       "      <td>0.858244</td>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.859167</td>\n",
       "      <td>1249.322870</td>\n",
       "      <td>0.251121</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.511392</td>\n",
       "      <td>0.055162</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>19.515725</td>\n",
       "      <td>1.946307</td>\n",
       "      <td>-11.297455</td>\n",
       "      <td>-12.750778</td>\n",
       "      <td>-1.955778</td>\n",
       "      <td>5.562889</td>\n",
       "      <td>2.733605</td>\n",
       "      <td>-0.980205</td>\n",
       "      <td>-1.747588</td>\n",
       "      <td>-3.314120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079770</td>\n",
       "      <td>1.255755</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>1388.605000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.327250</td>\n",
       "      <td>0.255181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>19.229018</td>\n",
       "      <td>-4.886454</td>\n",
       "      <td>20.073144</td>\n",
       "      <td>-14.276159</td>\n",
       "      <td>7.799430</td>\n",
       "      <td>1.149055</td>\n",
       "      <td>9.521034</td>\n",
       "      <td>-0.200415</td>\n",
       "      <td>-3.550065</td>\n",
       "      <td>5.731454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395315</td>\n",
       "      <td>1.475948</td>\n",
       "      <td>0.256522</td>\n",
       "      <td>0.178333</td>\n",
       "      <td>1343.843478</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>1.371179</td>\n",
       "      <td>0.242722</td>\n",
       "      <td>0.739300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>15.833040</td>\n",
       "      <td>10.900338</td>\n",
       "      <td>17.698568</td>\n",
       "      <td>-12.457373</td>\n",
       "      <td>2.723195</td>\n",
       "      <td>-2.610725</td>\n",
       "      <td>8.489573</td>\n",
       "      <td>-7.575831</td>\n",
       "      <td>-0.392949</td>\n",
       "      <td>-2.492455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079066</td>\n",
       "      <td>1.315856</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.533056</td>\n",
       "      <td>1279.308550</td>\n",
       "      <td>0.249071</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.311618</td>\n",
       "      <td>0.233492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>12.384787</td>\n",
       "      <td>-1.564100</td>\n",
       "      <td>-9.387893</td>\n",
       "      <td>-5.248689</td>\n",
       "      <td>2.829056</td>\n",
       "      <td>3.688155</td>\n",
       "      <td>-6.718119</td>\n",
       "      <td>-1.332926</td>\n",
       "      <td>-4.809891</td>\n",
       "      <td>2.177380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143844</td>\n",
       "      <td>1.446732</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.543611</td>\n",
       "      <td>1214.692683</td>\n",
       "      <td>0.248780</td>\n",
       "      <td>1.171569</td>\n",
       "      <td>0.263534</td>\n",
       "      <td>0.383316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>35.282715</td>\n",
       "      <td>1.605001</td>\n",
       "      <td>8.985617</td>\n",
       "      <td>3.324267</td>\n",
       "      <td>-25.776958</td>\n",
       "      <td>8.908518</td>\n",
       "      <td>-4.520455</td>\n",
       "      <td>3.688603</td>\n",
       "      <td>1.437349</td>\n",
       "      <td>4.809446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.362430</td>\n",
       "      <td>1.339007</td>\n",
       "      <td>0.173423</td>\n",
       "      <td>0.137222</td>\n",
       "      <td>1260.198198</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.031603</td>\n",
       "      <td>0.285965</td>\n",
       "      <td>0.558745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>22.899398</td>\n",
       "      <td>28.790211</td>\n",
       "      <td>-2.853864</td>\n",
       "      <td>0.518014</td>\n",
       "      <td>-5.737385</td>\n",
       "      <td>13.667635</td>\n",
       "      <td>7.112067</td>\n",
       "      <td>7.904376</td>\n",
       "      <td>-0.047028</td>\n",
       "      <td>1.585041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315420</td>\n",
       "      <td>1.338535</td>\n",
       "      <td>0.246649</td>\n",
       "      <td>0.706111</td>\n",
       "      <td>1237.621984</td>\n",
       "      <td>0.249330</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.297242</td>\n",
       "      <td>0.261752</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>10.109697</td>\n",
       "      <td>-0.534179</td>\n",
       "      <td>9.083594</td>\n",
       "      <td>-9.989619</td>\n",
       "      <td>5.325208</td>\n",
       "      <td>3.840458</td>\n",
       "      <td>-0.516786</td>\n",
       "      <td>-0.355747</td>\n",
       "      <td>-4.024504</td>\n",
       "      <td>4.291348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352857</td>\n",
       "      <td>1.234931</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>0.249583</td>\n",
       "      <td>1190.303419</td>\n",
       "      <td>0.252137</td>\n",
       "      <td>0.948498</td>\n",
       "      <td>0.341369</td>\n",
       "      <td>0.311928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>28.727091</td>\n",
       "      <td>4.460696</td>\n",
       "      <td>-17.737821</td>\n",
       "      <td>-8.568014</td>\n",
       "      <td>2.184781</td>\n",
       "      <td>-4.353866</td>\n",
       "      <td>-7.278990</td>\n",
       "      <td>8.086681</td>\n",
       "      <td>-4.452230</td>\n",
       "      <td>-3.061602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.529118</td>\n",
       "      <td>1.389305</td>\n",
       "      <td>0.174216</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>1406.651568</td>\n",
       "      <td>0.250871</td>\n",
       "      <td>1.220280</td>\n",
       "      <td>0.269920</td>\n",
       "      <td>0.549116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>16.708753</td>\n",
       "      <td>-1.445953</td>\n",
       "      <td>4.006640</td>\n",
       "      <td>-10.407775</td>\n",
       "      <td>-1.372284</td>\n",
       "      <td>4.169791</td>\n",
       "      <td>-7.593610</td>\n",
       "      <td>-1.823122</td>\n",
       "      <td>-0.533504</td>\n",
       "      <td>-0.710331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>1.200819</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>1270.547170</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.359381</td>\n",
       "      <td>0.301816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>-14.589255</td>\n",
       "      <td>24.841091</td>\n",
       "      <td>0.592879</td>\n",
       "      <td>-12.930398</td>\n",
       "      <td>3.003011</td>\n",
       "      <td>-0.479167</td>\n",
       "      <td>0.592950</td>\n",
       "      <td>1.268763</td>\n",
       "      <td>0.800157</td>\n",
       "      <td>-2.415303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238899</td>\n",
       "      <td>1.287192</td>\n",
       "      <td>0.387234</td>\n",
       "      <td>0.809722</td>\n",
       "      <td>1072.093617</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>1.004274</td>\n",
       "      <td>0.321865</td>\n",
       "      <td>0.638008</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>50.942770</td>\n",
       "      <td>10.806731</td>\n",
       "      <td>-19.721361</td>\n",
       "      <td>8.340857</td>\n",
       "      <td>-32.353401</td>\n",
       "      <td>0.202926</td>\n",
       "      <td>9.124116</td>\n",
       "      <td>0.243394</td>\n",
       "      <td>-3.626521</td>\n",
       "      <td>2.615711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540825</td>\n",
       "      <td>1.136690</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>1306.529851</td>\n",
       "      <td>0.251244</td>\n",
       "      <td>0.807980</td>\n",
       "      <td>0.428195</td>\n",
       "      <td>0.057549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>6.660772</td>\n",
       "      <td>18.774476</td>\n",
       "      <td>2.211635</td>\n",
       "      <td>-13.141572</td>\n",
       "      <td>-12.306636</td>\n",
       "      <td>3.991652</td>\n",
       "      <td>11.318564</td>\n",
       "      <td>2.828238</td>\n",
       "      <td>3.027072</td>\n",
       "      <td>-6.186215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381093</td>\n",
       "      <td>1.201517</td>\n",
       "      <td>0.214575</td>\n",
       "      <td>0.741111</td>\n",
       "      <td>1248.655870</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.334065</td>\n",
       "      <td>0.339369</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>4.565552</td>\n",
       "      <td>2.001047</td>\n",
       "      <td>-4.991591</td>\n",
       "      <td>-2.488794</td>\n",
       "      <td>3.774549</td>\n",
       "      <td>-0.880130</td>\n",
       "      <td>-1.752852</td>\n",
       "      <td>-1.050395</td>\n",
       "      <td>1.504125</td>\n",
       "      <td>-2.953061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292262</td>\n",
       "      <td>1.300817</td>\n",
       "      <td>0.210046</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>1109.118721</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.922018</td>\n",
       "      <td>0.316862</td>\n",
       "      <td>0.625202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>55.143343</td>\n",
       "      <td>12.673649</td>\n",
       "      <td>21.268550</td>\n",
       "      <td>27.536053</td>\n",
       "      <td>-8.691158</td>\n",
       "      <td>1.590301</td>\n",
       "      <td>1.867865</td>\n",
       "      <td>2.002636</td>\n",
       "      <td>-10.896158</td>\n",
       "      <td>5.159406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150875</td>\n",
       "      <td>1.158217</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.300278</td>\n",
       "      <td>1184.531250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.784038</td>\n",
       "      <td>0.419419</td>\n",
       "      <td>0.099430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>-3.964172</td>\n",
       "      <td>22.893215</td>\n",
       "      <td>-5.001387</td>\n",
       "      <td>-11.151228</td>\n",
       "      <td>4.800311</td>\n",
       "      <td>5.292133</td>\n",
       "      <td>4.868103</td>\n",
       "      <td>5.233376</td>\n",
       "      <td>-2.822094</td>\n",
       "      <td>1.064827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362125</td>\n",
       "      <td>1.069240</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>1130.827731</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.755274</td>\n",
       "      <td>0.413424</td>\n",
       "      <td>0.529647</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>9.982597</td>\n",
       "      <td>1.233299</td>\n",
       "      <td>4.366212</td>\n",
       "      <td>-4.498651</td>\n",
       "      <td>-0.542160</td>\n",
       "      <td>-3.126854</td>\n",
       "      <td>5.185793</td>\n",
       "      <td>1.389276</td>\n",
       "      <td>-1.535660</td>\n",
       "      <td>-0.518658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148607</td>\n",
       "      <td>1.239811</td>\n",
       "      <td>0.078067</td>\n",
       "      <td>0.373472</td>\n",
       "      <td>1155.007435</td>\n",
       "      <td>0.245353</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.346416</td>\n",
       "      <td>0.507218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>33.229563</td>\n",
       "      <td>5.877482</td>\n",
       "      <td>-19.276149</td>\n",
       "      <td>-9.445955</td>\n",
       "      <td>-9.482312</td>\n",
       "      <td>-5.610207</td>\n",
       "      <td>2.965458</td>\n",
       "      <td>-3.737145</td>\n",
       "      <td>-8.958739</td>\n",
       "      <td>1.904718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260829</td>\n",
       "      <td>1.496491</td>\n",
       "      <td>0.186851</td>\n",
       "      <td>0.235556</td>\n",
       "      <td>1422.553633</td>\n",
       "      <td>0.245675</td>\n",
       "      <td>1.253472</td>\n",
       "      <td>0.240550</td>\n",
       "      <td>0.426661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>-9.410127</td>\n",
       "      <td>1.405786</td>\n",
       "      <td>-4.863681</td>\n",
       "      <td>4.097648</td>\n",
       "      <td>-0.428274</td>\n",
       "      <td>0.271327</td>\n",
       "      <td>6.355352</td>\n",
       "      <td>-0.929348</td>\n",
       "      <td>-0.500602</td>\n",
       "      <td>-0.158288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.687801</td>\n",
       "      <td>1.301151</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.153889</td>\n",
       "      <td>919.012308</td>\n",
       "      <td>0.249231</td>\n",
       "      <td>1.046296</td>\n",
       "      <td>0.300317</td>\n",
       "      <td>0.979873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>18.630213</td>\n",
       "      <td>0.309461</td>\n",
       "      <td>4.104553</td>\n",
       "      <td>-12.629557</td>\n",
       "      <td>-7.013169</td>\n",
       "      <td>-0.524197</td>\n",
       "      <td>-2.458145</td>\n",
       "      <td>4.225193</td>\n",
       "      <td>4.917298</td>\n",
       "      <td>-0.953974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>1.513973</td>\n",
       "      <td>0.182222</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>1292.951111</td>\n",
       "      <td>0.248889</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.258746</td>\n",
       "      <td>0.245883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>46.562663</td>\n",
       "      <td>8.590602</td>\n",
       "      <td>-5.380619</td>\n",
       "      <td>17.140850</td>\n",
       "      <td>-0.568038</td>\n",
       "      <td>1.429418</td>\n",
       "      <td>4.274856</td>\n",
       "      <td>11.332335</td>\n",
       "      <td>-3.256507</td>\n",
       "      <td>-1.132952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379207</td>\n",
       "      <td>1.188743</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>1250.078394</td>\n",
       "      <td>0.248566</td>\n",
       "      <td>0.768199</td>\n",
       "      <td>0.352295</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>16.365574</td>\n",
       "      <td>-5.234236</td>\n",
       "      <td>18.390408</td>\n",
       "      <td>-18.399680</td>\n",
       "      <td>-0.653204</td>\n",
       "      <td>-1.960561</td>\n",
       "      <td>-5.301496</td>\n",
       "      <td>-8.029610</td>\n",
       "      <td>-0.166289</td>\n",
       "      <td>-5.879510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554063</td>\n",
       "      <td>1.378308</td>\n",
       "      <td>0.200893</td>\n",
       "      <td>0.083889</td>\n",
       "      <td>1359.825893</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228700</td>\n",
       "      <td>0.253866</td>\n",
       "      <td>0.903824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>48.347671</td>\n",
       "      <td>6.326245</td>\n",
       "      <td>26.561090</td>\n",
       "      <td>3.600284</td>\n",
       "      <td>-18.627871</td>\n",
       "      <td>4.337232</td>\n",
       "      <td>-23.631518</td>\n",
       "      <td>0.098326</td>\n",
       "      <td>-5.827032</td>\n",
       "      <td>3.219304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101233</td>\n",
       "      <td>1.553445</td>\n",
       "      <td>0.273707</td>\n",
       "      <td>0.433056</td>\n",
       "      <td>1268.467672</td>\n",
       "      <td>0.247845</td>\n",
       "      <td>1.382289</td>\n",
       "      <td>0.219986</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>-8.846367</td>\n",
       "      <td>22.965476</td>\n",
       "      <td>-9.353197</td>\n",
       "      <td>-9.323590</td>\n",
       "      <td>-1.110193</td>\n",
       "      <td>-6.428081</td>\n",
       "      <td>-0.770292</td>\n",
       "      <td>-3.809209</td>\n",
       "      <td>-0.839959</td>\n",
       "      <td>1.847927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252205</td>\n",
       "      <td>1.291371</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>1076.918367</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.327514</td>\n",
       "      <td>0.587148</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>32.194061</td>\n",
       "      <td>4.328843</td>\n",
       "      <td>-12.861302</td>\n",
       "      <td>-8.997731</td>\n",
       "      <td>-0.403383</td>\n",
       "      <td>5.000128</td>\n",
       "      <td>3.642669</td>\n",
       "      <td>-3.721963</td>\n",
       "      <td>-6.917732</td>\n",
       "      <td>0.772333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000830</td>\n",
       "      <td>1.565127</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.401944</td>\n",
       "      <td>1381.936567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228464</td>\n",
       "      <td>0.229923</td>\n",
       "      <td>0.150973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>-11.240644</td>\n",
       "      <td>38.191345</td>\n",
       "      <td>2.053651</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>-3.187149</td>\n",
       "      <td>0.353069</td>\n",
       "      <td>-3.564333</td>\n",
       "      <td>-4.061618</td>\n",
       "      <td>2.608191</td>\n",
       "      <td>-3.025811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144829</td>\n",
       "      <td>1.375909</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.735556</td>\n",
       "      <td>1036.200000</td>\n",
       "      <td>0.243678</td>\n",
       "      <td>0.907834</td>\n",
       "      <td>0.274652</td>\n",
       "      <td>0.580426</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>-22.530234</td>\n",
       "      <td>31.830443</td>\n",
       "      <td>-5.155541</td>\n",
       "      <td>-6.073602</td>\n",
       "      <td>2.675920</td>\n",
       "      <td>1.076088</td>\n",
       "      <td>-2.166935</td>\n",
       "      <td>-1.123116</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>-0.718346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342201</td>\n",
       "      <td>1.073918</td>\n",
       "      <td>0.371025</td>\n",
       "      <td>0.890833</td>\n",
       "      <td>960.448763</td>\n",
       "      <td>0.250883</td>\n",
       "      <td>0.687943</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.746696</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>-25.681521</td>\n",
       "      <td>25.066416</td>\n",
       "      <td>-0.517062</td>\n",
       "      <td>-11.098346</td>\n",
       "      <td>0.320960</td>\n",
       "      <td>-0.722261</td>\n",
       "      <td>-0.316471</td>\n",
       "      <td>-1.407688</td>\n",
       "      <td>-1.067839</td>\n",
       "      <td>2.649285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388896</td>\n",
       "      <td>1.000470</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.928611</td>\n",
       "      <td>929.228070</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.638767</td>\n",
       "      <td>0.416551</td>\n",
       "      <td>0.873609</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>12.691205</td>\n",
       "      <td>1.361096</td>\n",
       "      <td>1.730498</td>\n",
       "      <td>-4.063565</td>\n",
       "      <td>2.698011</td>\n",
       "      <td>-0.263857</td>\n",
       "      <td>-2.108461</td>\n",
       "      <td>0.051035</td>\n",
       "      <td>-0.066095</td>\n",
       "      <td>0.649859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131732</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.388194</td>\n",
       "      <td>1176.053640</td>\n",
       "      <td>0.249042</td>\n",
       "      <td>0.776923</td>\n",
       "      <td>0.375024</td>\n",
       "      <td>0.520802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>26.208748</td>\n",
       "      <td>1.223819</td>\n",
       "      <td>-0.750656</td>\n",
       "      <td>-14.497578</td>\n",
       "      <td>21.059123</td>\n",
       "      <td>1.539668</td>\n",
       "      <td>3.829611</td>\n",
       "      <td>-2.383363</td>\n",
       "      <td>5.219389</td>\n",
       "      <td>4.505694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490975</td>\n",
       "      <td>1.552178</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.184722</td>\n",
       "      <td>1452.404545</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.534247</td>\n",
       "      <td>0.227603</td>\n",
       "      <td>0.805789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>25.898912</td>\n",
       "      <td>-0.875074</td>\n",
       "      <td>17.089111</td>\n",
       "      <td>-16.163478</td>\n",
       "      <td>-3.181579</td>\n",
       "      <td>-3.717163</td>\n",
       "      <td>-12.909274</td>\n",
       "      <td>0.747702</td>\n",
       "      <td>7.321352</td>\n",
       "      <td>-2.666413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345681</td>\n",
       "      <td>1.414951</td>\n",
       "      <td>0.164794</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>1379.501873</td>\n",
       "      <td>0.250936</td>\n",
       "      <td>1.135338</td>\n",
       "      <td>0.263982</td>\n",
       "      <td>0.671698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>5.944021</td>\n",
       "      <td>10.971361</td>\n",
       "      <td>-7.028612</td>\n",
       "      <td>-3.729516</td>\n",
       "      <td>16.252878</td>\n",
       "      <td>4.574362</td>\n",
       "      <td>-5.971668</td>\n",
       "      <td>6.157987</td>\n",
       "      <td>4.949524</td>\n",
       "      <td>5.259026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066686</td>\n",
       "      <td>1.431018</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.595556</td>\n",
       "      <td>1206.288136</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>1.068085</td>\n",
       "      <td>0.281313</td>\n",
       "      <td>0.269215</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>26.131340</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>23.011535</td>\n",
       "      <td>-12.728496</td>\n",
       "      <td>8.075412</td>\n",
       "      <td>2.314124</td>\n",
       "      <td>10.166097</td>\n",
       "      <td>-1.258650</td>\n",
       "      <td>-5.445485</td>\n",
       "      <td>4.485016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507646</td>\n",
       "      <td>1.174809</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>1314.291304</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>0.359282</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>16.477927</td>\n",
       "      <td>17.254603</td>\n",
       "      <td>15.624358</td>\n",
       "      <td>22.925548</td>\n",
       "      <td>-3.732207</td>\n",
       "      <td>-5.545847</td>\n",
       "      <td>-0.287058</td>\n",
       "      <td>2.493493</td>\n",
       "      <td>-2.856931</td>\n",
       "      <td>-1.472151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829380</td>\n",
       "      <td>1.724176</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.034167</td>\n",
       "      <td>1052.803008</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>1.692771</td>\n",
       "      <td>0.187160</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>30.734405</td>\n",
       "      <td>0.195722</td>\n",
       "      <td>12.751227</td>\n",
       "      <td>-7.813799</td>\n",
       "      <td>6.543426</td>\n",
       "      <td>-2.407274</td>\n",
       "      <td>-2.644401</td>\n",
       "      <td>1.560591</td>\n",
       "      <td>-2.331377</td>\n",
       "      <td>13.273872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257941</td>\n",
       "      <td>1.568366</td>\n",
       "      <td>0.281967</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>1335.285246</td>\n",
       "      <td>0.249180</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.216469</td>\n",
       "      <td>0.567205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>11.901789</td>\n",
       "      <td>2.450261</td>\n",
       "      <td>-10.521368</td>\n",
       "      <td>-6.382197</td>\n",
       "      <td>-5.168251</td>\n",
       "      <td>-3.993819</td>\n",
       "      <td>-6.422926</td>\n",
       "      <td>3.484094</td>\n",
       "      <td>-3.876889</td>\n",
       "      <td>4.011051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379774</td>\n",
       "      <td>1.355320</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>0.668611</td>\n",
       "      <td>1236.621359</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.287445</td>\n",
       "      <td>0.441319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>28.762754</td>\n",
       "      <td>-1.056746</td>\n",
       "      <td>-3.490334</td>\n",
       "      <td>-7.397567</td>\n",
       "      <td>-6.997795</td>\n",
       "      <td>20.513098</td>\n",
       "      <td>6.709869</td>\n",
       "      <td>1.375369</td>\n",
       "      <td>-1.890947</td>\n",
       "      <td>-4.380331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519832</td>\n",
       "      <td>1.414330</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.056944</td>\n",
       "      <td>1375.405844</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>1.211726</td>\n",
       "      <td>0.250042</td>\n",
       "      <td>0.779798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>0.742950</td>\n",
       "      <td>-2.627058</td>\n",
       "      <td>-3.835256</td>\n",
       "      <td>-6.145804</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>-3.033757</td>\n",
       "      <td>-0.100329</td>\n",
       "      <td>-0.043938</td>\n",
       "      <td>-1.506061</td>\n",
       "      <td>-1.085429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>1.577346</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.459306</td>\n",
       "      <td>1072.140000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.567839</td>\n",
       "      <td>0.235550</td>\n",
       "      <td>0.698647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>22.258937</td>\n",
       "      <td>-0.865007</td>\n",
       "      <td>15.276752</td>\n",
       "      <td>-17.701465</td>\n",
       "      <td>14.155350</td>\n",
       "      <td>-4.693674</td>\n",
       "      <td>-2.443168</td>\n",
       "      <td>-2.805695</td>\n",
       "      <td>-2.804628</td>\n",
       "      <td>-2.417563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244673</td>\n",
       "      <td>1.444424</td>\n",
       "      <td>0.187793</td>\n",
       "      <td>0.311389</td>\n",
       "      <td>1383.929577</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>1.179245</td>\n",
       "      <td>0.269149</td>\n",
       "      <td>0.440071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>25.401560</td>\n",
       "      <td>8.282808</td>\n",
       "      <td>-31.277136</td>\n",
       "      <td>13.145835</td>\n",
       "      <td>16.997314</td>\n",
       "      <td>-7.945109</td>\n",
       "      <td>-8.147648</td>\n",
       "      <td>-8.896687</td>\n",
       "      <td>-5.843419</td>\n",
       "      <td>-0.626392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206216</td>\n",
       "      <td>1.502625</td>\n",
       "      <td>0.374194</td>\n",
       "      <td>0.606944</td>\n",
       "      <td>1226.735484</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>1.291262</td>\n",
       "      <td>0.242310</td>\n",
       "      <td>0.074802</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>13.706690</td>\n",
       "      <td>7.791517</td>\n",
       "      <td>7.306411</td>\n",
       "      <td>10.541593</td>\n",
       "      <td>-9.516593</td>\n",
       "      <td>3.309520</td>\n",
       "      <td>-1.542335</td>\n",
       "      <td>4.351730</td>\n",
       "      <td>-3.554876</td>\n",
       "      <td>-5.031538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099977</td>\n",
       "      <td>0.877015</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.395417</td>\n",
       "      <td>1083.274038</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>0.573494</td>\n",
       "      <td>0.482329</td>\n",
       "      <td>0.491294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>26.912932</td>\n",
       "      <td>-0.454641</td>\n",
       "      <td>14.015744</td>\n",
       "      <td>-9.998340</td>\n",
       "      <td>-8.905399</td>\n",
       "      <td>8.952896</td>\n",
       "      <td>-11.056320</td>\n",
       "      <td>6.633441</td>\n",
       "      <td>-5.127221</td>\n",
       "      <td>-1.530622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206427</td>\n",
       "      <td>0.998815</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>1320.083893</td>\n",
       "      <td>0.251678</td>\n",
       "      <td>0.690236</td>\n",
       "      <td>0.473807</td>\n",
       "      <td>0.210702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>-30.570426</td>\n",
       "      <td>24.732419</td>\n",
       "      <td>-2.079454</td>\n",
       "      <td>-11.137881</td>\n",
       "      <td>1.518234</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.446444</td>\n",
       "      <td>-1.345274</td>\n",
       "      <td>1.313954</td>\n",
       "      <td>0.345504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559664</td>\n",
       "      <td>0.747592</td>\n",
       "      <td>0.563981</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>856.459716</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.496193</td>\n",
       "      <td>0.976002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-5.828762</td>\n",
       "      <td>2.545959</td>\n",
       "      <td>-0.645852</td>\n",
       "      <td>1.055244</td>\n",
       "      <td>-12.794977</td>\n",
       "      <td>-0.612679</td>\n",
       "      <td>-6.507652</td>\n",
       "      <td>0.730545</td>\n",
       "      <td>-2.138035</td>\n",
       "      <td>-1.641358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006055</td>\n",
       "      <td>1.310332</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.496944</td>\n",
       "      <td>986.786713</td>\n",
       "      <td>0.251748</td>\n",
       "      <td>1.017544</td>\n",
       "      <td>0.306739</td>\n",
       "      <td>0.998705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>22.675271</td>\n",
       "      <td>-1.925411</td>\n",
       "      <td>13.604115</td>\n",
       "      <td>-20.048106</td>\n",
       "      <td>-3.945546</td>\n",
       "      <td>2.531374</td>\n",
       "      <td>5.503931</td>\n",
       "      <td>-0.571000</td>\n",
       "      <td>0.317331</td>\n",
       "      <td>7.455540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073768</td>\n",
       "      <td>1.407055</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>1436.803828</td>\n",
       "      <td>0.248804</td>\n",
       "      <td>1.139423</td>\n",
       "      <td>0.290264</td>\n",
       "      <td>0.143894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>-28.685449</td>\n",
       "      <td>28.085449</td>\n",
       "      <td>2.383516</td>\n",
       "      <td>-6.788596</td>\n",
       "      <td>2.347656</td>\n",
       "      <td>0.579053</td>\n",
       "      <td>0.203732</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-0.268027</td>\n",
       "      <td>-1.756875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403970</td>\n",
       "      <td>1.070504</td>\n",
       "      <td>0.466135</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>901.521912</td>\n",
       "      <td>0.239044</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.400232</td>\n",
       "      <td>0.802037</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>15.635537</td>\n",
       "      <td>-4.674515</td>\n",
       "      <td>-21.750005</td>\n",
       "      <td>-6.050642</td>\n",
       "      <td>-6.309088</td>\n",
       "      <td>-6.900510</td>\n",
       "      <td>-12.422318</td>\n",
       "      <td>-2.050268</td>\n",
       "      <td>-4.072678</td>\n",
       "      <td>3.729193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569125</td>\n",
       "      <td>1.468102</td>\n",
       "      <td>0.294372</td>\n",
       "      <td>0.048333</td>\n",
       "      <td>1323.142857</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>1.339130</td>\n",
       "      <td>0.239145</td>\n",
       "      <td>0.779076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>31.667955</td>\n",
       "      <td>3.968138</td>\n",
       "      <td>-15.258337</td>\n",
       "      <td>-11.228208</td>\n",
       "      <td>-4.849364</td>\n",
       "      <td>-6.858703</td>\n",
       "      <td>2.468430</td>\n",
       "      <td>-1.729865</td>\n",
       "      <td>-0.044167</td>\n",
       "      <td>0.559501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530439</td>\n",
       "      <td>1.379800</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>1479.183099</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.190813</td>\n",
       "      <td>0.253323</td>\n",
       "      <td>0.972140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>5.693312</td>\n",
       "      <td>3.724910</td>\n",
       "      <td>0.837205</td>\n",
       "      <td>-2.108318</td>\n",
       "      <td>3.092695</td>\n",
       "      <td>0.562513</td>\n",
       "      <td>0.972588</td>\n",
       "      <td>1.002036</td>\n",
       "      <td>4.446282</td>\n",
       "      <td>0.449313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322635</td>\n",
       "      <td>1.302367</td>\n",
       "      <td>0.220408</td>\n",
       "      <td>0.696111</td>\n",
       "      <td>1116.616327</td>\n",
       "      <td>0.236735</td>\n",
       "      <td>0.897541</td>\n",
       "      <td>0.301858</td>\n",
       "      <td>0.677369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>-3.423319</td>\n",
       "      <td>17.990912</td>\n",
       "      <td>-9.466775</td>\n",
       "      <td>-8.092630</td>\n",
       "      <td>0.987898</td>\n",
       "      <td>3.026215</td>\n",
       "      <td>-4.959364</td>\n",
       "      <td>-1.497612</td>\n",
       "      <td>7.117767</td>\n",
       "      <td>1.759992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>1.537289</td>\n",
       "      <td>0.336032</td>\n",
       "      <td>0.601389</td>\n",
       "      <td>1153.404858</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>1.264228</td>\n",
       "      <td>0.248472</td>\n",
       "      <td>0.449470</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>-2.689915</td>\n",
       "      <td>5.327463</td>\n",
       "      <td>0.237086</td>\n",
       "      <td>8.228715</td>\n",
       "      <td>-0.121224</td>\n",
       "      <td>0.351049</td>\n",
       "      <td>-2.448782</td>\n",
       "      <td>-0.694674</td>\n",
       "      <td>-3.174310</td>\n",
       "      <td>-1.754148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129394</td>\n",
       "      <td>1.087075</td>\n",
       "      <td>0.029326</td>\n",
       "      <td>0.446528</td>\n",
       "      <td>976.988270</td>\n",
       "      <td>0.249267</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.393951</td>\n",
       "      <td>0.790033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>2.436708</td>\n",
       "      <td>2.565077</td>\n",
       "      <td>2.710996</td>\n",
       "      <td>2.542875</td>\n",
       "      <td>-0.756122</td>\n",
       "      <td>-1.675883</td>\n",
       "      <td>0.576098</td>\n",
       "      <td>-1.094740</td>\n",
       "      <td>1.337314</td>\n",
       "      <td>-3.562950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647279</td>\n",
       "      <td>1.152592</td>\n",
       "      <td>0.329218</td>\n",
       "      <td>0.849167</td>\n",
       "      <td>1036.823045</td>\n",
       "      <td>0.251029</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.354096</td>\n",
       "      <td>0.779507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>-36.729628</td>\n",
       "      <td>36.734406</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-7.837553</td>\n",
       "      <td>-0.683447</td>\n",
       "      <td>1.128580</td>\n",
       "      <td>0.061302</td>\n",
       "      <td>-1.135128</td>\n",
       "      <td>0.110130</td>\n",
       "      <td>0.619779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473018</td>\n",
       "      <td>0.800311</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.987778</td>\n",
       "      <td>851.797101</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.477421</td>\n",
       "      <td>0.954013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>15.530337</td>\n",
       "      <td>-4.464295</td>\n",
       "      <td>12.255507</td>\n",
       "      <td>-9.370663</td>\n",
       "      <td>-5.453862</td>\n",
       "      <td>1.494034</td>\n",
       "      <td>7.607356</td>\n",
       "      <td>-7.700408</td>\n",
       "      <td>-5.541105</td>\n",
       "      <td>9.505316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316729</td>\n",
       "      <td>1.405600</td>\n",
       "      <td>0.162055</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>1246.418972</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>1.146825</td>\n",
       "      <td>0.261010</td>\n",
       "      <td>0.590148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>11.747589</td>\n",
       "      <td>45.567337</td>\n",
       "      <td>6.371927</td>\n",
       "      <td>23.781958</td>\n",
       "      <td>9.076805</td>\n",
       "      <td>-1.852480</td>\n",
       "      <td>-0.988570</td>\n",
       "      <td>1.938058</td>\n",
       "      <td>-2.953191</td>\n",
       "      <td>-0.210857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416527</td>\n",
       "      <td>1.057506</td>\n",
       "      <td>0.228041</td>\n",
       "      <td>0.816944</td>\n",
       "      <td>1059.508446</td>\n",
       "      <td>0.241554</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.429151</td>\n",
       "      <td>0.403759</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          svd_1      svd_2      svd_3      svd_4      svd_5      svd_6  \\\n",
       "user                                                                     \n",
       "26    19.405226   1.056769 -21.038527   1.067305  26.873600   3.613976   \n",
       "199   24.701986   4.278532  -6.918118 -14.194389   4.933048   8.060955   \n",
       "202  -30.651479  27.972663  -1.808480 -11.094628   0.104153   0.539192   \n",
       "205    4.077090  27.859159  17.445291  -5.786915 -12.944545  -2.128206   \n",
       "231   34.053427   3.629981 -14.458684  -5.522036 -10.271650  -4.786838   \n",
       "284   19.995440  -3.963606  36.543998 -12.160488  -0.507532  10.580456   \n",
       "424   19.515725   1.946307 -11.297455 -12.750778  -1.955778   5.562889   \n",
       "459   19.229018  -4.886454  20.073144 -14.276159   7.799430   1.149055   \n",
       "469   15.833040  10.900338  17.698568 -12.457373   2.723195  -2.610725   \n",
       "561   12.384787  -1.564100  -9.387893  -5.248689   2.829056   3.688155   \n",
       "667   35.282715   1.605001   8.985617   3.324267 -25.776958   8.908518   \n",
       "699   22.899398  28.790211  -2.853864   0.518014  -5.737385  13.667635   \n",
       "730   10.109697  -0.534179   9.083594  -9.989619   5.325208   3.840458   \n",
       "786   28.727091   4.460696 -17.737821  -8.568014   2.184781  -4.353866   \n",
       "849   16.708753  -1.445953   4.006640 -10.407775  -1.372284   4.169791   \n",
       "1045 -14.589255  24.841091   0.592879 -12.930398   3.003011  -0.479167   \n",
       "1057  50.942770  10.806731 -19.721361   8.340857 -32.353401   0.202926   \n",
       "1066   6.660772  18.774476   2.211635 -13.141572 -12.306636   3.991652   \n",
       "1151   4.565552   2.001047  -4.991591  -2.488794   3.774549  -0.880130   \n",
       "1279  55.143343  12.673649  21.268550  27.536053  -8.691158   1.590301   \n",
       "1289  -3.964172  22.893215  -5.001387 -11.151228   4.800311   5.292133   \n",
       "1307   9.982597   1.233299   4.366212  -4.498651  -0.542160  -3.126854   \n",
       "1450  33.229563   5.877482 -19.276149  -9.445955  -9.482312  -5.610207   \n",
       "1455  -9.410127   1.405786  -4.863681   4.097648  -0.428274   0.271327   \n",
       "1561  18.630213   0.309461   4.104553 -12.629557  -7.013169  -0.524197   \n",
       "1563  46.562663   8.590602  -5.380619  17.140850  -0.568038   1.429418   \n",
       "1617  16.365574  -5.234236  18.390408 -18.399680  -0.653204  -1.960561   \n",
       "1640  48.347671   6.326245  26.561090   3.600284 -18.627871   4.337232   \n",
       "1682  -8.846367  22.965476  -9.353197  -9.323590  -1.110193  -6.428081   \n",
       "1722  32.194061   4.328843 -12.861302  -8.997731  -0.403383   5.000128   \n",
       "1818 -11.240644  38.191345   2.053651   0.160269  -3.187149   0.353069   \n",
       "1822 -22.530234  31.830443  -5.155541  -6.073602   2.675920   1.076088   \n",
       "1865 -25.681521  25.066416  -0.517062 -11.098346   0.320960  -0.722261   \n",
       "1903  12.691205   1.361096   1.730498  -4.063565   2.698011  -0.263857   \n",
       "2093  26.208748   1.223819  -0.750656 -14.497578  21.059123   1.539668   \n",
       "2120  25.898912  -0.875074  17.089111 -16.163478  -3.181579  -3.717163   \n",
       "2163   5.944021  10.971361  -7.028612  -3.729516  16.252878   4.574362   \n",
       "2225  26.131340   0.000671  23.011535 -12.728496   8.075412   2.314124   \n",
       "2328  16.477927  17.254603  15.624358  22.925548  -3.732207  -5.545847   \n",
       "2390  30.734405   0.195722  12.751227  -7.813799   6.543426  -2.407274   \n",
       "2458  11.901789   2.450261 -10.521368  -6.382197  -5.168251  -3.993819   \n",
       "2543  28.762754  -1.056746  -3.490334  -7.397567  -6.997795  20.513098   \n",
       "2548   0.742950  -2.627058  -3.835256  -6.145804   0.009457  -3.033757   \n",
       "2595  22.258937  -0.865007  15.276752 -17.701465  14.155350  -4.693674   \n",
       "2648  25.401560   8.282808 -31.277136  13.145835  16.997314  -7.945109   \n",
       "2767  13.706690   7.791517   7.306411  10.541593  -9.516593   3.309520   \n",
       "2778  26.912932  -0.454641  14.015744  -9.998340  -8.905399   8.952896   \n",
       "2782 -30.570426  24.732419  -2.079454 -11.137881   1.518234   0.011092   \n",
       "2800  -5.828762   2.545959  -0.645852   1.055244 -12.794977  -0.612679   \n",
       "2807  22.675271  -1.925411  13.604115 -20.048106  -3.945546   2.531374   \n",
       "2938 -28.685449  28.085449   2.383516  -6.788596   2.347656   0.579053   \n",
       "2980  15.635537  -4.674515 -21.750005  -6.050642  -6.309088  -6.900510   \n",
       "3082  31.667955   3.968138 -15.258337 -11.228208  -4.849364  -6.858703   \n",
       "3123   5.693312   3.724910   0.837205  -2.108318   3.092695   0.562513   \n",
       "3153  -3.423319  17.990912  -9.466775  -8.092630   0.987898   3.026215   \n",
       "3292  -2.689915   5.327463   0.237086   8.228715  -0.121224   0.351049   \n",
       "3401   2.436708   2.565077   2.710996   2.542875  -0.756122  -1.675883   \n",
       "3465 -36.729628  36.734406  -0.051635  -7.837553  -0.683447   1.128580   \n",
       "3499  15.530337  -4.464295  12.255507  -9.370663  -5.453862   1.494034   \n",
       "3576  11.747589  45.567337   6.371927  23.781958   9.076805  -1.852480   \n",
       "\n",
       "          svd_7      svd_8      svd_9     svd_10  ...  mean_item_alignment  \\\n",
       "user                                              ...                        \n",
       "26    -2.008504  -1.611251   4.041583  -4.170670  ...            -0.399362   \n",
       "199    6.287393   4.449363  -1.056309  -0.967260  ...             0.100368   \n",
       "202   -0.447467  -1.427426  -0.590070   0.858616  ...             0.477618   \n",
       "205   -3.253641  -6.447440   2.451911  -3.451880  ...             0.170565   \n",
       "231    2.645228   9.534818  -3.295730   0.831030  ...            -0.633656   \n",
       "284   -0.204711   1.751987   4.764085  -3.733991  ...             0.846178   \n",
       "424    2.733605  -0.980205  -1.747588  -3.314120  ...             0.079770   \n",
       "459    9.521034  -0.200415  -3.550065   5.731454  ...            -0.395315   \n",
       "469    8.489573  -7.575831  -0.392949  -2.492455  ...             0.079066   \n",
       "561   -6.718119  -1.332926  -4.809891   2.177380  ...             0.143844   \n",
       "667   -4.520455   3.688603   1.437349   4.809446  ...            -0.362430   \n",
       "699    7.112067   7.904376  -0.047028   1.585041  ...             0.315420   \n",
       "730   -0.516786  -0.355747  -4.024504   4.291348  ...            -0.352857   \n",
       "786   -7.278990   8.086681  -4.452230  -3.061602  ...            -0.529118   \n",
       "849   -7.593610  -1.823122  -0.533504  -0.710331  ...             0.282600   \n",
       "1045   0.592950   1.268763   0.800157  -2.415303  ...             0.238899   \n",
       "1057   9.124116   0.243394  -3.626521   2.615711  ...             0.540825   \n",
       "1066  11.318564   2.828238   3.027072  -6.186215  ...             0.381093   \n",
       "1151  -1.752852  -1.050395   1.504125  -2.953061  ...             0.292262   \n",
       "1279   1.867865   2.002636 -10.896158   5.159406  ...            -0.150875   \n",
       "1289   4.868103   5.233376  -2.822094   1.064827  ...             0.362125   \n",
       "1307   5.185793   1.389276  -1.535660  -0.518658  ...            -0.148607   \n",
       "1450   2.965458  -3.737145  -8.958739   1.904718  ...            -0.260829   \n",
       "1455   6.355352  -0.929348  -0.500602  -0.158288  ...            -0.687801   \n",
       "1561  -2.458145   4.225193   4.917298  -0.953974  ...             0.040100   \n",
       "1563   4.274856  11.332335  -3.256507  -1.132952  ...            -0.379207   \n",
       "1617  -5.301496  -8.029610  -0.166289  -5.879510  ...            -0.554063   \n",
       "1640 -23.631518   0.098326  -5.827032   3.219304  ...             0.101233   \n",
       "1682  -0.770292  -3.809209  -0.839959   1.847927  ...             0.252205   \n",
       "1722   3.642669  -3.721963  -6.917732   0.772333  ...            -0.000830   \n",
       "1818  -3.564333  -4.061618   2.608191  -3.025811  ...             0.144829   \n",
       "1822  -2.166935  -1.123116   0.019499  -0.718346  ...             0.342201   \n",
       "1865  -0.316471  -1.407688  -1.067839   2.649285  ...             0.388896   \n",
       "1903  -2.108461   0.051035  -0.066095   0.649859  ...            -0.131732   \n",
       "2093   3.829611  -2.383363   5.219389   4.505694  ...            -0.490975   \n",
       "2120 -12.909274   0.747702   7.321352  -2.666413  ...            -0.345681   \n",
       "2163  -5.971668   6.157987   4.949524   5.259026  ...             0.066686   \n",
       "2225  10.166097  -1.258650  -5.445485   4.485016  ...             0.507646   \n",
       "2328  -0.287058   2.493493  -2.856931  -1.472151  ...            -0.829380   \n",
       "2390  -2.644401   1.560591  -2.331377  13.273872  ...            -0.257941   \n",
       "2458  -6.422926   3.484094  -3.876889   4.011051  ...             0.379774   \n",
       "2543   6.709869   1.375369  -1.890947  -4.380331  ...            -0.519832   \n",
       "2548  -0.100329  -0.043938  -1.506061  -1.085429  ...            -0.049011   \n",
       "2595  -2.443168  -2.805695  -2.804628  -2.417563  ...            -0.244673   \n",
       "2648  -8.147648  -8.896687  -5.843419  -0.626392  ...             0.206216   \n",
       "2767  -1.542335   4.351730  -3.554876  -5.031538  ...            -0.099977   \n",
       "2778 -11.056320   6.633441  -5.127221  -1.530622  ...            -0.206427   \n",
       "2782   0.446444  -1.345274   1.313954   0.345504  ...             0.559664   \n",
       "2800  -6.507652   0.730545  -2.138035  -1.641358  ...            -0.006055   \n",
       "2807   5.503931  -0.571000   0.317331   7.455540  ...            -0.073768   \n",
       "2938   0.203732  -0.804791  -0.268027  -1.756875  ...             0.403970   \n",
       "2980 -12.422318  -2.050268  -4.072678   3.729193  ...            -0.569125   \n",
       "3082   2.468430  -1.729865  -0.044167   0.559501  ...            -0.530439   \n",
       "3123   0.972588   1.002036   4.446282   0.449313  ...             0.322635   \n",
       "3153  -4.959364  -1.497612   7.117767   1.759992  ...             0.000731   \n",
       "3292  -2.448782  -0.694674  -3.174310  -1.754148  ...            -0.129394   \n",
       "3401   0.576098  -1.094740   1.337314  -3.562950  ...             0.647279   \n",
       "3465   0.061302  -1.135128   0.110130   0.619779  ...             0.473018   \n",
       "3499   7.607356  -7.700408  -5.541105   9.505316  ...            -0.316729   \n",
       "3576  -0.988570   1.938058  -2.953191  -0.210857  ...             0.416527   \n",
       "\n",
       "      rating_entropy  extreme_ratio  user_mean_rank  avg_item_popularity  \\\n",
       "user                                                                       \n",
       "26          1.503556       0.151639        0.248333          1296.213115   \n",
       "199         1.441563       0.189573        0.515833          1429.568720   \n",
       "202         0.745018       0.471963        0.987222           873.476636   \n",
       "205         1.399122       0.234332        0.651111          1128.376022   \n",
       "231         1.384519       0.265363        0.028056          1383.776536   \n",
       "284         0.858244       0.219731        0.859167          1249.322870   \n",
       "424         1.255755       0.075000        0.479167          1388.605000   \n",
       "459         1.475948       0.256522        0.178333          1343.843478   \n",
       "469         1.315856       0.148699        0.533056          1279.308550   \n",
       "561         1.446732       0.214634        0.543611          1214.692683   \n",
       "667         1.339007       0.173423        0.137222          1260.198198   \n",
       "699         1.338535       0.246649        0.706111          1237.621984   \n",
       "730         1.234931       0.047009        0.249583          1190.303419   \n",
       "786         1.389305       0.174216        0.098611          1406.651568   \n",
       "849         1.200819       0.099057        0.604167          1270.547170   \n",
       "1045        1.287192       0.387234        0.809722          1072.093617   \n",
       "1057        1.136690       0.119403        0.678333          1306.529851   \n",
       "1066        1.201517       0.214575        0.741111          1248.655870   \n",
       "1151        1.300817       0.210046        0.677778          1109.118721   \n",
       "1279        1.158217       0.070312        0.300278          1184.531250   \n",
       "1289        1.069240       0.256303        0.836667          1130.827731   \n",
       "1307        1.239811       0.078067        0.373472          1155.007435   \n",
       "1450        1.496491       0.186851        0.235556          1422.553633   \n",
       "1455        1.301151       0.092308        0.153889           919.012308   \n",
       "1561        1.513973       0.182222        0.436944          1292.951111   \n",
       "1563        1.188743       0.053537        0.162500          1250.078394   \n",
       "1617        1.378308       0.200893        0.083889          1359.825893   \n",
       "1640        1.553445       0.273707        0.433056          1268.467672   \n",
       "1682        1.291371       0.424490        0.792500          1076.918367   \n",
       "1722        1.565127       0.223881        0.401944          1381.936567   \n",
       "1818        1.375909       0.310345        0.735556          1036.200000   \n",
       "1822        1.073918       0.371025        0.890833           960.448763   \n",
       "1865        1.000470       0.412281        0.928611           929.228070   \n",
       "1903        1.154581       0.053640        0.388194          1176.053640   \n",
       "2093        1.552178       0.345455        0.184722          1452.404545   \n",
       "2120        1.414951       0.164794        0.192500          1379.501873   \n",
       "2163        1.431018       0.211864        0.595556          1206.288136   \n",
       "2225        1.174809       0.182609        0.729444          1314.291304   \n",
       "2328        1.724176       0.345865        0.034167          1052.803008   \n",
       "2390        1.568366       0.281967        0.243056          1335.285246   \n",
       "2458        1.355320       0.242718        0.668611          1236.621359   \n",
       "2543        1.414330       0.211039        0.056944          1375.405844   \n",
       "2548        1.577346       0.230000        0.459306          1072.140000   \n",
       "2595        1.444424       0.187793        0.311389          1383.929577   \n",
       "2648        1.502625       0.374194        0.606944          1226.735484   \n",
       "2767        0.877015       0.002404        0.395417          1083.274038   \n",
       "2778        0.998815       0.063758        0.253333          1320.083893   \n",
       "2782        0.747592       0.563981        0.998056           856.459716   \n",
       "2800        1.310332       0.115385        0.496944           986.786713   \n",
       "2807        1.407055       0.114833        0.317500          1436.803828   \n",
       "2938        1.070504       0.466135        0.923889           901.521912   \n",
       "2980        1.468102       0.294372        0.048333          1323.142857   \n",
       "3082        1.379800       0.225352        0.071111          1479.183099   \n",
       "3123        1.302367       0.220408        0.696111          1116.616327   \n",
       "3153        1.537289       0.336032        0.601389          1153.404858   \n",
       "3292        1.087075       0.029326        0.446528           976.988270   \n",
       "3401        1.152592       0.329218        0.849167          1036.823045   \n",
       "3465        0.800311       0.489855        0.987778           851.797101   \n",
       "3499        1.405600       0.162055        0.205556          1246.418972   \n",
       "3576        1.057506       0.228041        0.816944          1059.508446   \n",
       "\n",
       "      rare_item_ratio  rating_volatility  rating_concentration     label  \\\n",
       "user                                                                       \n",
       "26           0.250000           1.395062              0.254065  0.558222   \n",
       "199          0.251185           1.219048              0.276117  0.165112   \n",
       "202          0.252336           0.516432              0.491833  0.925028   \n",
       "205          0.250681           1.019126              0.279347  0.380860   \n",
       "231          0.251397           1.263305              0.250882  0.951103   \n",
       "284          0.251121           0.540541              0.511392  0.055162   \n",
       "424          0.250000           0.864322              0.327250  0.255181   \n",
       "459          0.252174           1.371179              0.242722  0.739300   \n",
       "469          0.249071           0.891791              0.311618  0.233492   \n",
       "561          0.248780           1.171569              0.263534  0.383316   \n",
       "667          0.250000           1.031603              0.285965  0.558745   \n",
       "699          0.249330           0.930108              0.297242  0.261752   \n",
       "730          0.252137           0.948498              0.341369  0.311928   \n",
       "786          0.250871           1.220280              0.269920  0.549116   \n",
       "849          0.250000           0.895735              0.359381  0.301816   \n",
       "1045         0.246809           1.004274              0.321865  0.638008   \n",
       "1057         0.251244           0.807980              0.428195  0.057549   \n",
       "1066         0.251012           0.841463              0.334065  0.339369   \n",
       "1151         0.251142           0.922018              0.316862  0.625202   \n",
       "1279         0.250000           0.784038              0.419419  0.099430   \n",
       "1289         0.252101           0.755274              0.413424  0.529647   \n",
       "1307         0.245353           0.888060              0.346416  0.507218   \n",
       "1450         0.245675           1.253472              0.240550  0.426661   \n",
       "1455         0.249231           1.046296              0.300317  0.979873   \n",
       "1561         0.248889           1.187500              0.258746  0.245883   \n",
       "1563         0.248566           0.768199              0.352295  0.008819   \n",
       "1617         0.250000           1.228700              0.253866  0.903824   \n",
       "1640         0.247845           1.382289              0.219986  0.203252   \n",
       "1682         0.244898           0.991803              0.327514  0.587148   \n",
       "1722         0.250000           1.228464              0.229923  0.150973   \n",
       "1818         0.243678           0.907834              0.274652  0.580426   \n",
       "1822         0.250883           0.687943              0.393200  0.746696   \n",
       "1865         0.245614           0.638767              0.416551  0.873609   \n",
       "1903         0.249042           0.776923              0.375024  0.520802   \n",
       "2093         0.250000           1.534247              0.227603  0.805789   \n",
       "2120         0.250936           1.135338              0.263982  0.671698   \n",
       "2163         0.245763           1.068085              0.281313  0.269215   \n",
       "2225         0.252174           0.786026              0.359282  0.072081   \n",
       "2328         0.248120           1.692771              0.187160  0.288185   \n",
       "2390         0.249180           1.312500              0.216469  0.567205   \n",
       "2458         0.252427           0.975610              0.287445  0.441319   \n",
       "2543         0.246753           1.211726              0.250042  0.779798   \n",
       "2548         0.250000           1.567839              0.235550  0.698647   \n",
       "2595         0.248826           1.179245              0.269149  0.440071   \n",
       "2648         0.251613           1.291262              0.242310  0.074802   \n",
       "2767         0.247596           0.573494              0.482329  0.491294   \n",
       "2778         0.251678           0.690236              0.473807  0.210702   \n",
       "2782         0.251185           0.571429              0.496193  0.976002   \n",
       "2800         0.251748           1.017544              0.306739  0.998705   \n",
       "2807         0.248804           1.139423              0.290264  0.143894   \n",
       "2938         0.239044           0.792000              0.400232  0.802037   \n",
       "2980         0.251082           1.339130              0.239145  0.779076   \n",
       "3082         0.250000           1.190813              0.253323  0.972140   \n",
       "3123         0.236735           0.897541              0.301858  0.677369   \n",
       "3153         0.251012           1.264228              0.248472  0.449470   \n",
       "3292         0.249267           0.785294              0.393951  0.790033   \n",
       "3401         0.251029           0.867769              0.354096  0.779507   \n",
       "3465         0.246377           0.546512              0.477421  0.954013   \n",
       "3499         0.249012           1.146825              0.261010  0.590148   \n",
       "3576         0.241554           0.639594              0.429151  0.403759   \n",
       "\n",
       "      anomtype  \n",
       "user            \n",
       "26           0  \n",
       "199          1  \n",
       "202          2  \n",
       "205          2  \n",
       "231          0  \n",
       "284          2  \n",
       "424          1  \n",
       "459          0  \n",
       "469          2  \n",
       "561          1  \n",
       "667          0  \n",
       "699          2  \n",
       "730          1  \n",
       "786          0  \n",
       "849          1  \n",
       "1045         2  \n",
       "1057         0  \n",
       "1066         2  \n",
       "1151         1  \n",
       "1279         0  \n",
       "1289         2  \n",
       "1307         1  \n",
       "1450         0  \n",
       "1455         1  \n",
       "1561         1  \n",
       "1563         2  \n",
       "1617         0  \n",
       "1640         0  \n",
       "1682         2  \n",
       "1722         1  \n",
       "1818         2  \n",
       "1822         2  \n",
       "1865         2  \n",
       "1903         1  \n",
       "2093         0  \n",
       "2120         0  \n",
       "2163         2  \n",
       "2225         1  \n",
       "2328         1  \n",
       "2390         0  \n",
       "2458         1  \n",
       "2543         0  \n",
       "2548         1  \n",
       "2595         0  \n",
       "2648         2  \n",
       "2767         1  \n",
       "2778         0  \n",
       "2782         2  \n",
       "2800         1  \n",
       "2807         0  \n",
       "2938         2  \n",
       "2980         0  \n",
       "3082         0  \n",
       "3123         1  \n",
       "3153         2  \n",
       "3292         1  \n",
       "3401         1  \n",
       "3465         2  \n",
       "3499         0  \n",
       "3576         2  \n",
       "\n",
       "[60 rows x 329 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "anomtype\n",
       "0    20\n",
       "1    20\n",
       "2    20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine dataframe with anomtype: use y_cat instead of y\n",
    "\n",
    "df_cla = engineer_features(X, y_cat)\n",
    "\n",
    "# We convert all column names to strings so it does not throw an error later\n",
    "df_cla.columns = df_cla.columns.astype(str)\n",
    "display(df_cla)\n",
    "df_cla[\"anomtype\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ecca7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_1</th>\n",
       "      <th>svd_2</th>\n",
       "      <th>svd_3</th>\n",
       "      <th>svd_4</th>\n",
       "      <th>svd_5</th>\n",
       "      <th>svd_6</th>\n",
       "      <th>svd_7</th>\n",
       "      <th>svd_8</th>\n",
       "      <th>svd_9</th>\n",
       "      <th>svd_10</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.405226</td>\n",
       "      <td>1.056769</td>\n",
       "      <td>-21.038527</td>\n",
       "      <td>1.067305</td>\n",
       "      <td>26.873600</td>\n",
       "      <td>3.613976</td>\n",
       "      <td>-2.008504</td>\n",
       "      <td>-1.611251</td>\n",
       "      <td>4.041583</td>\n",
       "      <td>-4.170670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439797</td>\n",
       "      <td>0.143443</td>\n",
       "      <td>-0.399362</td>\n",
       "      <td>1.503556</td>\n",
       "      <td>0.151639</td>\n",
       "      <td>0.248333</td>\n",
       "      <td>1296.213115</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.395062</td>\n",
       "      <td>0.254065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>24.701986</td>\n",
       "      <td>4.278532</td>\n",
       "      <td>-6.918118</td>\n",
       "      <td>-14.194389</td>\n",
       "      <td>4.933048</td>\n",
       "      <td>8.060955</td>\n",
       "      <td>6.287393</td>\n",
       "      <td>4.449363</td>\n",
       "      <td>-1.056309</td>\n",
       "      <td>-0.967260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.100368</td>\n",
       "      <td>1.441563</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>0.515833</td>\n",
       "      <td>1429.568720</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>1.219048</td>\n",
       "      <td>0.276117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-30.651479</td>\n",
       "      <td>27.972663</td>\n",
       "      <td>-1.808480</td>\n",
       "      <td>-11.094628</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.539192</td>\n",
       "      <td>-0.447467</td>\n",
       "      <td>-1.427426</td>\n",
       "      <td>-0.590070</td>\n",
       "      <td>0.858616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500438</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.477618</td>\n",
       "      <td>0.745018</td>\n",
       "      <td>0.471963</td>\n",
       "      <td>0.987222</td>\n",
       "      <td>873.476636</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.516432</td>\n",
       "      <td>0.491833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>4.077090</td>\n",
       "      <td>27.859159</td>\n",
       "      <td>17.445291</td>\n",
       "      <td>-5.786915</td>\n",
       "      <td>-12.944545</td>\n",
       "      <td>-2.128206</td>\n",
       "      <td>-3.253641</td>\n",
       "      <td>-6.447440</td>\n",
       "      <td>2.451911</td>\n",
       "      <td>-3.451880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171325</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.170565</td>\n",
       "      <td>1.399122</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>0.651111</td>\n",
       "      <td>1128.376022</td>\n",
       "      <td>0.250681</td>\n",
       "      <td>1.019126</td>\n",
       "      <td>0.279347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>34.053427</td>\n",
       "      <td>3.629981</td>\n",
       "      <td>-14.458684</td>\n",
       "      <td>-5.522036</td>\n",
       "      <td>-10.271650</td>\n",
       "      <td>-4.786838</td>\n",
       "      <td>2.645228</td>\n",
       "      <td>9.534818</td>\n",
       "      <td>-3.295730</td>\n",
       "      <td>0.831030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.691520</td>\n",
       "      <td>0.270950</td>\n",
       "      <td>-0.633656</td>\n",
       "      <td>1.384519</td>\n",
       "      <td>0.265363</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>1383.776536</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>1.263305</td>\n",
       "      <td>0.250882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>19.995440</td>\n",
       "      <td>-3.963606</td>\n",
       "      <td>36.543998</td>\n",
       "      <td>-12.160488</td>\n",
       "      <td>-0.507532</td>\n",
       "      <td>10.580456</td>\n",
       "      <td>-0.204711</td>\n",
       "      <td>1.751987</td>\n",
       "      <td>4.764085</td>\n",
       "      <td>-3.733991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915377</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>0.846178</td>\n",
       "      <td>0.858244</td>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.859167</td>\n",
       "      <td>1249.322870</td>\n",
       "      <td>0.251121</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.511392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>19.515725</td>\n",
       "      <td>1.946307</td>\n",
       "      <td>-11.297455</td>\n",
       "      <td>-12.750778</td>\n",
       "      <td>-1.955778</td>\n",
       "      <td>5.562889</td>\n",
       "      <td>2.733605</td>\n",
       "      <td>-0.980205</td>\n",
       "      <td>-1.747588</td>\n",
       "      <td>-3.314120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097125</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.079770</td>\n",
       "      <td>1.255755</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>1388.605000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.327250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>19.229018</td>\n",
       "      <td>-4.886454</td>\n",
       "      <td>20.073144</td>\n",
       "      <td>-14.276159</td>\n",
       "      <td>7.799430</td>\n",
       "      <td>1.149055</td>\n",
       "      <td>9.521034</td>\n",
       "      <td>-0.200415</td>\n",
       "      <td>-3.550065</td>\n",
       "      <td>5.731454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441914</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>-0.395315</td>\n",
       "      <td>1.475948</td>\n",
       "      <td>0.256522</td>\n",
       "      <td>0.178333</td>\n",
       "      <td>1343.843478</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>1.371179</td>\n",
       "      <td>0.242722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>15.833040</td>\n",
       "      <td>10.900338</td>\n",
       "      <td>17.698568</td>\n",
       "      <td>-12.457373</td>\n",
       "      <td>2.723195</td>\n",
       "      <td>-2.610725</td>\n",
       "      <td>8.489573</td>\n",
       "      <td>-7.575831</td>\n",
       "      <td>-0.392949</td>\n",
       "      <td>-2.492455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082205</td>\n",
       "      <td>0.048327</td>\n",
       "      <td>0.079066</td>\n",
       "      <td>1.315856</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.533056</td>\n",
       "      <td>1279.308550</td>\n",
       "      <td>0.249071</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.311618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>12.384787</td>\n",
       "      <td>-1.564100</td>\n",
       "      <td>-9.387893</td>\n",
       "      <td>-5.248689</td>\n",
       "      <td>2.829056</td>\n",
       "      <td>3.688155</td>\n",
       "      <td>-6.718119</td>\n",
       "      <td>-1.332926</td>\n",
       "      <td>-4.809891</td>\n",
       "      <td>2.177380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171960</td>\n",
       "      <td>0.180488</td>\n",
       "      <td>0.143844</td>\n",
       "      <td>1.446732</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.543611</td>\n",
       "      <td>1214.692683</td>\n",
       "      <td>0.248780</td>\n",
       "      <td>1.171569</td>\n",
       "      <td>0.263534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>35.282715</td>\n",
       "      <td>1.605001</td>\n",
       "      <td>8.985617</td>\n",
       "      <td>3.324267</td>\n",
       "      <td>-25.776958</td>\n",
       "      <td>8.908518</td>\n",
       "      <td>-4.520455</td>\n",
       "      <td>3.688603</td>\n",
       "      <td>1.437349</td>\n",
       "      <td>4.809446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393263</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>-0.362430</td>\n",
       "      <td>1.339007</td>\n",
       "      <td>0.173423</td>\n",
       "      <td>0.137222</td>\n",
       "      <td>1260.198198</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.031603</td>\n",
       "      <td>0.285965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>22.899398</td>\n",
       "      <td>28.790211</td>\n",
       "      <td>-2.853864</td>\n",
       "      <td>0.518014</td>\n",
       "      <td>-5.737385</td>\n",
       "      <td>13.667635</td>\n",
       "      <td>7.112067</td>\n",
       "      <td>7.904376</td>\n",
       "      <td>-0.047028</td>\n",
       "      <td>1.585041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339959</td>\n",
       "      <td>0.104558</td>\n",
       "      <td>0.315420</td>\n",
       "      <td>1.338535</td>\n",
       "      <td>0.246649</td>\n",
       "      <td>0.706111</td>\n",
       "      <td>1237.621984</td>\n",
       "      <td>0.249330</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.297242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>10.109697</td>\n",
       "      <td>-0.534179</td>\n",
       "      <td>9.083594</td>\n",
       "      <td>-9.989619</td>\n",
       "      <td>5.325208</td>\n",
       "      <td>3.840458</td>\n",
       "      <td>-0.516786</td>\n",
       "      <td>-0.355747</td>\n",
       "      <td>-4.024504</td>\n",
       "      <td>4.291348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371623</td>\n",
       "      <td>0.068376</td>\n",
       "      <td>-0.352857</td>\n",
       "      <td>1.234931</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>0.249583</td>\n",
       "      <td>1190.303419</td>\n",
       "      <td>0.252137</td>\n",
       "      <td>0.948498</td>\n",
       "      <td>0.341369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>28.727091</td>\n",
       "      <td>4.460696</td>\n",
       "      <td>-17.737821</td>\n",
       "      <td>-8.568014</td>\n",
       "      <td>2.184781</td>\n",
       "      <td>-4.353866</td>\n",
       "      <td>-7.278990</td>\n",
       "      <td>8.086681</td>\n",
       "      <td>-4.452230</td>\n",
       "      <td>-3.061602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579775</td>\n",
       "      <td>0.174216</td>\n",
       "      <td>-0.529118</td>\n",
       "      <td>1.389305</td>\n",
       "      <td>0.174216</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>1406.651568</td>\n",
       "      <td>0.250871</td>\n",
       "      <td>1.220280</td>\n",
       "      <td>0.269920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>16.708753</td>\n",
       "      <td>-1.445953</td>\n",
       "      <td>4.006640</td>\n",
       "      <td>-10.407775</td>\n",
       "      <td>-1.372284</td>\n",
       "      <td>4.169791</td>\n",
       "      <td>-7.593610</td>\n",
       "      <td>-1.823122</td>\n",
       "      <td>-0.533504</td>\n",
       "      <td>-0.710331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309073</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>1.200819</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>1270.547170</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.359381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>-14.589255</td>\n",
       "      <td>24.841091</td>\n",
       "      <td>0.592879</td>\n",
       "      <td>-12.930398</td>\n",
       "      <td>3.003011</td>\n",
       "      <td>-0.479167</td>\n",
       "      <td>0.592950</td>\n",
       "      <td>1.268763</td>\n",
       "      <td>0.800157</td>\n",
       "      <td>-2.415303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246324</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>0.238899</td>\n",
       "      <td>1.287192</td>\n",
       "      <td>0.387234</td>\n",
       "      <td>0.809722</td>\n",
       "      <td>1072.093617</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>1.004274</td>\n",
       "      <td>0.321865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>50.942770</td>\n",
       "      <td>10.806731</td>\n",
       "      <td>-19.721361</td>\n",
       "      <td>8.340857</td>\n",
       "      <td>-32.353401</td>\n",
       "      <td>0.202926</td>\n",
       "      <td>9.124116</td>\n",
       "      <td>0.243394</td>\n",
       "      <td>-3.626521</td>\n",
       "      <td>2.615711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580949</td>\n",
       "      <td>0.092040</td>\n",
       "      <td>0.540825</td>\n",
       "      <td>1.136690</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>1306.529851</td>\n",
       "      <td>0.251244</td>\n",
       "      <td>0.807980</td>\n",
       "      <td>0.428195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>6.660772</td>\n",
       "      <td>18.774476</td>\n",
       "      <td>2.211635</td>\n",
       "      <td>-13.141572</td>\n",
       "      <td>-12.306636</td>\n",
       "      <td>3.991652</td>\n",
       "      <td>11.318564</td>\n",
       "      <td>2.828238</td>\n",
       "      <td>3.027072</td>\n",
       "      <td>-6.186215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399783</td>\n",
       "      <td>0.012146</td>\n",
       "      <td>0.381093</td>\n",
       "      <td>1.201517</td>\n",
       "      <td>0.214575</td>\n",
       "      <td>0.741111</td>\n",
       "      <td>1248.655870</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.334065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>4.565552</td>\n",
       "      <td>2.001047</td>\n",
       "      <td>-4.991591</td>\n",
       "      <td>-2.488794</td>\n",
       "      <td>3.774549</td>\n",
       "      <td>-0.880130</td>\n",
       "      <td>-1.752852</td>\n",
       "      <td>-1.050395</td>\n",
       "      <td>1.504125</td>\n",
       "      <td>-2.953061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329951</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.292262</td>\n",
       "      <td>1.300817</td>\n",
       "      <td>0.210046</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>1109.118721</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.922018</td>\n",
       "      <td>0.316862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>55.143343</td>\n",
       "      <td>12.673649</td>\n",
       "      <td>21.268550</td>\n",
       "      <td>27.536053</td>\n",
       "      <td>-8.691158</td>\n",
       "      <td>1.590301</td>\n",
       "      <td>1.867865</td>\n",
       "      <td>2.002636</td>\n",
       "      <td>-10.896158</td>\n",
       "      <td>5.159406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164866</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>-0.150875</td>\n",
       "      <td>1.158217</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.300278</td>\n",
       "      <td>1184.531250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.784038</td>\n",
       "      <td>0.419419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>-3.964172</td>\n",
       "      <td>22.893215</td>\n",
       "      <td>-5.001387</td>\n",
       "      <td>-11.151228</td>\n",
       "      <td>4.800311</td>\n",
       "      <td>5.292133</td>\n",
       "      <td>4.868103</td>\n",
       "      <td>5.233376</td>\n",
       "      <td>-2.822094</td>\n",
       "      <td>1.064827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385464</td>\n",
       "      <td>0.021008</td>\n",
       "      <td>0.362125</td>\n",
       "      <td>1.069240</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>1130.827731</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.755274</td>\n",
       "      <td>0.413424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>9.982597</td>\n",
       "      <td>1.233299</td>\n",
       "      <td>4.366212</td>\n",
       "      <td>-4.498651</td>\n",
       "      <td>-0.542160</td>\n",
       "      <td>-3.126854</td>\n",
       "      <td>5.185793</td>\n",
       "      <td>1.389276</td>\n",
       "      <td>-1.535660</td>\n",
       "      <td>-0.518658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145973</td>\n",
       "      <td>0.078067</td>\n",
       "      <td>-0.148607</td>\n",
       "      <td>1.239811</td>\n",
       "      <td>0.078067</td>\n",
       "      <td>0.373472</td>\n",
       "      <td>1155.007435</td>\n",
       "      <td>0.245353</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.346416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>33.229563</td>\n",
       "      <td>5.877482</td>\n",
       "      <td>-19.276149</td>\n",
       "      <td>-9.445955</td>\n",
       "      <td>-9.482312</td>\n",
       "      <td>-5.610207</td>\n",
       "      <td>2.965458</td>\n",
       "      <td>-3.737145</td>\n",
       "      <td>-8.958739</td>\n",
       "      <td>1.904718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277095</td>\n",
       "      <td>0.155709</td>\n",
       "      <td>-0.260829</td>\n",
       "      <td>1.496491</td>\n",
       "      <td>0.186851</td>\n",
       "      <td>0.235556</td>\n",
       "      <td>1422.553633</td>\n",
       "      <td>0.245675</td>\n",
       "      <td>1.253472</td>\n",
       "      <td>0.240550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>-9.410127</td>\n",
       "      <td>1.405786</td>\n",
       "      <td>-4.863681</td>\n",
       "      <td>4.097648</td>\n",
       "      <td>-0.428274</td>\n",
       "      <td>0.271327</td>\n",
       "      <td>6.355352</td>\n",
       "      <td>-0.929348</td>\n",
       "      <td>-0.500602</td>\n",
       "      <td>-0.158288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709848</td>\n",
       "      <td>0.181538</td>\n",
       "      <td>-0.687801</td>\n",
       "      <td>1.301151</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.153889</td>\n",
       "      <td>919.012308</td>\n",
       "      <td>0.249231</td>\n",
       "      <td>1.046296</td>\n",
       "      <td>0.300317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>18.630213</td>\n",
       "      <td>0.309461</td>\n",
       "      <td>4.104553</td>\n",
       "      <td>-12.629557</td>\n",
       "      <td>-7.013169</td>\n",
       "      <td>-0.524197</td>\n",
       "      <td>-2.458145</td>\n",
       "      <td>4.225193</td>\n",
       "      <td>4.917298</td>\n",
       "      <td>-0.953974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049099</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>1.513973</td>\n",
       "      <td>0.182222</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>1292.951111</td>\n",
       "      <td>0.248889</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>0.258746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>46.562663</td>\n",
       "      <td>8.590602</td>\n",
       "      <td>-5.380619</td>\n",
       "      <td>17.140850</td>\n",
       "      <td>-0.568038</td>\n",
       "      <td>1.429418</td>\n",
       "      <td>4.274856</td>\n",
       "      <td>11.332335</td>\n",
       "      <td>-3.256507</td>\n",
       "      <td>-1.132952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405009</td>\n",
       "      <td>0.034417</td>\n",
       "      <td>-0.379207</td>\n",
       "      <td>1.188743</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>1250.078394</td>\n",
       "      <td>0.248566</td>\n",
       "      <td>0.768199</td>\n",
       "      <td>0.352295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>16.365574</td>\n",
       "      <td>-5.234236</td>\n",
       "      <td>18.390408</td>\n",
       "      <td>-18.399680</td>\n",
       "      <td>-0.653204</td>\n",
       "      <td>-1.960561</td>\n",
       "      <td>-5.301496</td>\n",
       "      <td>-8.029610</td>\n",
       "      <td>-0.166289</td>\n",
       "      <td>-5.879510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613316</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>-0.554063</td>\n",
       "      <td>1.378308</td>\n",
       "      <td>0.200893</td>\n",
       "      <td>0.083889</td>\n",
       "      <td>1359.825893</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228700</td>\n",
       "      <td>0.253866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>48.347671</td>\n",
       "      <td>6.326245</td>\n",
       "      <td>26.561090</td>\n",
       "      <td>3.600284</td>\n",
       "      <td>-18.627871</td>\n",
       "      <td>4.337232</td>\n",
       "      <td>-23.631518</td>\n",
       "      <td>0.098326</td>\n",
       "      <td>-5.827032</td>\n",
       "      <td>3.219304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106968</td>\n",
       "      <td>0.198276</td>\n",
       "      <td>0.101233</td>\n",
       "      <td>1.553445</td>\n",
       "      <td>0.273707</td>\n",
       "      <td>0.433056</td>\n",
       "      <td>1268.467672</td>\n",
       "      <td>0.247845</td>\n",
       "      <td>1.382289</td>\n",
       "      <td>0.219986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>-8.846367</td>\n",
       "      <td>22.965476</td>\n",
       "      <td>-9.353197</td>\n",
       "      <td>-9.323590</td>\n",
       "      <td>-1.110193</td>\n",
       "      <td>-6.428081</td>\n",
       "      <td>-0.770292</td>\n",
       "      <td>-3.809209</td>\n",
       "      <td>-0.839959</td>\n",
       "      <td>1.847927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267113</td>\n",
       "      <td>0.069388</td>\n",
       "      <td>0.252205</td>\n",
       "      <td>1.291371</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>1076.918367</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.327514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>32.194061</td>\n",
       "      <td>4.328843</td>\n",
       "      <td>-12.861302</td>\n",
       "      <td>-8.997731</td>\n",
       "      <td>-0.403383</td>\n",
       "      <td>5.000128</td>\n",
       "      <td>3.642669</td>\n",
       "      <td>-3.721963</td>\n",
       "      <td>-6.917732</td>\n",
       "      <td>0.772333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>-0.000830</td>\n",
       "      <td>1.565127</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.401944</td>\n",
       "      <td>1381.936567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.228464</td>\n",
       "      <td>0.229923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>-11.240644</td>\n",
       "      <td>38.191345</td>\n",
       "      <td>2.053651</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>-3.187149</td>\n",
       "      <td>0.353069</td>\n",
       "      <td>-3.564333</td>\n",
       "      <td>-4.061618</td>\n",
       "      <td>2.608191</td>\n",
       "      <td>-3.025811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151092</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.144829</td>\n",
       "      <td>1.375909</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.735556</td>\n",
       "      <td>1036.200000</td>\n",
       "      <td>0.243678</td>\n",
       "      <td>0.907834</td>\n",
       "      <td>0.274652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>-22.530234</td>\n",
       "      <td>31.830443</td>\n",
       "      <td>-5.155541</td>\n",
       "      <td>-6.073602</td>\n",
       "      <td>2.675920</td>\n",
       "      <td>1.076088</td>\n",
       "      <td>-2.166935</td>\n",
       "      <td>-1.123116</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>-0.718346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357938</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.342201</td>\n",
       "      <td>1.073918</td>\n",
       "      <td>0.371025</td>\n",
       "      <td>0.890833</td>\n",
       "      <td>960.448763</td>\n",
       "      <td>0.250883</td>\n",
       "      <td>0.687943</td>\n",
       "      <td>0.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>-25.681521</td>\n",
       "      <td>25.066416</td>\n",
       "      <td>-0.517062</td>\n",
       "      <td>-11.098346</td>\n",
       "      <td>0.320960</td>\n",
       "      <td>-0.722261</td>\n",
       "      <td>-0.316471</td>\n",
       "      <td>-1.407688</td>\n",
       "      <td>-1.067839</td>\n",
       "      <td>2.649285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388896</td>\n",
       "      <td>1.000470</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.928611</td>\n",
       "      <td>929.228070</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.638767</td>\n",
       "      <td>0.416551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>12.691205</td>\n",
       "      <td>1.361096</td>\n",
       "      <td>1.730498</td>\n",
       "      <td>-4.063565</td>\n",
       "      <td>2.698011</td>\n",
       "      <td>-0.263857</td>\n",
       "      <td>-2.108461</td>\n",
       "      <td>0.051035</td>\n",
       "      <td>-0.066095</td>\n",
       "      <td>0.649859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130152</td>\n",
       "      <td>0.049808</td>\n",
       "      <td>-0.131732</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.388194</td>\n",
       "      <td>1176.053640</td>\n",
       "      <td>0.249042</td>\n",
       "      <td>0.776923</td>\n",
       "      <td>0.375024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>26.208748</td>\n",
       "      <td>1.223819</td>\n",
       "      <td>-0.750656</td>\n",
       "      <td>-14.497578</td>\n",
       "      <td>21.059123</td>\n",
       "      <td>1.539668</td>\n",
       "      <td>3.829611</td>\n",
       "      <td>-2.383363</td>\n",
       "      <td>5.219389</td>\n",
       "      <td>4.505694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534002</td>\n",
       "      <td>0.313636</td>\n",
       "      <td>-0.490975</td>\n",
       "      <td>1.552178</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.184722</td>\n",
       "      <td>1452.404545</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.534247</td>\n",
       "      <td>0.227603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>25.898912</td>\n",
       "      <td>-0.875074</td>\n",
       "      <td>17.089111</td>\n",
       "      <td>-16.163478</td>\n",
       "      <td>-3.181579</td>\n",
       "      <td>-3.717163</td>\n",
       "      <td>-12.909274</td>\n",
       "      <td>0.747702</td>\n",
       "      <td>7.321352</td>\n",
       "      <td>-2.666413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376031</td>\n",
       "      <td>0.161049</td>\n",
       "      <td>-0.345681</td>\n",
       "      <td>1.414951</td>\n",
       "      <td>0.164794</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>1379.501873</td>\n",
       "      <td>0.250936</td>\n",
       "      <td>1.135338</td>\n",
       "      <td>0.263982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>5.944021</td>\n",
       "      <td>10.971361</td>\n",
       "      <td>-7.028612</td>\n",
       "      <td>-3.729516</td>\n",
       "      <td>16.252878</td>\n",
       "      <td>4.574362</td>\n",
       "      <td>-5.971668</td>\n",
       "      <td>6.157987</td>\n",
       "      <td>4.949524</td>\n",
       "      <td>5.259026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069949</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.066686</td>\n",
       "      <td>1.431018</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.595556</td>\n",
       "      <td>1206.288136</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>1.068085</td>\n",
       "      <td>0.281313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>26.131340</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>23.011535</td>\n",
       "      <td>-12.728496</td>\n",
       "      <td>8.075412</td>\n",
       "      <td>2.314124</td>\n",
       "      <td>10.166097</td>\n",
       "      <td>-1.258650</td>\n",
       "      <td>-5.445485</td>\n",
       "      <td>4.485016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555058</td>\n",
       "      <td>0.078261</td>\n",
       "      <td>0.507646</td>\n",
       "      <td>1.174809</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>1314.291304</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>0.359282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>16.477927</td>\n",
       "      <td>17.254603</td>\n",
       "      <td>15.624358</td>\n",
       "      <td>22.925548</td>\n",
       "      <td>-3.732207</td>\n",
       "      <td>-5.545847</td>\n",
       "      <td>-0.287058</td>\n",
       "      <td>2.493493</td>\n",
       "      <td>-2.856931</td>\n",
       "      <td>-1.472151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.873794</td>\n",
       "      <td>0.403008</td>\n",
       "      <td>-0.829380</td>\n",
       "      <td>1.724176</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.034167</td>\n",
       "      <td>1052.803008</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>1.692771</td>\n",
       "      <td>0.187160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>30.734405</td>\n",
       "      <td>0.195722</td>\n",
       "      <td>12.751227</td>\n",
       "      <td>-7.813799</td>\n",
       "      <td>6.543426</td>\n",
       "      <td>-2.407274</td>\n",
       "      <td>-2.644401</td>\n",
       "      <td>1.560591</td>\n",
       "      <td>-2.331377</td>\n",
       "      <td>13.273872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286722</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.257941</td>\n",
       "      <td>1.568366</td>\n",
       "      <td>0.281967</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>1335.285246</td>\n",
       "      <td>0.249180</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.216469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>11.901789</td>\n",
       "      <td>2.450261</td>\n",
       "      <td>-10.521368</td>\n",
       "      <td>-6.382197</td>\n",
       "      <td>-5.168251</td>\n",
       "      <td>-3.993819</td>\n",
       "      <td>-6.422926</td>\n",
       "      <td>3.484094</td>\n",
       "      <td>-3.876889</td>\n",
       "      <td>4.011051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410873</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>0.379774</td>\n",
       "      <td>1.355320</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>0.668611</td>\n",
       "      <td>1236.621359</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.287445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>28.762754</td>\n",
       "      <td>-1.056746</td>\n",
       "      <td>-3.490334</td>\n",
       "      <td>-7.397567</td>\n",
       "      <td>-6.997795</td>\n",
       "      <td>20.513098</td>\n",
       "      <td>6.709869</td>\n",
       "      <td>1.375369</td>\n",
       "      <td>-1.890947</td>\n",
       "      <td>-4.380331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569337</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>-0.519832</td>\n",
       "      <td>1.414330</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.056944</td>\n",
       "      <td>1375.405844</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>1.211726</td>\n",
       "      <td>0.250042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>0.742950</td>\n",
       "      <td>-2.627058</td>\n",
       "      <td>-3.835256</td>\n",
       "      <td>-6.145804</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>-3.033757</td>\n",
       "      <td>-0.100329</td>\n",
       "      <td>-0.043938</td>\n",
       "      <td>-1.506061</td>\n",
       "      <td>-1.085429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035166</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>1.577346</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.459306</td>\n",
       "      <td>1072.140000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.567839</td>\n",
       "      <td>0.235550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>22.258937</td>\n",
       "      <td>-0.865007</td>\n",
       "      <td>15.276752</td>\n",
       "      <td>-17.701465</td>\n",
       "      <td>14.155350</td>\n",
       "      <td>-4.693674</td>\n",
       "      <td>-2.443168</td>\n",
       "      <td>-2.805695</td>\n",
       "      <td>-2.804628</td>\n",
       "      <td>-2.417563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270898</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>-0.244673</td>\n",
       "      <td>1.444424</td>\n",
       "      <td>0.187793</td>\n",
       "      <td>0.311389</td>\n",
       "      <td>1383.929577</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>1.179245</td>\n",
       "      <td>0.269149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>25.401560</td>\n",
       "      <td>8.282808</td>\n",
       "      <td>-31.277136</td>\n",
       "      <td>13.145835</td>\n",
       "      <td>16.997314</td>\n",
       "      <td>-7.945109</td>\n",
       "      <td>-8.147648</td>\n",
       "      <td>-8.896687</td>\n",
       "      <td>-5.843419</td>\n",
       "      <td>-0.626392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232083</td>\n",
       "      <td>0.196774</td>\n",
       "      <td>0.206216</td>\n",
       "      <td>1.502625</td>\n",
       "      <td>0.374194</td>\n",
       "      <td>0.606944</td>\n",
       "      <td>1226.735484</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>1.291262</td>\n",
       "      <td>0.242310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>13.706690</td>\n",
       "      <td>7.791517</td>\n",
       "      <td>7.306411</td>\n",
       "      <td>10.541593</td>\n",
       "      <td>-9.516593</td>\n",
       "      <td>3.309520</td>\n",
       "      <td>-1.542335</td>\n",
       "      <td>4.351730</td>\n",
       "      <td>-3.554876</td>\n",
       "      <td>-5.031538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099858</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>-0.099977</td>\n",
       "      <td>0.877015</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.395417</td>\n",
       "      <td>1083.274038</td>\n",
       "      <td>0.247596</td>\n",
       "      <td>0.573494</td>\n",
       "      <td>0.482329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>26.912932</td>\n",
       "      <td>-0.454641</td>\n",
       "      <td>14.015744</td>\n",
       "      <td>-9.998340</td>\n",
       "      <td>-8.905399</td>\n",
       "      <td>8.952896</td>\n",
       "      <td>-11.056320</td>\n",
       "      <td>6.633441</td>\n",
       "      <td>-5.127221</td>\n",
       "      <td>-1.530622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227136</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>-0.206427</td>\n",
       "      <td>0.998815</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>1320.083893</td>\n",
       "      <td>0.251678</td>\n",
       "      <td>0.690236</td>\n",
       "      <td>0.473807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>-30.570426</td>\n",
       "      <td>24.732419</td>\n",
       "      <td>-2.079454</td>\n",
       "      <td>-11.137881</td>\n",
       "      <td>1.518234</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.446444</td>\n",
       "      <td>-1.345274</td>\n",
       "      <td>1.313954</td>\n",
       "      <td>0.345504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559664</td>\n",
       "      <td>0.747592</td>\n",
       "      <td>0.563981</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>856.459716</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.496193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-5.828762</td>\n",
       "      <td>2.545959</td>\n",
       "      <td>-0.645852</td>\n",
       "      <td>1.055244</td>\n",
       "      <td>-12.794977</td>\n",
       "      <td>-0.612679</td>\n",
       "      <td>-6.507652</td>\n",
       "      <td>0.730545</td>\n",
       "      <td>-2.138035</td>\n",
       "      <td>-1.641358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>-0.006055</td>\n",
       "      <td>1.310332</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.496944</td>\n",
       "      <td>986.786713</td>\n",
       "      <td>0.251748</td>\n",
       "      <td>1.017544</td>\n",
       "      <td>0.306739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>22.675271</td>\n",
       "      <td>-1.925411</td>\n",
       "      <td>13.604115</td>\n",
       "      <td>-20.048106</td>\n",
       "      <td>-3.945546</td>\n",
       "      <td>2.531374</td>\n",
       "      <td>5.503931</td>\n",
       "      <td>-0.571000</td>\n",
       "      <td>0.317331</td>\n",
       "      <td>7.455540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079787</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>-0.073768</td>\n",
       "      <td>1.407055</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>1436.803828</td>\n",
       "      <td>0.248804</td>\n",
       "      <td>1.139423</td>\n",
       "      <td>0.290264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>-28.685449</td>\n",
       "      <td>28.085449</td>\n",
       "      <td>2.383516</td>\n",
       "      <td>-6.788596</td>\n",
       "      <td>2.347656</td>\n",
       "      <td>0.579053</td>\n",
       "      <td>0.203732</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-0.268027</td>\n",
       "      <td>-1.756875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416477</td>\n",
       "      <td>0.031873</td>\n",
       "      <td>0.403970</td>\n",
       "      <td>1.070504</td>\n",
       "      <td>0.466135</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>901.521912</td>\n",
       "      <td>0.239044</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.400232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>15.635537</td>\n",
       "      <td>-4.674515</td>\n",
       "      <td>-21.750005</td>\n",
       "      <td>-6.050642</td>\n",
       "      <td>-6.309088</td>\n",
       "      <td>-6.900510</td>\n",
       "      <td>-12.422318</td>\n",
       "      <td>-2.050268</td>\n",
       "      <td>-4.072678</td>\n",
       "      <td>3.729193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619993</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>-0.569125</td>\n",
       "      <td>1.468102</td>\n",
       "      <td>0.294372</td>\n",
       "      <td>0.048333</td>\n",
       "      <td>1323.142857</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>1.339130</td>\n",
       "      <td>0.239145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>31.667955</td>\n",
       "      <td>3.968138</td>\n",
       "      <td>-15.258337</td>\n",
       "      <td>-11.228208</td>\n",
       "      <td>-4.849364</td>\n",
       "      <td>-6.858703</td>\n",
       "      <td>2.468430</td>\n",
       "      <td>-1.729865</td>\n",
       "      <td>-0.044167</td>\n",
       "      <td>0.559501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594845</td>\n",
       "      <td>0.214789</td>\n",
       "      <td>-0.530439</td>\n",
       "      <td>1.379800</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>1479.183099</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.190813</td>\n",
       "      <td>0.253323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>5.693312</td>\n",
       "      <td>3.724910</td>\n",
       "      <td>0.837205</td>\n",
       "      <td>-2.108318</td>\n",
       "      <td>3.092695</td>\n",
       "      <td>0.562513</td>\n",
       "      <td>0.972588</td>\n",
       "      <td>1.002036</td>\n",
       "      <td>4.446282</td>\n",
       "      <td>0.449313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359014</td>\n",
       "      <td>0.110204</td>\n",
       "      <td>0.322635</td>\n",
       "      <td>1.302367</td>\n",
       "      <td>0.220408</td>\n",
       "      <td>0.696111</td>\n",
       "      <td>1116.616327</td>\n",
       "      <td>0.236735</td>\n",
       "      <td>0.897541</td>\n",
       "      <td>0.301858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>-3.423319</td>\n",
       "      <td>17.990912</td>\n",
       "      <td>-9.466775</td>\n",
       "      <td>-8.092630</td>\n",
       "      <td>0.987898</td>\n",
       "      <td>3.026215</td>\n",
       "      <td>-4.959364</td>\n",
       "      <td>-1.497612</td>\n",
       "      <td>7.117767</td>\n",
       "      <td>1.759992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004212</td>\n",
       "      <td>0.109312</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>1.537289</td>\n",
       "      <td>0.336032</td>\n",
       "      <td>0.601389</td>\n",
       "      <td>1153.404858</td>\n",
       "      <td>0.251012</td>\n",
       "      <td>1.264228</td>\n",
       "      <td>0.248472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>-2.689915</td>\n",
       "      <td>5.327463</td>\n",
       "      <td>0.237086</td>\n",
       "      <td>8.228715</td>\n",
       "      <td>-0.121224</td>\n",
       "      <td>0.351049</td>\n",
       "      <td>-2.448782</td>\n",
       "      <td>-0.694674</td>\n",
       "      <td>-3.174310</td>\n",
       "      <td>-1.754148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115902</td>\n",
       "      <td>0.055718</td>\n",
       "      <td>-0.129394</td>\n",
       "      <td>1.087075</td>\n",
       "      <td>0.029326</td>\n",
       "      <td>0.446528</td>\n",
       "      <td>976.988270</td>\n",
       "      <td>0.249267</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.393951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>2.436708</td>\n",
       "      <td>2.565077</td>\n",
       "      <td>2.710996</td>\n",
       "      <td>2.542875</td>\n",
       "      <td>-0.756122</td>\n",
       "      <td>-1.675883</td>\n",
       "      <td>0.576098</td>\n",
       "      <td>-1.094740</td>\n",
       "      <td>1.337314</td>\n",
       "      <td>-3.562950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703093</td>\n",
       "      <td>0.164609</td>\n",
       "      <td>0.647279</td>\n",
       "      <td>1.152592</td>\n",
       "      <td>0.329218</td>\n",
       "      <td>0.849167</td>\n",
       "      <td>1036.823045</td>\n",
       "      <td>0.251029</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.354096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>-36.729628</td>\n",
       "      <td>36.734406</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-7.837553</td>\n",
       "      <td>-0.683447</td>\n",
       "      <td>1.128580</td>\n",
       "      <td>0.061302</td>\n",
       "      <td>-1.135128</td>\n",
       "      <td>0.110130</td>\n",
       "      <td>0.619779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473018</td>\n",
       "      <td>0.800311</td>\n",
       "      <td>0.489855</td>\n",
       "      <td>0.987778</td>\n",
       "      <td>851.797101</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.477421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>15.530337</td>\n",
       "      <td>-4.464295</td>\n",
       "      <td>12.255507</td>\n",
       "      <td>-9.370663</td>\n",
       "      <td>-5.453862</td>\n",
       "      <td>1.494034</td>\n",
       "      <td>7.607356</td>\n",
       "      <td>-7.700408</td>\n",
       "      <td>-5.541105</td>\n",
       "      <td>9.505316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345386</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>-0.316729</td>\n",
       "      <td>1.405600</td>\n",
       "      <td>0.162055</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>1246.418972</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>1.146825</td>\n",
       "      <td>0.261010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>11.747589</td>\n",
       "      <td>45.567337</td>\n",
       "      <td>6.371927</td>\n",
       "      <td>23.781958</td>\n",
       "      <td>9.076805</td>\n",
       "      <td>-1.852480</td>\n",
       "      <td>-0.988570</td>\n",
       "      <td>1.938058</td>\n",
       "      <td>-2.953191</td>\n",
       "      <td>-0.210857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449579</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>0.416527</td>\n",
       "      <td>1.057506</td>\n",
       "      <td>0.228041</td>\n",
       "      <td>0.816944</td>\n",
       "      <td>1059.508446</td>\n",
       "      <td>0.241554</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.429151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          svd_1      svd_2      svd_3      svd_4      svd_5      svd_6  \\\n",
       "user                                                                     \n",
       "26    19.405226   1.056769 -21.038527   1.067305  26.873600   3.613976   \n",
       "199   24.701986   4.278532  -6.918118 -14.194389   4.933048   8.060955   \n",
       "202  -30.651479  27.972663  -1.808480 -11.094628   0.104153   0.539192   \n",
       "205    4.077090  27.859159  17.445291  -5.786915 -12.944545  -2.128206   \n",
       "231   34.053427   3.629981 -14.458684  -5.522036 -10.271650  -4.786838   \n",
       "284   19.995440  -3.963606  36.543998 -12.160488  -0.507532  10.580456   \n",
       "424   19.515725   1.946307 -11.297455 -12.750778  -1.955778   5.562889   \n",
       "459   19.229018  -4.886454  20.073144 -14.276159   7.799430   1.149055   \n",
       "469   15.833040  10.900338  17.698568 -12.457373   2.723195  -2.610725   \n",
       "561   12.384787  -1.564100  -9.387893  -5.248689   2.829056   3.688155   \n",
       "667   35.282715   1.605001   8.985617   3.324267 -25.776958   8.908518   \n",
       "699   22.899398  28.790211  -2.853864   0.518014  -5.737385  13.667635   \n",
       "730   10.109697  -0.534179   9.083594  -9.989619   5.325208   3.840458   \n",
       "786   28.727091   4.460696 -17.737821  -8.568014   2.184781  -4.353866   \n",
       "849   16.708753  -1.445953   4.006640 -10.407775  -1.372284   4.169791   \n",
       "1045 -14.589255  24.841091   0.592879 -12.930398   3.003011  -0.479167   \n",
       "1057  50.942770  10.806731 -19.721361   8.340857 -32.353401   0.202926   \n",
       "1066   6.660772  18.774476   2.211635 -13.141572 -12.306636   3.991652   \n",
       "1151   4.565552   2.001047  -4.991591  -2.488794   3.774549  -0.880130   \n",
       "1279  55.143343  12.673649  21.268550  27.536053  -8.691158   1.590301   \n",
       "1289  -3.964172  22.893215  -5.001387 -11.151228   4.800311   5.292133   \n",
       "1307   9.982597   1.233299   4.366212  -4.498651  -0.542160  -3.126854   \n",
       "1450  33.229563   5.877482 -19.276149  -9.445955  -9.482312  -5.610207   \n",
       "1455  -9.410127   1.405786  -4.863681   4.097648  -0.428274   0.271327   \n",
       "1561  18.630213   0.309461   4.104553 -12.629557  -7.013169  -0.524197   \n",
       "1563  46.562663   8.590602  -5.380619  17.140850  -0.568038   1.429418   \n",
       "1617  16.365574  -5.234236  18.390408 -18.399680  -0.653204  -1.960561   \n",
       "1640  48.347671   6.326245  26.561090   3.600284 -18.627871   4.337232   \n",
       "1682  -8.846367  22.965476  -9.353197  -9.323590  -1.110193  -6.428081   \n",
       "1722  32.194061   4.328843 -12.861302  -8.997731  -0.403383   5.000128   \n",
       "1818 -11.240644  38.191345   2.053651   0.160269  -3.187149   0.353069   \n",
       "1822 -22.530234  31.830443  -5.155541  -6.073602   2.675920   1.076088   \n",
       "1865 -25.681521  25.066416  -0.517062 -11.098346   0.320960  -0.722261   \n",
       "1903  12.691205   1.361096   1.730498  -4.063565   2.698011  -0.263857   \n",
       "2093  26.208748   1.223819  -0.750656 -14.497578  21.059123   1.539668   \n",
       "2120  25.898912  -0.875074  17.089111 -16.163478  -3.181579  -3.717163   \n",
       "2163   5.944021  10.971361  -7.028612  -3.729516  16.252878   4.574362   \n",
       "2225  26.131340   0.000671  23.011535 -12.728496   8.075412   2.314124   \n",
       "2328  16.477927  17.254603  15.624358  22.925548  -3.732207  -5.545847   \n",
       "2390  30.734405   0.195722  12.751227  -7.813799   6.543426  -2.407274   \n",
       "2458  11.901789   2.450261 -10.521368  -6.382197  -5.168251  -3.993819   \n",
       "2543  28.762754  -1.056746  -3.490334  -7.397567  -6.997795  20.513098   \n",
       "2548   0.742950  -2.627058  -3.835256  -6.145804   0.009457  -3.033757   \n",
       "2595  22.258937  -0.865007  15.276752 -17.701465  14.155350  -4.693674   \n",
       "2648  25.401560   8.282808 -31.277136  13.145835  16.997314  -7.945109   \n",
       "2767  13.706690   7.791517   7.306411  10.541593  -9.516593   3.309520   \n",
       "2778  26.912932  -0.454641  14.015744  -9.998340  -8.905399   8.952896   \n",
       "2782 -30.570426  24.732419  -2.079454 -11.137881   1.518234   0.011092   \n",
       "2800  -5.828762   2.545959  -0.645852   1.055244 -12.794977  -0.612679   \n",
       "2807  22.675271  -1.925411  13.604115 -20.048106  -3.945546   2.531374   \n",
       "2938 -28.685449  28.085449   2.383516  -6.788596   2.347656   0.579053   \n",
       "2980  15.635537  -4.674515 -21.750005  -6.050642  -6.309088  -6.900510   \n",
       "3082  31.667955   3.968138 -15.258337 -11.228208  -4.849364  -6.858703   \n",
       "3123   5.693312   3.724910   0.837205  -2.108318   3.092695   0.562513   \n",
       "3153  -3.423319  17.990912  -9.466775  -8.092630   0.987898   3.026215   \n",
       "3292  -2.689915   5.327463   0.237086   8.228715  -0.121224   0.351049   \n",
       "3401   2.436708   2.565077   2.710996   2.542875  -0.756122  -1.675883   \n",
       "3465 -36.729628  36.734406  -0.051635  -7.837553  -0.683447   1.128580   \n",
       "3499  15.530337  -4.464295  12.255507  -9.370663  -5.453862   1.494034   \n",
       "3576  11.747589  45.567337   6.371927  23.781958   9.076805  -1.852480   \n",
       "\n",
       "          svd_7      svd_8      svd_9     svd_10  ...  user_bias  \\\n",
       "user                                              ...              \n",
       "26    -2.008504  -1.611251   4.041583  -4.170670  ...  -0.439797   \n",
       "199    6.287393   4.449363  -1.056309  -0.967260  ...   0.109502   \n",
       "202   -0.447467  -1.427426  -0.590070   0.858616  ...   0.500438   \n",
       "205   -3.253641  -6.447440   2.451911  -3.451880  ...   0.171325   \n",
       "231    2.645228   9.534818  -3.295730   0.831030  ...  -0.691520   \n",
       "284   -0.204711   1.751987   4.764085  -3.733991  ...   0.915377   \n",
       "424    2.733605  -0.980205  -1.747588  -3.314120  ...   0.097125   \n",
       "459    9.521034  -0.200415  -3.550065   5.731454  ...  -0.441914   \n",
       "469    8.489573  -7.575831  -0.392949  -2.492455  ...   0.082205   \n",
       "561   -6.718119  -1.332926  -4.809891   2.177380  ...   0.171960   \n",
       "667   -4.520455   3.688603   1.437349   4.809446  ...  -0.393263   \n",
       "699    7.112067   7.904376  -0.047028   1.585041  ...   0.339959   \n",
       "730   -0.516786  -0.355747  -4.024504   4.291348  ...  -0.371623   \n",
       "786   -7.278990   8.086681  -4.452230  -3.061602  ...  -0.579775   \n",
       "849   -7.593610  -1.823122  -0.533504  -0.710331  ...   0.309073   \n",
       "1045   0.592950   1.268763   0.800157  -2.415303  ...   0.246324   \n",
       "1057   9.124116   0.243394  -3.626521   2.615711  ...   0.580949   \n",
       "1066  11.318564   2.828238   3.027072  -6.186215  ...   0.399783   \n",
       "1151  -1.752852  -1.050395   1.504125  -2.953061  ...   0.329951   \n",
       "1279   1.867865   2.002636 -10.896158   5.159406  ...  -0.164866   \n",
       "1289   4.868103   5.233376  -2.822094   1.064827  ...   0.385464   \n",
       "1307   5.185793   1.389276  -1.535660  -0.518658  ...  -0.145973   \n",
       "1450   2.965458  -3.737145  -8.958739   1.904718  ...  -0.277095   \n",
       "1455   6.355352  -0.929348  -0.500602  -0.158288  ...  -0.709848   \n",
       "1561  -2.458145   4.225193   4.917298  -0.953974  ...   0.049099   \n",
       "1563   4.274856  11.332335  -3.256507  -1.132952  ...  -0.405009   \n",
       "1617  -5.301496  -8.029610  -0.166289  -5.879510  ...  -0.613316   \n",
       "1640 -23.631518   0.098326  -5.827032   3.219304  ...   0.106968   \n",
       "1682  -0.770292  -3.809209  -0.839959   1.847927  ...   0.267113   \n",
       "1722   3.642669  -3.721963  -6.917732   0.772333  ...   0.007276   \n",
       "1818  -3.564333  -4.061618   2.608191  -3.025811  ...   0.151092   \n",
       "1822  -2.166935  -1.123116   0.019499  -0.718346  ...   0.357938   \n",
       "1865  -0.316471  -1.407688  -1.067839   2.649285  ...   0.400242   \n",
       "1903  -2.108461   0.051035  -0.066095   0.649859  ...  -0.130152   \n",
       "2093   3.829611  -2.383363   5.219389   4.505694  ...  -0.534002   \n",
       "2120 -12.909274   0.747702   7.321352  -2.666413  ...  -0.376031   \n",
       "2163  -5.971668   6.157987   4.949524   5.259026  ...   0.069949   \n",
       "2225  10.166097  -1.258650  -5.445485   4.485016  ...   0.555058   \n",
       "2328  -0.287058   2.493493  -2.856931  -1.472151  ...  -0.873794   \n",
       "2390  -2.644401   1.560591  -2.331377  13.273872  ...  -0.286722   \n",
       "2458  -6.422926   3.484094  -3.876889   4.011051  ...   0.410873   \n",
       "2543   6.709869   1.375369  -1.890947  -4.380331  ...  -0.569337   \n",
       "2548  -0.100329  -0.043938  -1.506061  -1.085429  ...  -0.035166   \n",
       "2595  -2.443168  -2.805695  -2.804628  -2.417563  ...  -0.270898   \n",
       "2648  -8.147648  -8.896687  -5.843419  -0.626392  ...   0.232083   \n",
       "2767  -1.542335   4.351730  -3.554876  -5.031538  ...  -0.099858   \n",
       "2778 -11.056320   6.633441  -5.127221  -1.530622  ...  -0.227136   \n",
       "2782   0.446444  -1.345274   1.313954   0.345504  ...   0.570789   \n",
       "2800  -6.507652   0.730545  -2.138035  -1.641358  ...   0.006366   \n",
       "2807   5.503931  -0.571000   0.317331   7.455540  ...  -0.079787   \n",
       "2938   0.203732  -0.804791  -0.268027  -1.756875  ...   0.416477   \n",
       "2980 -12.422318  -2.050268  -4.072678   3.729193  ...  -0.619993   \n",
       "3082   2.468430  -1.729865  -0.044167   0.559501  ...  -0.594845   \n",
       "3123   0.972588   1.002036   4.446282   0.449313  ...   0.359014   \n",
       "3153  -4.959364  -1.497612   7.117767   1.759992  ...  -0.004212   \n",
       "3292  -2.448782  -0.694674  -3.174310  -1.754148  ...  -0.115902   \n",
       "3401   0.576098  -1.094740   1.337314  -3.562950  ...   0.703093   \n",
       "3465   0.061302  -1.135128   0.110130   0.619779  ...   0.486003   \n",
       "3499   7.607356  -7.700408  -5.541105   9.505316  ...  -0.345386   \n",
       "3576  -0.988570   1.938058  -2.953191  -0.210857  ...   0.449579   \n",
       "\n",
       "      outlier_frac  mean_item_alignment  rating_entropy  extreme_ratio  \\\n",
       "user                                                                     \n",
       "26        0.143443            -0.399362        1.503556       0.151639   \n",
       "199       0.123223             0.100368        1.441563       0.189573   \n",
       "202       0.004673             0.477618        0.745018       0.471963   \n",
       "205       0.059946             0.170565        1.399122       0.234332   \n",
       "231       0.270950            -0.633656        1.384519       0.265363   \n",
       "284       0.152466             0.846178        0.858244       0.219731   \n",
       "424       0.060000             0.079770        1.255755       0.075000   \n",
       "459       0.239130            -0.395315        1.475948       0.256522   \n",
       "469       0.048327             0.079066        1.315856       0.148699   \n",
       "561       0.180488             0.143844        1.446732       0.214634   \n",
       "667       0.180180            -0.362430        1.339007       0.173423   \n",
       "699       0.104558             0.315420        1.338535       0.246649   \n",
       "730       0.068376            -0.352857        1.234931       0.047009   \n",
       "786       0.174216            -0.529118        1.389305       0.174216   \n",
       "849       0.056604             0.282600        1.200819       0.099057   \n",
       "1045      0.025532             0.238899        1.287192       0.387234   \n",
       "1057      0.092040             0.540825        1.136690       0.119403   \n",
       "1066      0.012146             0.381093        1.201517       0.214575   \n",
       "1151      0.105023             0.292262        1.300817       0.210046   \n",
       "1279      0.070312            -0.150875        1.158217       0.070312   \n",
       "1289      0.021008             0.362125        1.069240       0.256303   \n",
       "1307      0.078067            -0.148607        1.239811       0.078067   \n",
       "1450      0.155709            -0.260829        1.496491       0.186851   \n",
       "1455      0.181538            -0.687801        1.301151       0.092308   \n",
       "1561      0.160000             0.040100        1.513973       0.182222   \n",
       "1563      0.034417            -0.379207        1.188743       0.053537   \n",
       "1617      0.223214            -0.554063        1.378308       0.200893   \n",
       "1640      0.198276             0.101233        1.553445       0.273707   \n",
       "1682      0.069388             0.252205        1.291371       0.424490   \n",
       "1722      0.123134            -0.000830        1.565127       0.223881   \n",
       "1818      0.027586             0.144829        1.375909       0.310345   \n",
       "1822      0.014134             0.342201        1.073918       0.371025   \n",
       "1865      0.000000             0.388896        1.000470       0.412281   \n",
       "1903      0.049808            -0.131732        1.154581       0.053640   \n",
       "2093      0.313636            -0.490975        1.552178       0.345455   \n",
       "2120      0.161049            -0.345681        1.414951       0.164794   \n",
       "2163      0.080508             0.066686        1.431018       0.211864   \n",
       "2225      0.078261             0.507646        1.174809       0.182609   \n",
       "2328      0.403008            -0.829380        1.724176       0.345865   \n",
       "2390      0.200000            -0.257941        1.568366       0.281967   \n",
       "2458      0.165049             0.379774        1.355320       0.242718   \n",
       "2543      0.214286            -0.519832        1.414330       0.211039   \n",
       "2548      0.215000            -0.049011        1.577346       0.230000   \n",
       "2595      0.154930            -0.244673        1.444424       0.187793   \n",
       "2648      0.196774             0.206216        1.502625       0.374194   \n",
       "2767      0.028846            -0.099977        0.877015       0.002404   \n",
       "2778      0.063758            -0.206427        0.998815       0.063758   \n",
       "2782      0.000000             0.559664        0.747592       0.563981   \n",
       "2800      0.122378            -0.006055        1.310332       0.115385   \n",
       "2807      0.110048            -0.073768        1.407055       0.114833   \n",
       "2938      0.031873             0.403970        1.070504       0.466135   \n",
       "2980      0.246753            -0.569125        1.468102       0.294372   \n",
       "3082      0.214789            -0.530439        1.379800       0.225352   \n",
       "3123      0.110204             0.322635        1.302367       0.220408   \n",
       "3153      0.109312             0.000731        1.537289       0.336032   \n",
       "3292      0.055718            -0.129394        1.087075       0.029326   \n",
       "3401      0.164609             0.647279        1.152592       0.329218   \n",
       "3465      0.000000             0.473018        0.800311       0.489855   \n",
       "3499      0.169960            -0.316729        1.405600       0.162055   \n",
       "3576      0.016892             0.416527        1.057506       0.228041   \n",
       "\n",
       "      user_mean_rank  avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                                            \n",
       "26          0.248333          1296.213115         0.250000           1.395062   \n",
       "199         0.515833          1429.568720         0.251185           1.219048   \n",
       "202         0.987222           873.476636         0.252336           0.516432   \n",
       "205         0.651111          1128.376022         0.250681           1.019126   \n",
       "231         0.028056          1383.776536         0.251397           1.263305   \n",
       "284         0.859167          1249.322870         0.251121           0.540541   \n",
       "424         0.479167          1388.605000         0.250000           0.864322   \n",
       "459         0.178333          1343.843478         0.252174           1.371179   \n",
       "469         0.533056          1279.308550         0.249071           0.891791   \n",
       "561         0.543611          1214.692683         0.248780           1.171569   \n",
       "667         0.137222          1260.198198         0.250000           1.031603   \n",
       "699         0.706111          1237.621984         0.249330           0.930108   \n",
       "730         0.249583          1190.303419         0.252137           0.948498   \n",
       "786         0.098611          1406.651568         0.250871           1.220280   \n",
       "849         0.604167          1270.547170         0.250000           0.895735   \n",
       "1045        0.809722          1072.093617         0.246809           1.004274   \n",
       "1057        0.678333          1306.529851         0.251244           0.807980   \n",
       "1066        0.741111          1248.655870         0.251012           0.841463   \n",
       "1151        0.677778          1109.118721         0.251142           0.922018   \n",
       "1279        0.300278          1184.531250         0.250000           0.784038   \n",
       "1289        0.836667          1130.827731         0.252101           0.755274   \n",
       "1307        0.373472          1155.007435         0.245353           0.888060   \n",
       "1450        0.235556          1422.553633         0.245675           1.253472   \n",
       "1455        0.153889           919.012308         0.249231           1.046296   \n",
       "1561        0.436944          1292.951111         0.248889           1.187500   \n",
       "1563        0.162500          1250.078394         0.248566           0.768199   \n",
       "1617        0.083889          1359.825893         0.250000           1.228700   \n",
       "1640        0.433056          1268.467672         0.247845           1.382289   \n",
       "1682        0.792500          1076.918367         0.244898           0.991803   \n",
       "1722        0.401944          1381.936567         0.250000           1.228464   \n",
       "1818        0.735556          1036.200000         0.243678           0.907834   \n",
       "1822        0.890833           960.448763         0.250883           0.687943   \n",
       "1865        0.928611           929.228070         0.245614           0.638767   \n",
       "1903        0.388194          1176.053640         0.249042           0.776923   \n",
       "2093        0.184722          1452.404545         0.250000           1.534247   \n",
       "2120        0.192500          1379.501873         0.250936           1.135338   \n",
       "2163        0.595556          1206.288136         0.245763           1.068085   \n",
       "2225        0.729444          1314.291304         0.252174           0.786026   \n",
       "2328        0.034167          1052.803008         0.248120           1.692771   \n",
       "2390        0.243056          1335.285246         0.249180           1.312500   \n",
       "2458        0.668611          1236.621359         0.252427           0.975610   \n",
       "2543        0.056944          1375.405844         0.246753           1.211726   \n",
       "2548        0.459306          1072.140000         0.250000           1.567839   \n",
       "2595        0.311389          1383.929577         0.248826           1.179245   \n",
       "2648        0.606944          1226.735484         0.251613           1.291262   \n",
       "2767        0.395417          1083.274038         0.247596           0.573494   \n",
       "2778        0.253333          1320.083893         0.251678           0.690236   \n",
       "2782        0.998056           856.459716         0.251185           0.571429   \n",
       "2800        0.496944           986.786713         0.251748           1.017544   \n",
       "2807        0.317500          1436.803828         0.248804           1.139423   \n",
       "2938        0.923889           901.521912         0.239044           0.792000   \n",
       "2980        0.048333          1323.142857         0.251082           1.339130   \n",
       "3082        0.071111          1479.183099         0.250000           1.190813   \n",
       "3123        0.696111          1116.616327         0.236735           0.897541   \n",
       "3153        0.601389          1153.404858         0.251012           1.264228   \n",
       "3292        0.446528           976.988270         0.249267           0.785294   \n",
       "3401        0.849167          1036.823045         0.251029           0.867769   \n",
       "3465        0.987778           851.797101         0.246377           0.546512   \n",
       "3499        0.205556          1246.418972         0.249012           1.146825   \n",
       "3576        0.816944          1059.508446         0.241554           0.639594   \n",
       "\n",
       "      rating_concentration  \n",
       "user                        \n",
       "26                0.254065  \n",
       "199               0.276117  \n",
       "202               0.491833  \n",
       "205               0.279347  \n",
       "231               0.250882  \n",
       "284               0.511392  \n",
       "424               0.327250  \n",
       "459               0.242722  \n",
       "469               0.311618  \n",
       "561               0.263534  \n",
       "667               0.285965  \n",
       "699               0.297242  \n",
       "730               0.341369  \n",
       "786               0.269920  \n",
       "849               0.359381  \n",
       "1045              0.321865  \n",
       "1057              0.428195  \n",
       "1066              0.334065  \n",
       "1151              0.316862  \n",
       "1279              0.419419  \n",
       "1289              0.413424  \n",
       "1307              0.346416  \n",
       "1450              0.240550  \n",
       "1455              0.300317  \n",
       "1561              0.258746  \n",
       "1563              0.352295  \n",
       "1617              0.253866  \n",
       "1640              0.219986  \n",
       "1682              0.327514  \n",
       "1722              0.229923  \n",
       "1818              0.274652  \n",
       "1822              0.393200  \n",
       "1865              0.416551  \n",
       "1903              0.375024  \n",
       "2093              0.227603  \n",
       "2120              0.263982  \n",
       "2163              0.281313  \n",
       "2225              0.359282  \n",
       "2328              0.187160  \n",
       "2390              0.216469  \n",
       "2458              0.287445  \n",
       "2543              0.250042  \n",
       "2548              0.235550  \n",
       "2595              0.269149  \n",
       "2648              0.242310  \n",
       "2767              0.482329  \n",
       "2778              0.473807  \n",
       "2782              0.496193  \n",
       "2800              0.306739  \n",
       "2807              0.290264  \n",
       "2938              0.400232  \n",
       "2980              0.239145  \n",
       "3082              0.253323  \n",
       "3123              0.301858  \n",
       "3153              0.248472  \n",
       "3292              0.393951  \n",
       "3401              0.354096  \n",
       "3465              0.477421  \n",
       "3499              0.261010  \n",
       "3576              0.429151  \n",
       "\n",
       "[60 rows x 327 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user\n",
       "26      0\n",
       "199     1\n",
       "202     2\n",
       "205     2\n",
       "231     0\n",
       "284     2\n",
       "424     1\n",
       "459     0\n",
       "469     2\n",
       "561     1\n",
       "667     0\n",
       "699     2\n",
       "730     1\n",
       "786     0\n",
       "849     1\n",
       "1045    2\n",
       "1057    0\n",
       "1066    2\n",
       "1151    1\n",
       "1279    0\n",
       "1289    2\n",
       "1307    1\n",
       "1450    0\n",
       "1455    1\n",
       "1561    1\n",
       "1563    2\n",
       "1617    0\n",
       "1640    0\n",
       "1682    2\n",
       "1722    1\n",
       "1818    2\n",
       "1822    2\n",
       "1865    2\n",
       "1903    1\n",
       "2093    0\n",
       "2120    0\n",
       "2163    2\n",
       "2225    1\n",
       "2328    1\n",
       "2390    0\n",
       "2458    1\n",
       "2543    0\n",
       "2548    1\n",
       "2595    0\n",
       "2648    2\n",
       "2767    1\n",
       "2778    0\n",
       "2782    2\n",
       "2800    1\n",
       "2807    0\n",
       "2938    2\n",
       "2980    0\n",
       "3082    0\n",
       "3123    1\n",
       "3153    2\n",
       "3292    1\n",
       "3401    1\n",
       "3465    2\n",
       "3499    0\n",
       "3576    2\n",
       "Name: anomtype, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate the dataframes into input X and label y\n",
    "\n",
    "X_log = df_cla.drop(columns=[\"label\", \"anomtype\"])\n",
    "y_log = df_cla[\"anomtype\"]\n",
    "\n",
    "display(X_log)\n",
    "display(y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92dd4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features in X\n",
    "\n",
    "scaler_cla = StandardScaler().fit(X_log)\n",
    "X_log_std = scaler_cla.transform(X_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1e167",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d0153b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: {'estimator__C': 0.1, 'estimator__l1_ratio': 0.5}\n",
      "Best CV Accuracy: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>param_estimator__l1_ratio</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.113039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.084984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.081650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.124722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.113039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.074536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.040825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.040825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.084984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_estimator__C  param_estimator__l1_ratio  mean_test_score  \\\n",
       "0                 0.1                       0.25         0.700000   \n",
       "1                 0.1                       0.50         0.716667   \n",
       "2                 0.1                       0.75         0.683333   \n",
       "3                 1.0                       0.25         0.700000   \n",
       "4                 1.0                       0.50         0.716667   \n",
       "5                 1.0                       0.75         0.666667   \n",
       "6                10.0                       0.25         0.716667   \n",
       "7                10.0                       0.50         0.716667   \n",
       "8                10.0                       0.75         0.700000   \n",
       "\n",
       "   std_test_score  \n",
       "0        0.113039  \n",
       "1        0.084984  \n",
       "2        0.081650  \n",
       "3        0.124722  \n",
       "4        0.113039  \n",
       "5        0.074536  \n",
       "6        0.040825  \n",
       "7        0.040825  \n",
       "8        0.084984  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty=\"elasticnet\",\n",
    "    l1_ratio=0.5,\n",
    "    C=0.01,\n",
    "    solver=\"saga\",\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "# Wrap log_reg in OneVsRest classifier so the solver works for our multiclass classifier\n",
    "log_classifier = OneVsRestClassifier(log_reg)\n",
    "\n",
    "# Using StratifiedKFold, for n_splits=5, we train on 4 folds and validate on the remaining fold\n",
    "# then compute accuracy based on that fold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Code for GridSearchCV: to find the best value of C\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10], \n",
    "    'estimator__l1_ratio': [0.25, 0.5, 0.75]}\n",
    "grid = GridSearchCV(\n",
    "    estimator=log_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid.fit(X_log_std, y_log)\n",
    "\n",
    "print(\"Best C:\", grid.best_params_)\n",
    "print(\"Best CV Accuracy: %.4f\" % grid.best_score_)\n",
    "\n",
    "# Check the results of GridSearchCV on the parameters\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "display(cv_results[[\n",
    "    'param_estimator__C', \n",
    "    'param_estimator__l1_ratio', \n",
    "    'mean_test_score', \n",
    "    'std_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c570e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get final logistic regression model, we choose the best value of C\n",
    "# We don't use logistic regression as our model, so we comment this code out\n",
    "\n",
    "# log_reg = LogisticRegression(\n",
    "#     penalty=\"elasticnet\",\n",
    "#     C=grid.best_params_['estimator__C'],\n",
    "#     l1_ratio=grid.best_params_['estimator__l1_ratio'],\n",
    "#     solver=\"saga\",\n",
    "#     max_iter=10000\n",
    "# )\n",
    "\n",
    "# log_classifier = OneVsRestClassifier(log_reg)\n",
    "\n",
    "# final_model = log_classifier.fit(X_log_std, y_log)\n",
    "# final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44b608",
   "metadata": {},
   "source": [
    "We are able to get an overall maximum accuracy of 70% using this Logistic Regression model trained on ElasticNet.  \n",
    "We move on to see if unsupervised and semi-supervised methods can help us attain a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f74b54",
   "metadata": {},
   "source": [
    "### Unsupervised methods (KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed4f6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>interaction_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>202</td>\n",
       "      <td>0.470231</td>\n",
       "      <td>0.242574</td>\n",
       "      <td>0.534653</td>\n",
       "      <td>0.222772</td>\n",
       "      <td>0.202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750701</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>-0.676871</td>\n",
       "      <td>1.407269</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>1372.925743</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>1.273632</td>\n",
       "      <td>0.249240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>138</td>\n",
       "      <td>154</td>\n",
       "      <td>43</td>\n",
       "      <td>335</td>\n",
       "      <td>0.310541</td>\n",
       "      <td>0.128358</td>\n",
       "      <td>0.411940</td>\n",
       "      <td>0.459701</td>\n",
       "      <td>0.335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.434637</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>-0.400218</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>0.083582</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>1358.641791</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.880240</td>\n",
       "      <td>0.354564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>207</td>\n",
       "      <td>262</td>\n",
       "      <td>0.147537</td>\n",
       "      <td>0.790076</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>0.262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665060</td>\n",
       "      <td>0.041985</td>\n",
       "      <td>0.606235</td>\n",
       "      <td>0.866574</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.777639</td>\n",
       "      <td>1373.125954</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.536398</td>\n",
       "      <td>0.507750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "      <td>213</td>\n",
       "      <td>302</td>\n",
       "      <td>0.284512</td>\n",
       "      <td>0.705298</td>\n",
       "      <td>0.119205</td>\n",
       "      <td>0.175497</td>\n",
       "      <td>0.302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550003</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.511695</td>\n",
       "      <td>1.374852</td>\n",
       "      <td>0.301325</td>\n",
       "      <td>0.732639</td>\n",
       "      <td>1223.599338</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>1.016611</td>\n",
       "      <td>0.300776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>329</td>\n",
       "      <td>342</td>\n",
       "      <td>0.143587</td>\n",
       "      <td>0.961988</td>\n",
       "      <td>0.020468</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437630</td>\n",
       "      <td>0.854248</td>\n",
       "      <td>0.467836</td>\n",
       "      <td>0.970278</td>\n",
       "      <td>867.903509</td>\n",
       "      <td>0.251462</td>\n",
       "      <td>0.671554</td>\n",
       "      <td>0.463784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                         \n",
       "0              5            108             45          49   \n",
       "1              5            138            154          43   \n",
       "2              5              1             54         207   \n",
       "3              5             36             53         213   \n",
       "4              5              7              6         329   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0                    202        0.470231    0.242574       0.534653   \n",
       "1                    335        0.310541    0.128358       0.411940   \n",
       "2                    262        0.147537    0.790076       0.003817   \n",
       "3                    302        0.284512    0.705298       0.119205   \n",
       "4                    342        0.143587    0.961988       0.020468   \n",
       "\n",
       "      neutral_ratio  interaction_ratio  ...  user_bias  outlier_frac  \\\n",
       "user                                    ...                            \n",
       "0          0.222772              0.202  ...  -0.750701      0.272277   \n",
       "1          0.459701              0.335  ...  -0.434637      0.038806   \n",
       "2          0.206107              0.262  ...   0.665060      0.041985   \n",
       "3          0.175497              0.302  ...   0.550003      0.211921   \n",
       "4          0.017544              0.342  ...   0.443621      0.000000   \n",
       "\n",
       "      mean_item_alignment  rating_entropy  extreme_ratio  user_mean_rank  \\\n",
       "user                                                                       \n",
       "0               -0.676871        1.407269       0.282178        0.023889   \n",
       "1               -0.400218        1.212858       0.083582        0.132500   \n",
       "2                0.606235        0.866574       0.118321        0.777639   \n",
       "3                0.511695        1.374852       0.301325        0.732639   \n",
       "4                0.437630        0.854248       0.467836        0.970278   \n",
       "\n",
       "      avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                            \n",
       "0             1372.925743         0.252475           1.273632   \n",
       "1             1358.641791         0.250746           0.880240   \n",
       "2             1373.125954         0.251908           0.536398   \n",
       "3             1223.599338         0.251656           1.016611   \n",
       "4              867.903509         0.251462           0.671554   \n",
       "\n",
       "      rating_concentration  \n",
       "user                        \n",
       "0                 0.249240  \n",
       "1                 0.354564  \n",
       "2                 0.507750  \n",
       "3                 0.300776  \n",
       "4                 0.463784  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We try and cluster users just based off the created features\n",
    "# Create a dataframe without noise level nor anomtype\n",
    "\n",
    "df_cla_2 = engineer_features(X)\n",
    "df_cla_2 = df_cla_2.iloc[:, -23:]\n",
    "\n",
    "# # We convert all column names to strings so it does not throw an error later\n",
    "df_cla_2.columns = df_cla_2.columns.astype(str)\n",
    "df_cla_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b3f583c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.217773  ,  0.67637463, -0.75372421, ...,  1.07189726,\n",
       "         0.97681459, -0.90660634],\n",
       "       [ 0.217773  ,  1.17403758,  1.2064143 , ...,  0.55319392,\n",
       "        -0.49082773,  0.45819998],\n",
       "       [ 0.217773  , -1.09862321, -0.59187791, ...,  0.90183891,\n",
       "        -1.77360743,  2.44320143],\n",
       "       ...,\n",
       "       [ 0.217773  , -0.66731532, -0.8256559 , ..., -1.82728064,\n",
       "         0.19511647,  0.16412522],\n",
       "       [ 0.217773  ,  0.62660834, -0.23221947, ...,  0.61021326,\n",
       "         1.37252393, -1.55390269],\n",
       "       [ 0.217773  , -0.73367038,  0.34323404, ..., -0.31911795,\n",
       "        -0.49689691,  0.79808305]], shape=(3600, 23))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the features for KMeans (separate scaler for separate dataframe)\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "df_cla_2_scaled = scaler_features.fit_transform(df_cla_2)\n",
    "df_cla_2_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0c4a755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>interaction_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.676375</td>\n",
       "      <td>-0.753724</td>\n",
       "      <td>-1.307564</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>1.455813</td>\n",
       "      <td>-1.223091</td>\n",
       "      <td>2.072483</td>\n",
       "      <td>-0.360042</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.729360</td>\n",
       "      <td>1.555813</td>\n",
       "      <td>-1.677011</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>-1.649779</td>\n",
       "      <td>1.003710</td>\n",
       "      <td>1.071897</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>-0.906606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>1.174038</td>\n",
       "      <td>1.206414</td>\n",
       "      <td>-1.375877</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>-1.721433</td>\n",
       "      <td>1.295657</td>\n",
       "      <td>1.452570</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019037</td>\n",
       "      <td>-0.879531</td>\n",
       "      <td>-1.009237</td>\n",
       "      <td>-0.370275</td>\n",
       "      <td>-1.139736</td>\n",
       "      <td>-1.273539</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>0.553194</td>\n",
       "      <td>-0.490828</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.098623</td>\n",
       "      <td>-0.591878</td>\n",
       "      <td>0.491361</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>-1.419891</td>\n",
       "      <td>1.165742</td>\n",
       "      <td>-1.287937</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452419</td>\n",
       "      <td>-0.846373</td>\n",
       "      <td>1.420099</td>\n",
       "      <td>-2.008956</td>\n",
       "      <td>-0.871282</td>\n",
       "      <td>0.961288</td>\n",
       "      <td>1.004850</td>\n",
       "      <td>0.901839</td>\n",
       "      <td>-1.773607</td>\n",
       "      <td>2.443201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.518016</td>\n",
       "      <td>-0.609861</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>-0.199227</td>\n",
       "      <td>0.795841</td>\n",
       "      <td>-0.557479</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193840</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>1.191901</td>\n",
       "      <td>0.396310</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>0.805404</td>\n",
       "      <td>0.153503</td>\n",
       "      <td>0.826007</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>-0.238795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.999091</td>\n",
       "      <td>-1.455058</td>\n",
       "      <td>1.880405</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>-1.455088</td>\n",
       "      <td>1.915819</td>\n",
       "      <td>-1.182528</td>\n",
       "      <td>-1.930129</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954757</td>\n",
       "      <td>-1.284317</td>\n",
       "      <td>1.013126</td>\n",
       "      <td>-2.067286</td>\n",
       "      <td>1.829722</td>\n",
       "      <td>1.628609</td>\n",
       "      <td>-1.871693</td>\n",
       "      <td>0.767914</td>\n",
       "      <td>-1.269377</td>\n",
       "      <td>1.873478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.435073</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>-0.430872</td>\n",
       "      <td>-0.607792</td>\n",
       "      <td>-0.193510</td>\n",
       "      <td>-0.117082</td>\n",
       "      <td>-0.290259</td>\n",
       "      <td>0.556078</td>\n",
       "      <td>-0.607792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081018</td>\n",
       "      <td>-0.339776</td>\n",
       "      <td>-0.094876</td>\n",
       "      <td>0.042066</td>\n",
       "      <td>-0.964181</td>\n",
       "      <td>-0.066876</td>\n",
       "      <td>-0.905806</td>\n",
       "      <td>0.919871</td>\n",
       "      <td>-0.117763</td>\n",
       "      <td>0.123364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.766848</td>\n",
       "      <td>-1.077417</td>\n",
       "      <td>0.855701</td>\n",
       "      <td>-0.294320</td>\n",
       "      <td>-0.451492</td>\n",
       "      <td>1.351945</td>\n",
       "      <td>-0.848897</td>\n",
       "      <td>-1.344620</td>\n",
       "      <td>-0.294320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674505</td>\n",
       "      <td>-0.811832</td>\n",
       "      <td>0.705402</td>\n",
       "      <td>-0.350036</td>\n",
       "      <td>1.283952</td>\n",
       "      <td>1.204257</td>\n",
       "      <td>-1.058761</td>\n",
       "      <td>0.590638</td>\n",
       "      <td>-0.239701</td>\n",
       "      <td>0.545020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.667315</td>\n",
       "      <td>-0.825656</td>\n",
       "      <td>0.924014</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>-0.227664</td>\n",
       "      <td>1.133765</td>\n",
       "      <td>-0.766024</td>\n",
       "      <td>-1.062213</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.187697</td>\n",
       "      <td>0.115373</td>\n",
       "      <td>1.216609</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.954901</td>\n",
       "      <td>1.038749</td>\n",
       "      <td>0.212611</td>\n",
       "      <td>-1.827281</td>\n",
       "      <td>0.195116</td>\n",
       "      <td>0.164125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.626608</td>\n",
       "      <td>-0.232219</td>\n",
       "      <td>-0.863525</td>\n",
       "      <td>-0.484303</td>\n",
       "      <td>1.729776</td>\n",
       "      <td>-0.843437</td>\n",
       "      <td>1.177393</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>-0.484303</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.137427</td>\n",
       "      <td>0.825328</td>\n",
       "      <td>-1.138540</td>\n",
       "      <td>1.882849</td>\n",
       "      <td>0.095666</td>\n",
       "      <td>-1.151333</td>\n",
       "      <td>1.660008</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>1.372524</td>\n",
       "      <td>-1.553903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.733670</td>\n",
       "      <td>0.343234</td>\n",
       "      <td>0.616603</td>\n",
       "      <td>0.275628</td>\n",
       "      <td>-0.473418</td>\n",
       "      <td>0.459635</td>\n",
       "      <td>-0.892503</td>\n",
       "      <td>0.272674</td>\n",
       "      <td>0.275628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496048</td>\n",
       "      <td>-0.202136</td>\n",
       "      <td>0.440186</td>\n",
       "      <td>-0.501901</td>\n",
       "      <td>-0.872557</td>\n",
       "      <td>0.480644</td>\n",
       "      <td>-1.296826</td>\n",
       "      <td>-0.319118</td>\n",
       "      <td>-0.496897</td>\n",
       "      <td>0.798083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                         \n",
       "0       0.217773       0.676375      -0.753724   -1.307564   \n",
       "1       0.217773       1.174038       1.206414   -1.375877   \n",
       "2       0.217773      -1.098623      -0.591878    0.491361   \n",
       "3       0.217773      -0.518016      -0.609861    0.559675   \n",
       "4       0.217773      -0.999091      -1.455058    1.880405   \n",
       "...          ...            ...            ...         ...   \n",
       "3595    0.217773      -0.435073       0.001559   -0.430872   \n",
       "3596    0.217773      -0.766848      -1.077417    0.855701   \n",
       "3597    0.217773      -0.667315      -0.825656    0.924014   \n",
       "3598    0.217773       0.626608      -0.232219   -0.863525   \n",
       "3599    0.217773      -0.733670       0.343234    0.616603   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0              -1.101748        1.455813   -1.223091       2.072483   \n",
       "1               0.161638        0.032727   -1.721433       1.295657   \n",
       "2              -0.531799       -1.419891    1.165742      -1.287937   \n",
       "3              -0.151833       -0.199227    0.795841      -0.557479   \n",
       "4               0.228133       -1.455088    1.915819      -1.182528   \n",
       "...                  ...             ...         ...            ...   \n",
       "3595           -0.607792       -0.193510   -0.117082      -0.290259   \n",
       "3596           -0.294320       -0.451492    1.351945      -0.848897   \n",
       "3597           -0.047343       -0.227664    1.133765      -0.766024   \n",
       "3598           -0.484303        1.729776   -0.843437       1.177393   \n",
       "3599            0.275628       -0.473418    0.459635      -0.892503   \n",
       "\n",
       "      neutral_ratio  interaction_ratio  ...  user_bias  outlier_frac  \\\n",
       "user                                    ...                            \n",
       "0         -0.360042          -1.101748  ...  -1.729360      1.555813   \n",
       "1          1.452570           0.161638  ...  -1.019037     -0.879531   \n",
       "2         -0.487540          -0.531799  ...   1.452419     -0.846373   \n",
       "3         -0.721721          -0.151833  ...   1.193840      0.926231   \n",
       "4         -1.930129           0.228133  ...   0.954757     -1.284317   \n",
       "...             ...                ...  ...        ...           ...   \n",
       "3595       0.556078          -0.607792  ...  -0.081018     -0.339776   \n",
       "3596      -1.344620          -0.294320  ...   0.674505     -0.811832   \n",
       "3597      -1.062213          -0.047343  ...   1.187697      0.115373   \n",
       "3598       0.055999          -0.484303  ...  -1.137427      0.825328   \n",
       "3599       0.272674           0.275628  ...   0.496048     -0.202136   \n",
       "\n",
       "      mean_item_alignment  rating_entropy  extreme_ratio  user_mean_rank  \\\n",
       "user                                                                       \n",
       "0               -1.677011        0.549713       0.394985       -1.649779   \n",
       "1               -1.009237       -0.370275      -1.139736       -1.273539   \n",
       "2                1.420099       -2.008956      -0.871282        0.961288   \n",
       "3                1.191901        0.396310       0.542944        0.805404   \n",
       "4                1.013126       -2.067286       1.829722        1.628609   \n",
       "...                   ...             ...            ...             ...   \n",
       "3595            -0.094876        0.042066      -0.964181       -0.066876   \n",
       "3596             0.705402       -0.350036       1.283952        1.204257   \n",
       "3597             1.216609        0.012196       0.954901        1.038749   \n",
       "3598            -1.138540        1.882849       0.095666       -1.151333   \n",
       "3599             0.440186       -0.501901      -0.872557        0.480644   \n",
       "\n",
       "      avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                            \n",
       "0                1.003710         1.071897           0.976815   \n",
       "1                0.922383         0.553194          -0.490828   \n",
       "2                1.004850         0.901839          -1.773607   \n",
       "3                0.153503         0.826007           0.017939   \n",
       "4               -1.871693         0.767914          -1.269377   \n",
       "...                   ...              ...                ...   \n",
       "3595            -0.905806         0.919871          -0.117763   \n",
       "3596            -1.058761         0.590638          -0.239701   \n",
       "3597             0.212611        -1.827281           0.195116   \n",
       "3598             1.660008         0.610213           1.372524   \n",
       "3599            -1.296826        -0.319118          -0.496897   \n",
       "\n",
       "      rating_concentration  \n",
       "user                        \n",
       "0                -0.906606  \n",
       "1                 0.458200  \n",
       "2                 2.443201  \n",
       "3                -0.238795  \n",
       "4                 1.873478  \n",
       "...                    ...  \n",
       "3595              0.123364  \n",
       "3596              0.545020  \n",
       "3597              0.164125  \n",
       "3598             -1.553903  \n",
       "3599              0.798083  \n",
       "\n",
       "[3600 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring the features back into a dataframe\n",
    "\n",
    "X_kmeans = pd.DataFrame(\n",
    "    df_cla_2_scaled,\n",
    "    columns=df_cla_2.columns,\n",
    "    index=df_cla_2.index\n",
    ")\n",
    "X_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b9a1346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "0       1\n",
       "1       1\n",
       "2       2\n",
       "3       2\n",
       "4       2\n",
       "       ..\n",
       "3595    0\n",
       "3596    2\n",
       "3597    2\n",
       "3598    1\n",
       "3599    2\n",
       "Name: cluster, Length: 3600, dtype: int32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply KMeans to sort the users into diffent clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=67)\n",
    "X_kmeans[\"cluster\"] = kmeans.fit_predict(X_kmeans)\n",
    "X_kmeans[\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37c9497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>cluster</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>284</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>459</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>561</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>730</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>849</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1045</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1057</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1066</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1151</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1279</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1289</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1450</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1455</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1561</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1563</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1640</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1682</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1818</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1822</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1865</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1903</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2093</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2163</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2328</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2390</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2458</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2543</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2548</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2595</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2648</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2767</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2782</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2807</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2938</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3082</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3123</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3153</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>3292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3401</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3465</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3499</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3576</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user  cluster  anomtype\n",
       "0     26        1         0\n",
       "1    199        0         1\n",
       "2    202        2         2\n",
       "3    205        0         2\n",
       "4    231        1         0\n",
       "5    284        2         2\n",
       "6    424        0         1\n",
       "7    459        1         0\n",
       "8    469        0         2\n",
       "9    561        0         1\n",
       "10   667        1         0\n",
       "11   699        0         2\n",
       "12   730        0         1\n",
       "13   786        1         0\n",
       "14   849        0         1\n",
       "15  1045        2         2\n",
       "16  1057        2         0\n",
       "17  1066        0         2\n",
       "18  1151        0         1\n",
       "19  1279        0         0\n",
       "20  1289        2         2\n",
       "21  1307        0         1\n",
       "22  1450        1         0\n",
       "23  1455        1         1\n",
       "24  1561        0         1\n",
       "25  1563        1         2\n",
       "26  1617        1         0\n",
       "27  1640        1         0\n",
       "28  1682        2         2\n",
       "29  1722        1         1\n",
       "30  1818        0         2\n",
       "31  1822        2         2\n",
       "32  1865        2         2\n",
       "33  1903        0         1\n",
       "34  2093        1         0\n",
       "35  2120        1         0\n",
       "36  2163        0         2\n",
       "37  2225        0         1\n",
       "38  2328        1         1\n",
       "39  2390        1         0\n",
       "40  2458        0         1\n",
       "41  2543        1         0\n",
       "42  2548        1         1\n",
       "43  2595        1         0\n",
       "44  2648        0         2\n",
       "45  2767        0         1\n",
       "46  2778        0         0\n",
       "47  2782        2         2\n",
       "48  2800        0         1\n",
       "49  2807        0         0\n",
       "50  2938        2         2\n",
       "51  2980        1         0\n",
       "52  3082        1         0\n",
       "53  3123        0         1\n",
       "54  3153        0         2\n",
       "55  3292        0         1\n",
       "56  3401        2         1\n",
       "57  3465        2         2\n",
       "58  3499        1         0\n",
       "59  3576        2         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge with y_cat to check the difference between the predicted clusters and actual anomtype\n",
    "\n",
    "merged_df = pd.merge(X_kmeans, y_cat, on=\"user\", how=\"inner\")\n",
    "merged_df = merged_df[[\"user\", \"cluster\", \"anomtype\"]]\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76c5606a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAGJCAYAAACNYZoYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVOZJREFUeJzt3Ql8DHf/B/DvJnIQEkQOVMQd6r7vqyqldZQeWuqotk8rKFHX8xSlNEUdRVCqjqKO1lFaV1WpElfd6j4rIoQ4grj2//r8/HftbjaRTTbZzeTz7msaOzM7M7s7M9/fPTq9Xq8XIiIi0iwXRx8AERERZSwGeyIiIo1jsCciItI4BnsiIiKNY7AnIiLSOAZ7IiIijWOwJyIi0jgGeyIiIo1jsCciItK4DA/2n332meh0OskMjRs3VpPBH3/8ofb9448/Zsr+u3btKsHBweLMbt++Le+9954EBgaq76ZPnz522e6cOXPU9s6ePWuX7VHaGM55/E3tuqm5PrLCuZ0auD+UL1/e0Yfh9DLzvu2slixZIvnz51f3THtYu3at5M6dW65cuSJOH+wNN3TD5OnpKYUKFZLQ0FCZNGmS3Lp1yy4HFR0drU62ffv2ibNx5mNLjS+++EL9jh999JF8//338s4776S4/qNHj2T27NnqJokT38PDQ930u3XrJrt378604/7111/V956ZEOBwcZrC92A4/11cXMTb21vKlCmjvscNGzaIM1q4cKFMnDhRnIkhoZHctGjRInFmuC5w78OxrlmzxtGH47QsY0Zyk7MlJB89eiTDhg2TXr16md0DvvnmGylWrJi6F+Kav3nzptn7Hj9+LFWqVFH3WUsvvfSSlCxZUiIiIsQRcqTlTSNGjFAf+MGDBxITE6MuXOQQx48fLz///LNUrFjRuO6nn34qgwYNsjmgDh8+XJ0AlStXTvX71q9fLxktpWObOXOm+rGd2e+//y61a9dWJ/Kz3L17V9q1a6dSpA0bNpT//ve/6iRH7h2p3rlz58r58+flueeey5RgHxkZmekB3xp8XsMFm5CQICdPnpRly5bJ/Pnz5Y033lB/3dzcHHJs+J3wu7m7u5sF+0OHDqWrFCejzu3evXtLjRo1ksyvU6eOOPt1dOnSJXUfWLBggbRo0cLRh+SUcD4iU2EKJYs1a9aUDz74wDjPMlHtaKtWrZJjx46ZHePWrVtVJgnnbPHixdU9oH///ioBYHqd3LhxQ/r162d1u//5z3/kk08+UTEkT5484vTBHid29erVja8HDx6sTv5XXnlFWrduLf/884/kzJnzyQ5y5FBTRrpz547kypXL7AbnCI66wdsiNjZWypUrl6p1cSIj0E+YMCFJoEBiAfOzMjwD6t69e8ZzNbV8fHykU6dOZvO+/PJLdROYOnWqCgCjR48WR0BpA0rcssq53aBBA3nttdckq0GCrmrVqtKlSxeVCEaiz8vLy9GH5XQQFDGZ+vDDD9U8y2vImcyePVvq1asnhQsXNs5bvXq1KtkzlJKhVA+xzxDs4+PjVeYWr1ECak379u1VacHSpUvl3XfflSxZZ9+0aVMZMmSInDt3Tl0IKdX9oLizfv36kjdvXpWiQzEoLhhAKYEhpY+iYkMxD4qDTOvc9uzZo1KNCPKG91rW2ZsWyWAd1FPjgkSC5MKFC2br4AaNYltLptt81rFZq9fETQCpvCJFiqgTAJ/1q6++UoHGFLbTs2dPWbFihfp8WPf5559XwTa1Qbx79+4SEBCgbvaVKlVSOW/LYtMzZ87IL7/8Yjz25OrY//33X3XSvvjii1ZzhK6uriqFmlKuHtu3lhO3/K5RQoSUbqlSpdSx+/r6qvPDUCyOdZGrN2zTMBkgx4kLEN8X3o/vACno69evJ9kvEqTr1q1TiVUEedNUeXrg+0BVFhJSU6ZMUan71JzvyUGJCoKJqVatWqnPjdIzgx07dpgVJVvW2ePcxe+N6zK5IlN8f6NGjVK/Jb6/F154QZVWmLI8t3HeYFs4l2fMmCElSpRQ5yyuj127dom9b7y4v/j7+6t94DueNm2a1XXxPTRq1EjlmnAzxvGgZMPSkSNHpEmTJur+gRv6mDFjUn08KDlZvny5dOjQQZXk4PXKlSuTrQa6ePGitG3bVv3bz89PXTe4J6XnPoFgge8B5zBKQQ4ePKiW43xGUTF+R/z2ltf3n3/+Ka+//roEBQWp/WB/ffv2VZ8hJfhOcU+xBseKqty0QH047skff/yx1XsQritDKZqhSmDLli3q+sZ9Ar9x586dk1zrhnMBiUlsP0+ePPLyyy/L4cOHn3lMyADgvtusWTOz+fiO8uXLZ3yNUk5kNA1wr6tQoYK6dpODcxgl39bOl4xm1yw36jBwE0Nx+vvvv291HXzZuOHiA6M6ACccbix//fWXWl62bFk1f+jQoaoIBT8W1K1b17iNuLg4VbqAiw2pQ9zcU4IbGU6SgQMHqqCIwIAfEvXutuTqUnNspnChImGxadMmFYhR7I9AgxwzbgCWOWMUE6E4uEePHurkRPBAShBF5Tixk4OTEBc2vkfcCFDFgpsBbjZIbeJCwrGjOA0XNm7qhmIm3HyswYXy8OHDZ9bp2wMuElzQhuI91IOhPcDff/+tEhu4sFF9gqBpWSQIWI4bARJgyF0jQYOAu3fvXnVemeZKUTT31ltvqffgHMWNyl5wY8K2kejFb2m4uaR0vicH5xZuCPgucEPDuYT3IOeOGzbOK8C/MQ+5EGv+97//qYQHbpyG882yyBSlEtgGghDWReDr2LGjSkg8CwIp2urg+8Q1hvfiZnf69OlUlQbgvVevXk0yH+e7IUGHwI6EHD4zSglRxIprBImUsLAw43twDiC3hHWR40LiCucAbtxvv/22cT0EBtSf4jgRrNFAEfcG3KhTUxyPxBaCFO4/yEDg2kNRvuk+DBDUEQhr1aqlgvdvv/0m48aNU4kjFAmn5T6B3xzHYPjsuHZwjg0YMECVLOG7wWfEb4HvA6WuBrgvIEBh3/iOd+7cKZMnT1bnB5YlB/cBXC+oDjJt4IiE3fHjx1WONi1wLr766quyePFiVQ2Ma8jghx9+UN8NzkVTuMfht8V9A9czzg8kZg0JXcB9AqUu+O5Rynbnzh21HhLdOCdSaiOAjOT9+/eTJLaRcPz2229VfMM9Fr8j7leGxOP06dPV9/ks1apVU5m6TKe3wezZs5HM1O/atSvZdXx8fPRVqlQxvh42bJh6j8GECRPU6ytXriS7DWwf62B/lho1aqSWTZ8+3eoyTAabNm1S6xYuXFh/8+ZN4/wlS5ao+V9//bVxXtGiRfVdunR55jZTOja8H9sxWLFihVp35MiRZuu99tprep1Opz958qRxHtZzd3c3m7d//341f/LkyfqUTJw4Ua03f/5847z79+/r69Spo8+dO7fZZ8fxvfzyy/pn6du3r9rm3r179bacG2fOnDH7TPj9LVl+15UqVXrmMYWFhZmdRwZ//vmnmr9gwQKz+WvXrk0yH/vFPCxLDRyjl5eX2TycC88//3yy71m+fLnZuZWa890aw3n266+/qtcHDhxQr19//XV9rVq1jOu1bt3a7HoznPP4a4Dv1vS8tFy3bNmy+sTERON8HDvmHzx40Oy7MN0Gfmes4+vrq7927Zpx/sqVK9X8VatWpfj5DPtObrp06ZJx3Tt37iR5f2hoqL548eLG1/Hx8fo8efKo7+bu3btm6z5+/DjJ/WPevHnGefjsgYGB+vbt2+tT45VXXtHXq1fP+HrGjBn6HDly6GNjY83Ww3eGfY0YMcJsPn6vatWqpfk+4eHhYXadffPNN2o+PoPptT548OAk16S17zIiIkLt59y5c8net/H9enp66gcOHGj23t69e6tr5Pbt2/rUwvqm1/+6devUvtasWWO2XsWKFc3uvYZ7DL473N8MxowZo+bj3INbt27p8+bNq3///ffNthcTE6Pik+V8S99++22S8x8ePnyob9eunfEcLVKkiLouoXnz5voPP/wwVZ//iy++UO+/fPmyPjPZvesdUmoptcpHigyQa0lrgx/kjpCLSy0U85g2hkAdYcGCBVWjr4yE7SOlitymKeSqcd1atuJFaQNS/AbIDSJXh1zSs/aDHAZylQbIVWG/yIFs3rzZ5mM3tDLNjEYkOCeQAz5x4oTN70VuBHXoKAFADtEwIfWMcxG5JVNIkae1yDE1DLlmwzWQ1vMdLXqxLRRZGnJzKJHBuYwSD+RUcA6hBMFQwpRWuJZM27sYtves8w7efPNNs6JNW94LKCVDiY3lhCJSA9PSN5Q84PdFsTL2YaguwXvwnaMxsGWbBctqRHyvpvXF+OzIoaXmmFGqiFy36bWG0jfsA41WrUEdtSl8R6b7svU+gWoW05wpSg0Mx2F6vRrmm+7L9LtE1QG+S5RMYj/I8SYH11ibNm2MuW1DqQVy5KiiSE97Bdz30LMBpSMGKEE4cOCA1Xp9lKqalhqhlAIlPob7Oc4FlGjiNzK9J7i6uqrvxPKeYO03BtPzGvD+n376Sd2nUPKIEg2UBqGUBTn6zz//XJXEoLoNnwd/USJpybBdayVaGcnuwR7BJaUAgZsDihxRZIvidxSF4SKx5UaIOjZbGuOhLtgULkzUa2V0n3AULeFHt/w+UKRuWG4K9WjWTgxr9VGW+8FnRFFsavaTGkhkgL26U6YExdu4OEuXLq0uHhRf4kJPDVx4uOGjLgxVEqYTzkVU21gG+4xk6JNr+M3Ter7jxoK6WAR5wF8ECRRD4iYbFRWlig6vXbuW7mBved4ZbkbPOu/S+17A742bveVken2j+gLzEFCQeMJva2jzYAj2p06dUn9T04ceiSbLBEBqrjNAcEMbEyTGUB2DCb8BgohpsDJAwsOyqsxyX+m9TyAQA+rfrc033ReqBFG9h8SUoQ0BEk5g2s7EGiQ08X7DOYkqicuXL6e7qg/3LRTVo2jbUAeO7xLfHdoXPOt+js+BzJvhfm7INKCdh+U9Yf369UnuCcmxbC9hgNiBzASOD8X9SJShwXKBAgXU9Y0EFaqasNxa1Y5hu5k9joFd6+xR74MTBl9GcvBFILeC1BUaDqE+DRcQfhj8EKZ1Niltw96S++JxY03NMdlDcvtJ7qTLSCEhIeovGv7Y0v0xNSwbJ6GhJW7WyP3iHEC9GOopUQeGIJkSBE0Eems3WrC80WbEuWMKORIwXAPpOd8R2NHeBA2GcINF/TuCHQIaXhvaqqQ32KfnvMvocxbnBXKyOB9Rp4uAhoQAcnE4R9JSOpieYzacZ8m1kUAu2rT1eUbcO5Lb5rM+F647lIAhcYI2CvhOkYBCbhQJgGd9lygRwzmHBti4ZvEXJYqWDdnSAgmJsWPHqoCPHDnagqAdgiHBYgvD50C9PY7PUo5n9A4ztI9CIulZ3YpxDmJ7aEeARt8oaUObIZS8oM0EzgXERdPtGBJfSBxk2WBvaDz1rGJSpORwAWPCBYwBCHAjww0RJ469UzyWxcM4+ZEiNx0PAKlt5C4tIVVtevHacmxFixZVqV/kjk1T7UePHjUutwdsBzlhnOSmufv07AcNlXDzwAWd1pS7te8UKWH0T7aEnAaKkzEhd4ybCRrgGIJ9ct87qj3wHePmm9GB/FlwM8VNCi28EahTe74nB0Ec3xeKTnFDNgR1fDeGYI/SkGc1UM3KI6Ehh5SYmKiKSk1ztJZFsYbqLyS2UspspAdu4tu2bVM3dkNu2ADXHq4T/P62NlbLrPsEEu4oekYvHQRXg9QOBoX7AXKqaAiJRm8IzGi0Z48EDRKwKC1BYgqBESUIaDiY3P0cPSkMcL/APaVly5Zm5wIyAWlJiIT8f0YHvzdKnpKDfY4cOVJVJSLgG4rsUUpj+hfXrmmwx3YR6JNrHO30xfho8Yk6CxSTWraeNIVUpSVDzhEXNRjqf6wF37SYN2+eWXE0Wt/ihzJteYsTBEWjuLma9qu07KJny7Hh5EMAQMtwy9QgbsD2GogD+8HgRsgxGqAlPS4WFHFZ3phSAzkoXMjIfVq76HBzQ2tUpFqTg+/UUOdsgG5aljl7Qx2ZAY4ZN2zD+ZDS947W1Ngezj1L+A7sdQ49C44Bda4YYwJ/DdUgqTnfk4OiYdRN4saKxBBamQOCPs5VtMVITa4e392zimidlSGQmOa68VnQHc9U8+bNVaBEy3SUhGREKYMhV49W72j3YzrhPMR1llwJkzPcJ6x9l/j3119/neptIEGDnCl6XyDI2rOvPLaN+w16SyF3ndznxj0EVSkGaGWPa92wPjKbuP6QqDZdz+BZw9WiiB6lR88aIRTtQ5DwRs8OMCS6DYk03AvAsnQBrf0dMWhUmnL2aDCCD4QvGHU2CPRIHSIFihR4SoN6oH4WAQDdkrA+6k/QXQQpH0NuCEECxZUoxsUFjJsVbnxprW/FjRLbRq4Rx4uTCcHEtHsgcpBIBOCHw4WL4kPkak0bzNl6bGiggRQocnGoT0I/VZzMKK5G33XLbacVGqygfy2K4nAioQgJnwV1nfisaW1kh2CO7wHBC10CUayG3DpS3UjN4hxAHVVy8J2icRIaDqH4cP/+/apxk2XxFfoLo/sSLjL8VrjIcPzIQRlgGeBYcDHjxoV94waLGw9u8uhKiZs+AiRS/zhG3MjsPWgLgo1hLAnUMRpG0MN3hWMyTXik5nxPDkoI8LkR2A197AE3GDSuwpSaYI9tICEYHh6uug8hMYXtOQOUUFgGZ0CpGyb8nrjx4ngNAQajlCHXZlpChJs7giPOOXxG5EBxruKcw29kOuZEWiGQI6FmWTdugO5zGDAFDSgtu22lJLPuE8ixYlvoYoncJr4zNDhLbfsKQO4buXBcW2hTYMvnfBb8ZkhIYQwDNLpLrusmMmQoJcN9Gl3vcD3hWjJ0R8XnQgIAiQccH65JPz8/dd9CVRpKAS0TVqYQv3DeobQF1681aJCHa8q0bRHuuxi/A/dhdKFEdSRig2nJDK5/vMe0y2imsaXpvqHrg2FCVzF093jxxRdVdx3Tbh/JdeHYuHGjvk2bNvpChQqp9+PvW2+9pT9+/LjZ+9CNoly5cqpLi2lXt5S6PiXX9e6HH35Q3VD8/f31OXPmVF2RTLuZGIwbN05100PXFnSt2b17d5JtpnRslt2TDN1A0I0Nn9PNzU1fqlQp/dixY826AwG2g+5llpLrEmgJ3Ti6deumL1CggPpeK1SoYLV7YGq73pl2N0FXlAYNGqhuK/gM2Ab2Zdotz1rXu0ePHqmuOjimXLlyqe5S6EZk+ZnQ5ahmzZqquwx+n5CQEP2oUaPMutfgOHr16qX38/NT3YQsT110f0KXHLwfXbDw+QcMGKCPjo5O82dPruud6TWAro34TTt16qRfv359km2k9nxPTv/+/dV+Ro8ebTa/ZMmSav6pU6fM5lvreoduUW+//bb6frHMcI4a1l26dKnZNgzd6kzPn+S63uFctpRcl0trx5ncZPr+n3/+WXXDQtev4OBg9V189913Sc43w7p169ZV54G3t7c6r3D9GyR3/7B27Zras2eP2t+QIUOSXefs2bNqHVzvyZ0/1u6J6b1PJPdbWPt9jxw5om/WrJk6b3FdohuaoYuv6e9t7Rgtu7qhC1laWHa9M9WyZUu17W3btiVZZrjHbN68Wf/BBx/o8+XLpz5Hx44d9XFxcUnWx+fHPQf3LU9PT32JEiX0Xbt2Vff1Z1m2bJm6z5w/fz7JMvwm6OIZHh6eZBnubw0bNlTHhb+W1+e0adPUvdBarMxoOvwv85MYRESUFaG0DINzoRTCWg+i9MAAO2hbYDmCIxgGzsJAPqbDtWeER48eqRJHlB5YqyJMK5SMoBTTEUON83n2RESUKsgbzpo1S1Wf2TvQo1oGxeyZMWrns6CaEEX4GKbbno+4RfUiRnd0hIx9Qg0REWV5aB+C9ljoBYGctz3HdkfrdLQvQh036unRNsMZvPnmm2qyF7QHs1fCIS0Y7ImIKEVowY4GdGicjAGNDI3h7AG9SlA8j5ICNKS01jee0o919kRERA6C9gEYUwQ9fNCFGv3z0aIf4zUYeuAgTGOUPvREQXdi9ChAjwPL0QRTwjp7IiIiB8E4Ggjc6A6Ivvl4jdH3TMc3wWs8BRVdvvE0SnT5Rhdka91Wk8OcPRERkYNg/BIMyIOGjwYYmwQjgiK3jxCN3D7G4McYCYaxPvAe9FBIaawTU8zZExER2RFGx8STQ02n5EbMxFMHN27cqIYyBgwEhTH2DSMCogEjivdNh/7FMwMwYM/27dtTfUzZqoHeqSt3HX0IlInKN+/v6EOgTHR9V/KjopH2eGZw9MpZ5ekInrYa2KaADB8+3Gwe6txRN29t2F0kBjDCIbr8oQ4fD8AyDDuPQA+Wz8DAa8Oy1MhWwZ6IiChVdGkv+EZfegxPbcrDw8PqunjkNYZixkOU8PwLDPuNYZJRdN+lSxexFwZ7IiIiS+l4WiQCe3LB3VL//v1V7t5Q944n7eFpq3jeB4K9oSsinutSsGBB4/vw2pbHj7POnoiIyFrOPq2TDfCgJtNHkwOK8/FkUcBD1hDwUa9vgGJ/tMq35el5zNkTERE5CJ56iDp6DCqEYvy9e/fK+PHj5d1331XL0dcexfojR45U/eoR/IcMGaKK+du2bZvq/TDYExER2bEY3xboT4/g3aNHD/UIXARxDBk8dOhQ4zp49C+GLMbjzDGoDh7pi7H2U3qcfLbuZ8/W+NkLW+NnL2yNn71keGv8mk/6tKfF3Z1fibNhzp6IiMhBOfvMwmBPRERkx653zojBnoiISOM5e20lXYiIiCgJ5uyJiIgssRifiIhI43TaKsZnsCciIrLEnD0REZHG6ZizJyIi0jadtnL22vo0RERElARz9kRERBrP2TPYExERWXJhnT0REZG26ZizJyIi0jYdc/ZERETaptNWzl5bn4aIiIiSYM6eiIjIEovxiYiINE6nrYJvBnsiIiJLzNkTERFpnI45eyIiIm3TaStnr62kCxERESXBnD0REZElFuMTERFpnE5bxfgM9kRERJaYsyciItI4HYM9ERGRtum0VYyvraQLERERJcGcPRERkSUW4xMREWmcjsX4RERE2s/Z69I42SA4OFh0Ol2SKSwsTC2/d++e+revr6/kzp1b2rdvL5cvX7b54zDYExERWcvZp3Wywa5du+TSpUvGacOGDWr+66+/rv727dtXVq1aJUuXLpXNmzdLdHS0tGvXTmzFYnwiIiILyF1nBj8/P7PXX375pZQoUUIaNWokN27ckFmzZsnChQuladOmavns2bOlbNmyEhUVJbVr1071fpizJyIisqPExES5efOm2YR5z3L//n2ZP3++vPvuuyqxsWfPHnnw4IE0a9bMuE5ISIgEBQXJ9u3bbTomBnsiIiIL1urRUztFRESIj4+P2YR5z7JixQqJj4+Xrl27qtcxMTHi7u4uefPmNVsvICBALbMFi/GJiIgspaMUf/DgwRIeHm42z8PD45nvQ5F9ixYtpFChQmJvDPZERER2rLNHYE9NcDd17tw5+e2332TZsmXGeYGBgapoH7l909w9WuNjmS1YjE9ERGTHYvy0QMM7f39/efnll43zqlWrJm5ubrJx40bjvGPHjsn58+elTp06Nm2fOXsiIiIHtcaHx48fq2DfpUsXyZHjaVhGXX/37t1VlUD+/PnF29tbevXqpQK9LS3xgcE+izm4b4/8tHCunDz2j1yLuyKffjFe6jZ80iUDxo8aIr+tWWX2nmo168rn46emuN1VPy2Sn36YK9evxUmxEqXlo74DpUy5Chn2OejZXFx08umHLeWtljUkwNdbLl25Id+v2iFfzlxrtt6Qj16Wbq/Wlbx5csr2/ael9xeL5dT5Kylu+z9vNJS+XV5Q2z14/KKEj14quw+fy+BPRLaYFjlZpk+dYjYvuFgxWbna/Pc3tX7dGomc/LVEX7woQUWDpU/4J9KgYaNMOFpKDxTfI7eOVviWJkyYIC4uLmowHbToDw0NlalTU76fW8Ngn8Xcu3tXipUsLc1fbisj/2feAMSgWq160ve/w42v3dzcU9zm5o3rZOaUcdLzk/9JSLkKsmLJAhkS3kNm/LBS8ubLb/fPQKnTr+uL8v5rDeT9od/LkVOXpNrzQfLNZ53k5u27MvWHzf+/TjPp8VYjtc7Zi3EytMcrsioyTKq0HymJ9x9a3e5rzavK6H6vSq9Ri2XXobPS8+0m8vPUMKnUdoRcuX47kz8lpaREyVIy49vZxteuOVyTXXff3r9lUP9+0rtPuDRs1ER+/WWV9OkVJot+XCalSpXOpCPWDl0m5uybN28uer3e6jJPT0+JjIxUU3qwzj6LqVGnvnT5oKfUbfQ0N2/Jzd1N8vsWME55vL1T3ObyRd/LS63aqQREULES0rP/p+Lh6SnrV6/IgE9AqVW7UnFZvfmArN16WM5fuibLf9snG6OOSvXnixrXCXu7iYyeuU5W/3FQDp2IlveGzJOCfj7SukmlZLfbu1NTmb1sm3z/c5QcPR0jvUYtkrv37kuXtrbVAVLGy+HqKgX8/IxTvhQS3wvmz5O69RtI13ffk+IlSkjP3n2kbLlysmjh/Ew9Zs3QpWNyQk4Z7K9evSpjxoyRV199VdVNYMK/x44dK1eupFw8SSIH9+6Wt15pIu+/1UamfDVKbt6IT3ZdDNhw8vg/Url6LeM8FBnh9dHDBzLpiMmaqP2npUnNMlIyyF+9rlC6sNSpXFzW/3VEvQ4u7KsC++87jhrfc/P2PZVbr1Ux2Oo23XK4SpWyReT3HceM85CjwOuaFYtl+Gci25w7f06aNa4vLUNfkMED+sml6Ohk1z2wb5/Urm2eYKtbr76aT87fQC+jOV0xPsYJRp1Erly51KhBpUuXNnY1mDRpkhpKcN26dVK9evUUt4O6DcsRixITH9vcHSKrQRF+3UYvSEDBwnLp4gWZO2OKDP0kTMZNnyeurkmLAG/euC6PHz2SfPl9zebnze8rF86dzcQjJ0tfzd4g3rk9Zf/yT+XRI724uupkWORqWbRmt1oeWOBJiU3stVtm74uNu6Xq4q0pkC+35MjhauU9N6VMcECGfRayXYWKFeXzURESHFxMZXK+mRYp3Tp3lJ9WrhIvr9xWM0m+vgXM5uHhKVfjrmbiUWuHzkmDtmaCPVoa4gEA06dPT/JlIwfy4YcfqnWeNVQgRisaPny4+bY/+a98POBT0bJGzV4y/rtYiVKqsV33N19RuX3T3Ds5P9Std2hRQ7r+d66qs69YprCM/eQ11VBvwaodjj48ymD1GzxtWFe6TIhUqFhJWrzYRNatXSPt2j95SAplHB2Dfcbav3+/zJkzx+oXjXl4AlCVKlXSNILRvzcfS3ZTsPBz4p03n0T/e8FqsPf2yScurq6qFb6p+Gtxqr6fHOeLPm1V7n7puj3q9eGT0RJUML/07/aiCvYxV2+q+f758xj/rV775pEDx/61us2r12/Lw4eP1HtM+ft6S0zc022Q80G3q6JFg+XC+fNWlxcoUEDiLHLxcXFxUoDXMTljnT1GBdq5c2eyy7EM4wI/C4rrcXGYTlovwrfmauxluXUjXvIXsH7BY8CGkqXLyv49O836fO7bs1NCnq+YiUdKlnJ6ustjvXkC9dFjvWpTAWh9j1x+k1pljMvzeHlKjfLBsuOA9SqYBw8fyd5/Lpi9B4noJjVLy84DZzLss1D63UlIkAsXLqiGetZUrFxZdkRFmc2L2r5NzSfbsc4+g33yySfywQcfqKf9vPDCC8bAjjp7jCI0c+ZM+eqrryS7unvnjkRffJqyv3zpopw6cVTy5PGRPN4+snD2dKnXqJnk8/WVSxf/le+mTpSChYuovvYGgz/+QPXNb9W+g3r9aod3VP/8UiHlpHTZ8rJyyQJJvHtXXny5jUM+Iz3x65aDMrB7qFy4dF0V41cOeU56d2oi81Y8vaFHLtwkA997SU6ev6KC/7AeL6sEwM+b9j/dzvRe6vX0xVvU60nzf5eZI96RPUfOy+7/73qXK6eHzFtpHijIscaNHS2NGjeRgoUKyZXYWNXv3tXVRVq0fEUt/9/gAeLvHyAf9+2nXnfs1Fm6d31H5s75Tho2bCRr1/wqhw8dkiGfjXDwJ8midKIpThfsw8LCVHEUBhLAwAGPHj1S89G4DEMHooj/jTfekOzqxNHDMqj3+8bXMyePU3+btWglYZ/8T86cOqEG1Um4fUvyF/CTqjXqyDvvh4mb+9O+9mi4dyP+uvF1oxdC5Wb8dfn+22ly/dpVKV6yjIwYNzVJoz3KXBjoZliPV+Tr/74pfvlyqyA+68e/5IsZa4zrjJvzmwrUUz59Sw2qs23fKWkdNtWsj33xIgXEN+/TBl0/rv9bNdQb+tHLEqCK/C9Km7DIJI32yLEuX46RQf3D1bjo+fLnlypVq8n3C5eokdQg5tIlcdE9LZytXKWqRIz5SqZMmiiTJ45Xg+pMnBzJPvZppHPSHHpa6fTJ9eR3AugWhhamgAQAipzT49SVu3Y6MsoKyjfv7+hDoEx0fZf5aHOkbZ4ZnFX167Y4ze+9MvtNcTZOl7M3heBesGBBRx8GERFlMzqN5eydroEeERERZaOcPRERkUPoRFMY7ImIiDRejM9gT0REZIHBnoiISON0DPZERETaptNYsGdrfCIiIo1jzp6IiMiStjL2DPZERERaL8ZnsCciIrLAYE9ERKRxOo0FezbQIyIi0jjm7ImIiCxpK2PPYE9ERKT1YnwGeyIiIgsM9kRERBqnY7AnIiLSNp3Ggj1b4xMREWkcc/ZERESWtJWxZ7AnIiKyxGJ8IiKibBDsdWmcbHXx4kXp1KmT+Pr6Ss6cOaVChQqye/du43K9Xi9Dhw6VggULquXNmjWTEydO2LQPBnsiIiILiNlpnWxx/fp1qVevnri5ucmaNWvkyJEjMm7cOMmXL59xnTFjxsikSZNk+vTpsmPHDvHy8pLQ0FC5d+9eqvfDYnwiIiIHFeOPHj1aihQpIrNnzzbOK1asmFmufuLEifLpp59KmzZt1Lx58+ZJQECArFixQjp06JCq/TBnT0REZEeJiYly8+ZNswnzrPn555+levXq8vrrr4u/v79UqVJFZs6caVx+5swZiYmJUUX3Bj4+PlKrVi3Zvn17qo+JwZ6IiMiOxfgREREqIJtOmGfN6dOnZdq0aVKqVClZt26dfPTRR9K7d2+ZO3euWo5AD8jJm8Jrw7LUYDE+ERGRHYvxBw8eLOHh4WbzPDw8rK77+PFjlbP/4osv1Gvk7A8dOqTq57t06SL2wpw9ERGRHXP2COze3t5mU3LBHi3sy5UrZzavbNmycv78efXvwMBA9ffy5ctm6+C1YVlqMNgTERFZcHHRpXmyBVriHzt2zGze8ePHpWjRosbGegjqGzduNC5HGwC0yq9Tp06q98NifCIiIguZNaZO3759pW7duqoY/4033pCdO3fKjBkz1PTkOHTSp08fGTlypKrXR/AfMmSIFCpUSNq2bZvq/TDYExEROUiNGjVk+fLlqp5/xIgRKpijq13Hjh2N6wwYMEASEhLkgw8+kPj4eKlfv76sXbtWPD09U70fnR6d+LKJU1fuOvoQKBOVb97f0YdAmej6rimOPgTKRJ4ZnFUt/+mGNL/30MgXxdkwZ09ERGRBY0PjM9gTERFp/UE4DPZEREQWGOyJiIg0TqetWM9+9kRERFrHnD0REZEFFuMTERFpnE5bsZ7BnoiIyBJz9kRERBqn01asZ7AnIiLSes6erfGJiIg0jjl7IiIiCxrL2DPYExERab0YP1sF+/IdJjj6ECgTRa2McPQhEFEWpdNWrM9ewZ6IiCg1mLMnIiLSOJ22Yj1b4xMREWkdc/ZEREQaL8a3a87+1KlT8umnn8pbb70lsbGxat6aNWvk8OHD9twNERFRhtLp0j5pOthv3rxZKlSoIDt27JBly5bJ7du31fz9+/fLsGHD7LUbIiKiTMnZ69I4aTrYDxo0SEaOHCkbNmwQd3d34/ymTZtKVFSUvXZDRESU4XQaC/Z2q7M/ePCgLFy4MMl8f39/uXr1qr12Q0RElOF0zhmzHZ+zz5s3r1y6dCnJ/L1790rhwoXttRsiIiJyVLDv0KGDDBw4UGJiYlQxxuPHj+Wvv/6STz75RDp37myv3RAREWU4ncaK8e0W7L/44gsJCQmRIkWKqMZ55cqVk4YNG0rdunVVC30iIqKsQqex1vh2q7NHo7yZM2fKkCFD5NChQyrgV6lSRUqVKmWvXRAREWUKnbNGbWcZVCcoKEjl7rX4ZRERUfag01j4suugOrNmzZLy5cuLp6enmvDvb7/91p67ICIiynAuOl2aJ03n7IcOHSrjx4+XXr16SZ06ddS87du3S9++feX8+fMyYsQIe+2KiIiIHBHsp02bpursMVSuQevWraVixYoqAcBgT0REWYXOOTPoji/Gf/DggVSvXj3J/GrVqsnDhw/ttRsiIiLNdL377LPPkrwfPdsM7t27J2FhYeLr6yu5c+eW9u3by+XLlx0X7N955x2Vu7c0Y8YM6dixo712Q0RElOFcdGmfbPX888+rQekM09atW43LUBW+atUqWbp0qXoGTXR0tLRr186xrfHRQG/9+vVSu3Zt9RoPxUF9PQbVCQ8PN66Hun0iIiJnpcvEcvwcOXJIYGBgkvk3btxQcRVD0eM5MzB79mwpW7aseuaMIdamah/2Olj0ra9atarxUbdQoEABNWGZAbvjERGRs9OlI1QlJiaqyZSHh4earDlx4oQUKlRI9WJDA/eIiAjVjX3Pnj2qirxZs2bGdVHEj2VoAO+QYL9p0yZ7bYqIiCjLioiIkOHDh5vNw6PeUT9vqVatWjJnzhwpU6aMKsLH+xo0aKAyyRh+HgPW4dkzpgICAtQyW9gt2KNoAePj58yZ016bJCIicgidpD1rP3jwYLOqa0guV9+iRQvjv9F7DcG/aNGismTJErvGU7s+zx6pje7du8u2bdvstVkiIqIs1UDPw8NDvL29zabkgr0l5OJLly4tJ0+eVPX49+/fl/j4eLN10BrfWh1/ip9H7OTixYsyd+5c9ez6xo0bq3qF0aNH21zUQERElF2fenf79m3V7q1gwYKq67qbm5ts3LjRuPzYsWOq4bth8LpMD/ZoTfjqq6/KypUr5cKFC/L+++/LggULVEMCDK6D+XjsLRERkbPTZdJT7/AYeHSpO3v2rCoVRxx1dXVVA9T5+Pio0nJUCaBdHBrsdevWTQV6WxrnZciDcADF+fXr15fjx4+r6eDBg9KlSxfJly+fqttHzp+IiMhZuWRSz7F///1XBfa4uDjx8/NTsRPd6vBvmDBhgri4uKjBdNDCPzQ0VKZOnWrzfuwa7FGP8P3336uAfvr0aWnbtq2sXr1adRtISEhQQ+Yi6J87d86euyUiIsqSFi1alOJydMeLjIxUU3qkuxi/ePHiKkXSqlUr9WhbdCFAET7q8H/44Qdj/0AvLy/p16+fKuInIiJyZrpMKsbPLOnO2SOX/ujRI/H391f1Dik1GkCxxJkzZ9K7SyIiogylc9ao7ahgr9fr1V8M6ZeaLw/9B4mIiJyZTlux3j519uvWrVOtBlOCFvlERERZgYvGor1dgj0a3T0rR4+ifiIioqxAJ9pil372GDgHfeiTmxjoiYiIsnDOXmuNGIiIiHQai212a6BHRESkFS7aivXpD/aor+eT7oiISEt0zNmbw2h5REREWqLTVqzPmLHxiYiIsjKdxqK93Z56R0RERM6JOXsiIiKNN9CzW84edfd37tyx1+aIiIgcWoyvS+Ok6WA/aNAgCQwMlO7du8u2bdvstVkiIqJMp0vHpOlgj0fazp07V65evSqNGzeWkJAQGT16tBpdj4iIKKuNje+SxknTwT5Hjhzy6quvysqVK9Uz6/FM+wULFkhQUJB6CA7mY+hcIiIi0kBr/ICAAKlfv756tr2Li4scPHhQDb5TokQJ+eOPPzJil0RERHaj06V90nxr/MuXL8v333+vGuudPn1a2rZtK6tXr5ZmzZpJQkKCjBgxQgX9c+fO2XO32cbRBT2kaGDeJPOnr9wjfSetk3XjOkrDykXNls1c9bf0nrg2xe0O6dpQurWsLHlze8j2Q/9K76/XyqmL1+1+/GSb5T/Mlp1bN8nFC2fF3cNDSperKJ3e6yWFigQb17l/P1HmTZ8o2/5YLw8e3JdK1WvLe70HSd58vikOcb1k7jeycc1ySbh9W0Ker6TeU/C5oEz6ZJQa0yIny/SpU8zmBRcrJitXJ389r1+3RiInfy3RFy9KUNFg6RP+iTRo2CgTjlZ7dM4atR0d7Fu1aqWea1+6dGlVhN+5c2fJnz+/cbmXl5f069dPxo4da69dZjv1e8wRV5P+IOWK+cmvY9+WZZv/Mc6btXqvfD5ni/H1ncQHKW6zX4fa0uPV6vL+6FVyNiZehnZtJKu+7CBV3p0hiQ/4tEJHOnLgbwlt/bqUKFNOPTnyh+8iZeSgnjL+26Xi+f9DVM+dNl7+3rFVwod8Kbm8csusKWNk3Gf95fOvv0t2uysXz5U1KxZJ2IDPxD+wsCyeM01GDe4l42ctEXd3j0z8hPQsJUqWkhnfPh2l1DWHa7Lr7tv7twzq30969wmXho2ayK+/rJI+vcJk0Y/LpFSp0pl0xNqh01ast18xvr+/v2zevFkOHTokffr0MQv0Bn5+fnLmzBl77TLbuXrjjly+nmCcWtYuKacuXpM/9583rnM38YHZOrfu3E9xm2Htasro+X/J6m0n5NDpK/Le6FVSsEAeaV2/TCZ8IkrJ/yImS+PQVlIkuIQElygtYf0/k6uxMXL6xJPE3Z2E2/L72pXS5cO+Ur5KDSleuqz0+GSYHDtyQI4fOZhsrv7X5T9Iu47dpUbdxlK0eCnpOXCEXI+7Irv+YhWbs8nh6ioF/PyMU758Se+rBgvmz5O69RtI13ffk+IlSkjP3n2kbLlysmjh/Ew9Zq1wYQM962bNmqXq6J9VLFK0qHkxM6WNWw4X6dCsvMxde8Bs/psvlJcLy/rI7m/flxHdG0tOj+QLb4IL5pWCvrnl97+fJsBuJiTKrn+ipVa5whl6/GQ7BHfIncdb/T19/B959PChVKhay7hO4aBgKeAfKMf/MT8vDGJjLkr8tTipWKWmcR5KBEqGlE82gUCOc+78OWnWuL60DH1BBg/oJ5eio5Nd98C+fVK7tvk9uG69+mo+2Y519iYmTZqU6nV79+6dnl2Rhdb1ykje3J4yf93Tm/ri3w/L+cs35FLcbalQ3F9Gvt9EShfxlQ6f/WR1G4H5vNTf2OsJZvPxOuD/l5FzQE+WOdPGSZnnK0lQsZJqXvz1OMnh5iZeufOYreuTL78K6NYY5vtY1Omr91y3/h5yjAoVK8rnoyIkOLiYXLlyRb6ZFindOneUn1auEi+v3EnWR7dnX98CZvN8fX3latzVTDxqclbpCvYTJkxI1XrI0dsz2KNr37Bhw+S775Kvl0xMTFSTKf3jh6Jz0cYIwV1aVJJ1O0+pwG7w3S9PU/CHz1xRy9aO6yjFCuaVM5fiHXSkZA+zJo+WC2dPyYgJ3zr6UCiT1G/wtGFd6TIhUqFiJWnxYhNZt3aNtGv/ukOPLTvQOWsWPY3SFfkcVf9+7do1NYBPSsE+IiJChg8fbjbPNbipuBV/QbK6IH9vaVo1ONkcu8Guo0+K/EoUzmc12Mf8f47eP5+XxFx7mrvH6wOnLtv9uCntgR6N8IaPmyG+fgHG+Whx//DBA0m4fcssd3/j+jXJm996a3zD/BvX4ySfSS4Q70G7AHJe3t7eUrRosFw4/7SNjqkCBQpInEUuPi4uTgpY5PYpez4lzimzuT///HOKy9Gt71kGDx4s4eHhZvP820wULXjnpUoSG39H1kSdTHG9SiWeBIaYa09z/6bOXopXuf8mVYPlwKlYNS9PLnepUbaQ6rJHjoXGdN9NGSM7//pDPvvqG/EvaN6OAg3yXHPkkIN7d0rtBk8SsdEXzqpGfKXLVrS6TbS+R8A/uHeXBJcsY2wLcPLoIWneqn0mfCpKqzsJCapU8+XWflaXV6xcWXZERUmnzl2N86K2b1PzyXbM2afg33//VYH6/Pnzcv++eSvw8ePHp3o76J+PLxo3u7T+EB4eHmoye48GivDxsTu/VFEWrD8gjx4//X5QVP/mC8/Luh2nJO7mXVVnP6ZHM9VSH63sDfbN/o8M/XaT/PzXcfU6ctlOGdixnpz897rqejesW0O5dPWW/Lz1mEM+H5nn6Lf+vlYGDB8nOXPlkvhrV40N6tw9PNXfpi+1kXnTJ0juPD6SK5eXfBc5VvXHL12ugnE7fd5tL2+/21Nq1m+irpuWr74lyxbOkoKFi6gExKI50ySfr5/UqNfYgZ+WLI0bO1oaNW4iBQsVkiuxsarfvauri7Ro+Ypa/r/BA8TfP0A+7ttPve7YqbN07/qOzJ3znTRs2EjWrvlVDh86JEM+G+HgT5I1uWgr1tsv2G/cuFENi1u8eHE5evSolC9fXs6ePasCdtWqVW3aVsGCBWXq1KnSpk0bq8v37dsn1apVk+yoadViEhTgk6QV/oOHj9Synu1riJenu/wbe1NW/HlUvpz/l9l6ZYJ8xTv300TQuEVRksvTXaaEt1AN/rYdvCCtBy9mH3snsH7Vj+rvZ5/8x2w+utehSx50+ShcdDoXGTdigDzEoDrV6sh7vQearR994ZyxJT+0ebOLJN67J99M/ELu3L4lIeUry38jJrGPvZO5fDlGBvUPl/j4eMmXP79UqVpNvl+4xNitOebSJXHRPS1srlylqkSM+UqmTJookyeOV4PqTJwcyT72aeSisWCv06eUfbZBzZo1pUWLFqqePE+ePLJ//37V975jx47y0ksvyUcffZTqbSHRULlyZTXinjXYdpUqVWweaz/nC1/YtD5lbVGzezn6ECgTlSlk3iuBtM0zgwtqw38+mub3jm8dIs7Gbl/XP//8Iz/88MOTjebIIXfv3pXcuXOrgI0cui3Bvn///mp43eSULFlSNm3aZJfjJiIi0nqdvd0aHGI4XEM9PYrhT506Zdb/0xYNGjRQpQEp7atRI473TEREGVeM75LGKa2+/PJLlcjAKLQG9+7dk7CwMDVmAjLQ7du3V8+hsfnziJ3Url1btm7dqv7dsmVLNQ7+qFGj5N1331XLiIiIsgpdJo+gt2vXLvnmm2+kYkXznjR9+/aVVatWydKlS9WQ9NHR0dKuXTvHFeOjtf3t208aAaHeHv9evHixlCpVyqaW+ERERI7mkonF+IiXaN82c+ZMGTlypHH+jRs31FD0CxculKZNm6p5eKps2bJlJSoqyqaMtN2CPVrhmxazT58+3V6bJiIiylQu6XivtRFcrXUHN0Ax/csvv6weB28a7Pfs2SMPHjxQ8w1CQkIkKChItm/fblOwd8moVMrNmzfNJiIiouwgIiJCfHx8zCbMs2bRokXy999/W10eExMj7u7ukjdvXrP5AQEBapkt7Jazx9C5PXv2lD/++EM1KDBAzz40OMDzuImIiLICXTpK8a2N4GotV48RET/++GPZsGGDeHp6SkayW7Dv1KnTk+E9v/tOpTq01m2BiIiyD5d0xLCUiuxNoZg+NjbWbOA5ZIy3bNkiU6ZMkXXr1qlebhhYyTR3j9b4gYGBjgn2GOgGB16mzJPxtomIiLIqXSbkV1944QU5ePCg2bxu3bqpevmBAwdKkSJFxM3NTY1Qiy53cOzYMTUkfZ06dRwT7GvUqKGKJBjsiYgoq3PJhGCP0WYxtLwpNHBHn3rD/O7du6sqAQyTjCcf9urVSwV6W7u02y3Yf/vtt/Lhhx/KxYsX1UEiNWLKsu8gERGRs3JxkqroCRMmiIuLi8rZo4V/aGioenaMrewW7K9cuaJGzUMRhIHhyXVsoEdERPRsaORuCg33IiMj1ZQedgv2GCkPD6fB+PhsoEdERFmZTmMhzG7B/ty5c+pZ9nhIDRERUVbmorFgb7dBdTCUH1rkExERZXW6dPyn6Zx9q1at1ID96EZQoUKFJA308Ix6IiKirMDFOWO244M9WuIDnl9viQ30iIgoK3FhsLfu8ePH9toUEREROWOwJyIi0gqt9Siz61PvNm/erOru0SIfE+rp//zzT3vugoiIKFOK8V3SOGk62M+fP189czdXrlzSu3dvNeXMmVON/btw4UJ77YaIiCjD6XRpnzRdjD9q1CgZM2aMapFvgIA/fvx4+fzzz+Xtt9+2166IiIiyxXC5TpezP336tCrCt4SifDzrnoiIKKtwYTG+dXgUHx7DZ+m3335Ty4iIiCiLF+P369dPFdvv27dP6tatq+b99ddfMmfOHPn666/ttRsiIqIMp3PSHLrDg/1HH30kgYGBMm7cOFmyZImaV7ZsWVm8eLG0adPGXrshIiLKcC5OOuytU/Szf/XVV9VERESUlem0FevtP6jO/fv3JTY2NsmIekFBQfbeFRERUYZwYbC37sSJE+qZ9tu2bTObr9frOTY+ERFlKS4ay9rbLdh37dpVcuTIIatXr5aCBQtqbqhBIiIiye7BHq3w9+zZIyEhIfbaJBERkUPoNJZftVuwL1eunFy9etVemyMiInIYF41Fe7sNqjN69GgZMGCA/PHHHxIXFyc3b940m4iIiLIKHcfGtw4PwQE8+MYUG+gREVG2fiSsloL9pk2b7LUpIiIih9I5axbd0cG+UaNGyS47dOiQvXZDREREzlJScevWLZkxY4bUrFlTKlWqlFG7ISIisjtdOqZsEey3bNkiXbp0UX3tv/rqK2natKlERUXZezdEREQZ2hrfJY2TZovxY2Ji1NPtZs2apVrev/HGG5KYmCgrVqxQXfKIiIiyEp1oS7pz9q1atZIyZcrIgQMHZOLEiRIdHS2TJ0+2z9ERERE5gI5d78ytWbNGPccej7gtVaqUfY6KiIjIgXTOGrUdlbPfunWraoxXrVo1qVWrlkyZMoUj6REREWkp2NeuXVtmzpwply5dkv/85z+yaNEiKVSokHrE7YYNG1RCgIiIKKsFR5c0TraYNm2aVKxYUby9vdVUp04dVWJucO/ePQkLCxNfX1/JnTu3tG/fXi5fvpymz2MXXl5e6hG3yOkfPHhQ+vXrJ19++aX4+/tL69at7bUbIiKiTCnG16VxssVzzz2nYiUeJLd7927Vg61NmzZy+PBhtbxv376yatUqWbp0qWzevFm1i2vXrp3tn0eP8WwzCIbIxUF+99138vPPP4uj5XzhC0cfAmWiqNm9HH0IlInKFMrj6EOgTORptyHhrFu6L1rS6vXKhSQ98ufPL2PHjpXXXntN/Pz8ZOHCherfcPToUSlbtqxs375dlaynVoZ+Xa6urtK2bVs1ERERZYcGeomJiWoy5eHhoaZnZZCRg09ISFDF+cjtP3jwwPjsGcBj5IOCgpwr2Dubvv2epIwoe2BOL3tp/NVmRx8CZaKoQckP0W4PLul4b0REhAwfPtxs3rBhw+Szzz6zuj6qvhHcUT+Pevnly5erMWr27dsn7u7ukjdvXrP1AwIC1Pg2tshWwZ6IiCijDR48WMLDw83mpZSrx1g1COw3btyQH3/8UY1Ci/p5e2KwJyIismMxfmqK7E0h916yZEn1b3Rj37Vrl3z99dfy5ptvyv379yU+Pt4sd4/W+IGBgdn6kb1ERERZ+kE4jx8/VnX+CPxubm6yceNG47Jjx47J+fPnVbG/LZizJyIispBZA+ihyL9Fixaq0R3GpUHL+z/++EPWrVsnPj4+0r17d1UlgBb66Iffq1cvFehtaZwHDPZEREQWXDLpUTixsbHSuXNnNTAdgjsG2EGgf/HFF9XyCRMmiIuLixpMB7n90NBQmTp1qs37YbAnIiJyUM4eT4tNiaenp0RGRqopPVhnT0REpHHM2RMREVnQaeyJ9gz2REREFjT2hFsGeyIiIkc10MssDPZEREQWmLMnIiLSOJ3Ggj1b4xMREWkcc/ZEREQW2BqfiIhI41y0FesZ7ImIiCwxZ09ERKRxOm3FejbQIyIi0jrm7ImIiCywGJ+IiEjjXLQV6xnsiYiILDFnT0REpHE6bcV6BnsiIiJLGov1bI1PRESkdczZExERWXDRWDk+gz0REZEFbYV6BnsiIiLNR3sGeyIiIgvsekdERKRxOm3FerbGJyIi0jrm7ImIiCxoLGPPYE9ERKT1aM9gT0REZIEN9IiIiDROp61Yz2BPRERkSWOxnq3xiYiItI7BnoiIyFrWPq2TDSIiIqRGjRqSJ08e8ff3l7Zt28qxY8fM1rl3756EhYWJr6+v5M6dW9q3by+XL1+2aT8M9kRERFYa6KX1P1ts3rxZBfKoqCjZsGGDPHjwQJo3by4JCQnGdfr27SurVq2SpUuXqvWjo6OlXbt2Nu2HdfZEREQOaqC3du1as9dz5sxROfw9e/ZIw4YN5caNGzJr1ixZuHChNG3aVK0ze/ZsKVu2rEog1K5dO1X7Yc6eiIjIjqX4iYmJcvPmTbMJ81IDwR3y58+v/iLoI7ffrFkz4zohISESFBQk27dvl9RisCciIrJjtEc9vI+Pj9mEec/y+PFj6dOnj9SrV0/Kly+v5sXExIi7u7vkzZvXbN2AgAC1LLVYjE9ERGRHgwcPlvDwcLN5Hh4ez3wf6u4PHTokW7duFXtjsCciIrLjCHoI7KkJ7qZ69uwpq1evli1btshzzz1nnB8YGCj379+X+Ph4s9w9WuNjWWqxGJ+IiMhKA720TrbQ6/Uq0C9fvlx+//13KVasmNnyatWqiZubm2zcuNE4D13zzp8/L3Xq1En1fpizJyIictAIeii6R0v7lStXqr72hnp41PPnzJlT/e3evbuqFkCjPW9vb+nVq5cK9KltiQ8M9lnYsd+WyqFf5knJhq2l0qvvq3mPHtyXAytnyb97/5RHDx9IQEgVqfLaR+KZJ1+KKcsjaxfI2e3r5f69BPENLitVXu8hefwKZeKnoWeZFjlZpk+dYjYvuFgxWbnavOuOqfXr1kjk5K8l+uJFCSoaLH3CP5EGDRtlwtGSrd6rX1Teqx9sNu9s3B3pMHOX+vfUtytJ1SDzRlrL9kbLmHUnUtzu+w2CpU2lQMntkUMOXryp1r9w/W4GfAKN0WXObqZNm6b+Nm7c2Gw+utd17dpV/XvChAni4uKiBtNBq/7Q0FCZOnWqTfthsM+irp0/Lqe3rxWfQuY3h/0rvpWYI7ukVteB4ubpJft+mi5R30VI44/HJLut47//JKe2rJbqb/cRL98AObxmgWydPlSaD5oqrm7umfBpKLVKlCwlM76dbXztmsM12XX37f1bBvXvJ737hEvDRk3k119WSZ9eYbLox2VSqlTpTDpissWpKwnSa9F+4+tHj/Vmy1fsi5YZf541vr734HGK23unVhF5o1phGfHLUbkUf08+aBgsE9+sIG/N3CX3H5lvmxzz1Dtktp7F09NTIiMj1ZRWrLPPgh4m3pVd88dJ1Td6iVvO3Mb5D+4myNkdG6Rim/fEv1QlyVekpFR762OJO/uPxJ09muyJdnLzzxLS/A0pVKG2+BQqJjXe7iv3bl6T6INRmfipKDVyuLpKAT8/45Qv35O+uNYsmD9P6tZvIF3ffU+KlyghPXv3kbLlysmihfMz9Zgp9RDcryU8ME437j40W47gbrr8zv1HKW7vzRqFZfa2c/LniTg5eSVBhq8+KgVye0jD0gUy+JOQs2Gwz4L2/jhdAstWl4Aylc3mX//3pOgfPRT/MpWM87wDikiufH5yLZlgnxB3We7dui7+pZ9uyy2nl+QvWjrZBAI5zrnz56RZ4/rSMvQFGTygn1yKjk523QP79knt2uYNeOrWq6/mk3Mqki+nrAqrLT99WFOGtwqRAG/zFt2hz/vL2t51ZUH36vJRo2LikSP5W3ghH08V2HedvW6cl5D4SA5H35QKhb0z9HNogS6TGuhlFqctxr97964aOQgNEsqVK5fkoQBLliyRzp07J/t+1GtYjlj08MF9yZHFi6Uv/L1F4i+ekqZ9xydZdu/mdXFxzSHuJrl98MiTV+7dire6vcRbT24EHrnN6wLx2rCMnEOFihXl81EREhxcTK5cuSLfTIuUbp07yk8rV4mXl/lvDlevXhVfX/McHB6kcTXuaiYeNaXW4ehb8vkvR+X8tbvim9tdutcrKtM7VpaOs3arHPy6w7ESc/OeXL19X0r6eUlY4+JSNH9OGbT8iNXtYRuAEgBT1xLui69X1r4PZgadaItT5uyPHz+uxv3FuMAVKlSQRo0ayaVLl8yGE+zWrVuK27A2gtH2Jd9IVnbn+hXZv3ym1OzUj3Xp2VD9Bo2keWgLKV0mROrVbyBTps2QW7duyrq1axx9aGQH209fk9+PXVXF7TvOXJfwpQclj0cOeSHETy1fuf+Smo96/XVHYmX4L0elcRk/KZzX09GHrk26zHnqXbYO9gMHDlRDBcbGxqr+hOiOgOED0a/QlhGMkCgwneq88R/JylBMn3g7XjaO6yPL+rVR09VTh+Tkn6vUv9Hi/vGjh3L/7m2z9yXeihfPPOY5dwOP/2+lj+2aved2vHEZOSd0wSlaNFguJHNdFChQQOIscvFxcXFSwCK3T87pduIjOX/9jjyXL6fV5SiOh+SWx92+r/7m93Izm5/fy13iEp4sI8c/9S5bF+Nv27ZNfvvtN3WzwoRH+/Xo0UMaNGggmzZtEi8vrzSNYJTVi/DR6K7ZAPOuV3t+mCh5/J+T0i+8JrnyFhCdaw65cny/FK5UTy2/FfuvKhHIHxxidZtofY9EQuzx/ZK3cHE178G9O3Lt3HEpXrdlJnwqSqs7CQly4cIFebn1k5yfpYqVK8uOqCjp1PlJ9x2I2r5NzSfnl9PNRQrnzSlrb8daXV7aP7dZULcUfQNF/olSIzifnIh98rjUXO6u8nwhb9Vlj1LmrHXvmsrZo74+R46n6RCdTqf6IrZq1UoV6aOYPzty88wlPgWLmk2u7p7i7uWt/o2GdcG1XlT97GNPHJDrF07K7h++VoHe1yTYr4v4UC4e2G78bks2ai1HNyyW6EM75Eb0Wdm1YLx4eudXrfPJeYwbO1p279opFy/+q7rV9f24p7i6ukiLlq+o5f8bPEC+njDOuH7HTp1l219/ytw538mZ06dUP/3Dhw5Jh7c7OfBTUHJ6NSkuVYr4SEEfD9WAbnS78vJYr5f1R2JVUX23ukFSJiC3Wt6gpK8MfSVE/j4fr4r9DRa9X0MalfY1vl6866J0rRuk1i/h5yXDXglRCYAtx9luI7txypw9Ht+3e/duVW9vasqUJ7na1q1bO+jInF+ltu/JAZ1OouZEyGMMqlOmqhpUx9Tt2Ivy4N7TG0Tppu3l4f178veSKar7nm+xclL/P8PZLsDJXL4cI4P6h6sxsvPlzy9VqlaT7xcuMT4KM+bSJXHRPU2/V65SVSLGfCVTJk2UyRPHq0F1Jk6OZB97J+Wfx0NGtC4rPjndJP7OA9n/7w15b95eib/7QNxzuKgceocaz4mnm6vE3rwnfxy7Kt9tO2e2jWDfXGrwHIPvd1wQT3dXGfRSacntmUMO/HtD+iw+yD72qaCxjL3o9Knp0Z/J0Ljuzz//lF9//dXqchTpT58+XT0O0Bb//TV7lghkV0ObM6hlJ42/2uzoQ6BMFDUoY0eCPH75TprfWzoglzgbpyzGR+O65AI9YJhAWwM9ERFRarGBHhERkcbpnDNmpxmDPRERkQWNxXrnLMYnIiIi+2HOnoiISONZewZ7IiIiC87a0C6tGOyJiIgssIEeERGRxulEWxjsiYiINB7t2RqfiIhI45izJyIissAGekRERBqn01asZ7AnIiKypLFYz2BPRERkiTl7IiIizdOJlrA1PhERkcYxZ09ERGSBxfhEREQapxNtYbAnIiKywJw9ERGRxuk0lrdnsCciIrKkrVjP1vhERESOsmXLFmnVqpUUKlRIdDqdrFixwmy5Xq+XoUOHSsGCBSVnzpzSrFkzOXHihM37YbAnIiKykrFP62SLhIQEqVSpkkRGRlpdPmbMGJk0aZJMnz5dduzYIV5eXhIaGir37t2zaT8sxiciInJQA70WLVqoyRrk6idOnCiffvqptGnTRs2bN2+eBAQEqBKADh06pHo/zNkTERFZaaCX1v8SExPl5s2bZhPm2erMmTMSExOjiu4NfHx8pFatWrJ9+3abtsVgT0REZMdy/IiICBWUTSfMsxUCPSAnbwqvDctSi8X4REREFtJTij948GAJDw83m+fh4SGOxGBPRERkRwjs9gjugYGB6u/ly5dVa3wDvK5cubJN22IxPhERkZUGemmd7KVYsWIq4G/cuNE4D/X/aJVfp04dm7bFnD0REZGDRtC7ffu2nDx50qxR3r59+yR//vwSFBQkffr0kZEjR0qpUqVU8B8yZIjqk9+2bVub9sNgT0RE5KCud7t375YmTZoYXxvq+rt06SJz5syRAQMGqL74H3zwgcTHx0v9+vVl7dq14unpadN+GOyJiIgcpHHjxqo/fXIwqt6IESPUlB4M9kRERBp/6h0b6BEREWkcc/ZEREQW+IhbIiIijdNpK9Yz2BMREVnSWKxnsCciItJ6tGcDPSIiIo1jzp6IiMgCG+gRERFpnE5bsZ7BnoiIyJLGYj2DPRERkdajPYM9ERGRxuvs2RqfiIhI45izJyIi0ngDPZ0+pWfrUZaXmJgoERERMnjwYPHw8HD04VAG4++dvfD3ptRisNe4mzdvio+Pj9y4cUO8vb0dfTiUwfh7Zy/8vSm1WGdPRESkcQz2REREGsdgT0REpHEM9hqHRjvDhg1j451sgr939sLfm1KLDfSIiIg0jjl7IiIijWOwJyIi0jgGeyIiIo1jsCciItI4BnuNi4yMlODgYPH09JRatWrJzp07HX1IlAG2bNkirVq1kkKFColOp5MVK1Y4+pAoA2GI3Bo1akiePHnE399f2rZtK8eOHXP0YZETY7DXsMWLF0t4eLjqmvP3339LpUqVJDQ0VGJjYx19aGRnCQkJ6vdF4o60b/PmzRIWFiZRUVGyYcMGefDggTRv3lydB0TWsOudhiEnj9T/lClT1OvHjx9LkSJFpFevXjJo0CBHHx5lEOTsly9frnJ7lD1cuXJF5fCRCGjYsKGjD4ecEHP2GnX//n3Zs2ePNGvWzDjPxcVFvd6+fbtDj42I7AsPwoH8+fM7+lDISTHYa9TVq1fl0aNHEhAQYDYfr2NiYhx2XERkXyix69Onj9SrV0/Kly/v6MMhJ5XD0QdARERph7r7Q4cOydatWx19KOTEGOw1qkCBAuLq6iqXL182m4/XgYGBDjsuIrKfnj17yurVq1VvjOeee87Rh0NOjMX4GuXu7i7VqlWTjRs3mhX34XWdOnUcemxElD5oV41Aj4aYv//+uxQrVszRh0ROjjl7DUO3uy5dukj16tWlZs2aMnHiRNU1p1u3bo4+NLKz27dvy8mTJ42vz5w5I/v27VMNtoKCghx6bJQxRfcLFy6UlStXqr72hnY4Pj4+kjNnTkcfHjkhdr3TOHS7Gzt2rLoZVK5cWSZNmqS65JG2/PHHH9KkSZMk85HYmzNnjkOOiTK2e6U1s2fPlq5du2b68ZDzY7AnIiLSONbZExERaRyDPRERkcYx2BMREWkcgz0REZHGMdgTERFpHIM9ERGRxjHYExERaRyDPRERkcYx2BM5aAS0FStWOPowiCibYLAnsjMMTdyrVy8pXry4eHh4SJEiRaRVq1ZmDyWy91C5SDzEx8dLZiVO8NoweXl5SalSpdQwrXv27MmwYyCitGOwJ7Kjs2fPqqcN4klkeCbBwYMHZe3atWrcejy8xJlh5OyHDx+men2Mw37p0iU5fPiwREZGqofx4LkL8+bNy9DjJCLbMdgT2VGPHj1Ubnfnzp3Svn17KV26tDz//PPqCYRRUVGpzpnjiXWYh8QDnDt3TpUO5MuXT+Wksc1ff/1VLTc8AAfL8B7Dg1DwSOOIiAj1+FM8Ca1SpUry448/JtnvmjVrVAIFpRBbt25N9WfNmzevBAYGSnBwsDRv3lxtu2PHjurRq9evX0/zd0hE9sdH3BLZybVr11QuftSoUSogWwuOaYVSgfv378uWLVvUto8cOSK5c+dWVQQ//fSTSlgcO3ZMvL29jY84RaCfP3++TJ8+XRWz472dOnUSPz8/adSokXHbgwYNkq+++kpVOyDBkB59+/ZVOfsNGzbIG2+8ka5tEZH9MNgT2QmeJ4+i8JCQELtv+/z58yqgV6hQQb1GYDbAM+vB39/fmKBITEyUL774Qn777TepU6eO8T3IuX/zzTdmwX7EiBHy4osv2uU4DZ/dUCJBRM6BwZ7ITjLyadG9e/eWjz76SNavXy/NmjVTgb9ixYopJjzu3LmTJIijdKBKlSpm86pXr2737yC5560TkWMw2BPZCYrKEeSOHj1q0/tcXFySJBYePHhgts57770noaGh8ssvv6iAjyL6cePGqVb/1qCxHGD9woULmy1D3bwpa1UOafXPP/+ov2gnQETOgw30iOwExekIyGiZnpCQkGR5cl3jUIcOaNlu2kDPEurnP/zwQ1m2bJn069dPZs6cqea7u7urv48ePTKuW65cORXUUfxfsmRJswnbySgTJ05U7QZQ+kBEzoM5eyI7QqCvV6+e1KxZU9WFo6gd3dnQYG3atGnGnK8pQwD+7LPPVOO+48ePq1y7qT59+kiLFi1U6360dN+0aZOULVtWLStatKgqUVi9erW0bNlSNdDLkyePfPLJJ6rBHFrl169fX27cuCF//fWXCsZdunRJ92dF4gVjCqB9AI4ZbQHQFx8N9NLTGJGIMoCeiOwqOjpaHxYWpi9atKje3d1dX7hwYX3r1q31mzZtMq6DS2/58uXG11u3btVXqFBB7+npqW/QoIF+6dKlap0zZ86o5T179tSXKFFC7+Hhoffz89O/8847+qtXrxrfP2LECH1gYKBep9Ppu3TpouY9fvxYP3HiRH2ZMmX0bm5u6n2hoaH6zZs3q+U4Huzj+vXrz/xMlseL14YJx4xjw3737Nljp2+RiOxJh/9lRCKCiIiInAPr7ImIiDSOwZ6IiEjjGOyJiIg0jsGeiIhI4xjsiYiINI7BnoiISOMY7ImIiDSOwZ6IiEjjGOyJiIg0jsGeiIhI4xjsiYiIRNv+Dwli5sasUYMjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create heatmap for easy visualisation\n",
    "\n",
    "ct = pd.crosstab(merged_df[\"anomtype\"], merged_df[\"cluster\"], normalize='index') * 100\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(ct, annot=True, cmap=\"Blues\", fmt=\".1f\")\n",
    "plt.title(\"Distribution of Cluster IDs within Each Anomaly Type (%)\")\n",
    "plt.ylabel(\"Anomaly Type\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa98208",
   "metadata": {},
   "source": [
    "From this heatmap, we see that the best possible accuracy we can achieve is 70%, if we do the following mapping:\n",
    "- Cluster ID 0 to anomaly type 1\n",
    "- Cluster ID 1 to anomaly type 0\n",
    "- Cluster ID 2 to anomaly type 2.  \n",
    "\n",
    "This is equal than the accuracy obtained from logistic regression (70%).\n",
    "\n",
    "Lastly, we try using PCA in addition to KMeans to see if it improves our performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ad541",
   "metadata": {},
   "source": [
    "### Unsupervised methods (PCA + KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa62a720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.795093</td>\n",
       "      <td>-2.274128</td>\n",
       "      <td>-0.962924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.387708</td>\n",
       "      <td>2.556361</td>\n",
       "      <td>-1.652732</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.945953</td>\n",
       "      <td>1.132854</td>\n",
       "      <td>-2.356515</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.934387</td>\n",
       "      <td>-1.511143</td>\n",
       "      <td>0.470568</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.282631</td>\n",
       "      <td>-0.465729</td>\n",
       "      <td>1.130008</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.185940</td>\n",
       "      <td>0.359961</td>\n",
       "      <td>-1.212563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>4.182887</td>\n",
       "      <td>-1.589698</td>\n",
       "      <td>0.742799</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>3.544117</td>\n",
       "      <td>-1.738656</td>\n",
       "      <td>1.196028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>-4.285714</td>\n",
       "      <td>-1.921067</td>\n",
       "      <td>-0.154156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>2.783019</td>\n",
       "      <td>1.098654</td>\n",
       "      <td>0.360206</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2       PC3  cluster\n",
       "user                                       \n",
       "0    -4.795093 -2.274128 -0.962924        1\n",
       "1    -3.387708  2.556361 -1.652732        0\n",
       "2     3.945953  1.132854 -2.356515        2\n",
       "3     1.934387 -1.511143  0.470568        2\n",
       "4     6.282631 -0.465729  1.130008        2\n",
       "...        ...       ...       ...      ...\n",
       "3595  0.185940  0.359961 -1.212563        0\n",
       "3596  4.182887 -1.589698  0.742799        2\n",
       "3597  3.544117 -1.738656  1.196028        2\n",
       "3598 -4.285714 -1.921067 -0.154156        1\n",
       "3599  2.783019  1.098654  0.360206        2\n",
       "\n",
       "[3600 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply PCA to reduce features to 3 dimensions\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(df_cla_2_scaled)\n",
    "\n",
    "# Train KMeans on PCA-transformed features\n",
    "pca_kmeans = KMeans(n_clusters=3, random_state=67)\n",
    "pca_clusters = pca_kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Save clusters from numpy array into a dataframe\n",
    "df_pca_kmeans = pd.DataFrame(X_pca, index=df_cla_2.index, columns=[\n",
    "    \"PC1\", \"PC2\", \"PC3\"\n",
    "    ])\n",
    "df_pca_kmeans[\"cluster\"] = pca_clusters\n",
    "\n",
    "df_pca_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad41ca6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check how clusters align with true anomaly types\n",
    "pca_merged = pd.merge(df_pca_kmeans, y_cat, on=\"user\", how=\"inner\")\n",
    "\n",
    "# View alignment\n",
    "ct = pd.crosstab(pca_merged[\"anomtype\"], pca_merged[\"cluster\"], normalize=\"index\") * 100\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(ct, annot=True, cmap=\"Blues\", fmt=\".1f\")\n",
    "plt.title(\"Distribution of Cluster IDs within Each Anomaly Type (%)\")\n",
    "plt.ylabel(\"Anomaly Type\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89f228",
   "metadata": {},
   "source": [
    "We found that applying PCA to reduce our data does not improve our performance (still 70%).  \n",
    "We try another semi-supervised method called label propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e36c85",
   "metadata": {},
   "source": [
    "### Semi-supervised method (Label propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83cf66e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>interaction_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.676375</td>\n",
       "      <td>-0.753724</td>\n",
       "      <td>-1.307564</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>1.455813</td>\n",
       "      <td>-1.223091</td>\n",
       "      <td>2.072483</td>\n",
       "      <td>-0.360042</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.729360</td>\n",
       "      <td>1.555813</td>\n",
       "      <td>-1.677011</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>-1.649779</td>\n",
       "      <td>1.003710</td>\n",
       "      <td>1.071897</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>-0.906606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>1.174038</td>\n",
       "      <td>1.206414</td>\n",
       "      <td>-1.375877</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>-1.721433</td>\n",
       "      <td>1.295657</td>\n",
       "      <td>1.452570</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019037</td>\n",
       "      <td>-0.879531</td>\n",
       "      <td>-1.009237</td>\n",
       "      <td>-0.370275</td>\n",
       "      <td>-1.139736</td>\n",
       "      <td>-1.273539</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>0.553194</td>\n",
       "      <td>-0.490828</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.098623</td>\n",
       "      <td>-0.591878</td>\n",
       "      <td>0.491361</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>-1.419891</td>\n",
       "      <td>1.165742</td>\n",
       "      <td>-1.287937</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452419</td>\n",
       "      <td>-0.846373</td>\n",
       "      <td>1.420099</td>\n",
       "      <td>-2.008956</td>\n",
       "      <td>-0.871282</td>\n",
       "      <td>0.961288</td>\n",
       "      <td>1.004850</td>\n",
       "      <td>0.901839</td>\n",
       "      <td>-1.773607</td>\n",
       "      <td>2.443201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.518016</td>\n",
       "      <td>-0.609861</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>-0.199227</td>\n",
       "      <td>0.795841</td>\n",
       "      <td>-0.557479</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193840</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>1.191901</td>\n",
       "      <td>0.396310</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>0.805404</td>\n",
       "      <td>0.153503</td>\n",
       "      <td>0.826007</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>-0.238795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.999091</td>\n",
       "      <td>-1.455058</td>\n",
       "      <td>1.880405</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>-1.455088</td>\n",
       "      <td>1.915819</td>\n",
       "      <td>-1.182528</td>\n",
       "      <td>-1.930129</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954757</td>\n",
       "      <td>-1.284317</td>\n",
       "      <td>1.013126</td>\n",
       "      <td>-2.067286</td>\n",
       "      <td>1.829722</td>\n",
       "      <td>1.628609</td>\n",
       "      <td>-1.871693</td>\n",
       "      <td>0.767914</td>\n",
       "      <td>-1.269377</td>\n",
       "      <td>1.873478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.435073</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>-0.430872</td>\n",
       "      <td>-0.607792</td>\n",
       "      <td>-0.193510</td>\n",
       "      <td>-0.117082</td>\n",
       "      <td>-0.290259</td>\n",
       "      <td>0.556078</td>\n",
       "      <td>-0.607792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081018</td>\n",
       "      <td>-0.339776</td>\n",
       "      <td>-0.094876</td>\n",
       "      <td>0.042066</td>\n",
       "      <td>-0.964181</td>\n",
       "      <td>-0.066876</td>\n",
       "      <td>-0.905806</td>\n",
       "      <td>0.919871</td>\n",
       "      <td>-0.117763</td>\n",
       "      <td>0.123364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.766848</td>\n",
       "      <td>-1.077417</td>\n",
       "      <td>0.855701</td>\n",
       "      <td>-0.294320</td>\n",
       "      <td>-0.451492</td>\n",
       "      <td>1.351945</td>\n",
       "      <td>-0.848897</td>\n",
       "      <td>-1.344620</td>\n",
       "      <td>-0.294320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674505</td>\n",
       "      <td>-0.811832</td>\n",
       "      <td>0.705402</td>\n",
       "      <td>-0.350036</td>\n",
       "      <td>1.283952</td>\n",
       "      <td>1.204257</td>\n",
       "      <td>-1.058761</td>\n",
       "      <td>0.590638</td>\n",
       "      <td>-0.239701</td>\n",
       "      <td>0.545020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.667315</td>\n",
       "      <td>-0.825656</td>\n",
       "      <td>0.924014</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>-0.227664</td>\n",
       "      <td>1.133765</td>\n",
       "      <td>-0.766024</td>\n",
       "      <td>-1.062213</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.187697</td>\n",
       "      <td>0.115373</td>\n",
       "      <td>1.216609</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.954901</td>\n",
       "      <td>1.038749</td>\n",
       "      <td>0.212611</td>\n",
       "      <td>-1.827281</td>\n",
       "      <td>0.195116</td>\n",
       "      <td>0.164125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.626608</td>\n",
       "      <td>-0.232219</td>\n",
       "      <td>-0.863525</td>\n",
       "      <td>-0.484303</td>\n",
       "      <td>1.729776</td>\n",
       "      <td>-0.843437</td>\n",
       "      <td>1.177393</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>-0.484303</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.137427</td>\n",
       "      <td>0.825328</td>\n",
       "      <td>-1.138540</td>\n",
       "      <td>1.882849</td>\n",
       "      <td>0.095666</td>\n",
       "      <td>-1.151333</td>\n",
       "      <td>1.660008</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>1.372524</td>\n",
       "      <td>-1.553903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.733670</td>\n",
       "      <td>0.343234</td>\n",
       "      <td>0.616603</td>\n",
       "      <td>0.275628</td>\n",
       "      <td>-0.473418</td>\n",
       "      <td>0.459635</td>\n",
       "      <td>-0.892503</td>\n",
       "      <td>0.272674</td>\n",
       "      <td>0.275628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496048</td>\n",
       "      <td>-0.202136</td>\n",
       "      <td>0.440186</td>\n",
       "      <td>-0.501901</td>\n",
       "      <td>-0.872557</td>\n",
       "      <td>0.480644</td>\n",
       "      <td>-1.296826</td>\n",
       "      <td>-0.319118</td>\n",
       "      <td>-0.496897</td>\n",
       "      <td>0.798083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                         \n",
       "0       0.217773       0.676375      -0.753724   -1.307564   \n",
       "1       0.217773       1.174038       1.206414   -1.375877   \n",
       "2       0.217773      -1.098623      -0.591878    0.491361   \n",
       "3       0.217773      -0.518016      -0.609861    0.559675   \n",
       "4       0.217773      -0.999091      -1.455058    1.880405   \n",
       "...          ...            ...            ...         ...   \n",
       "3595    0.217773      -0.435073       0.001559   -0.430872   \n",
       "3596    0.217773      -0.766848      -1.077417    0.855701   \n",
       "3597    0.217773      -0.667315      -0.825656    0.924014   \n",
       "3598    0.217773       0.626608      -0.232219   -0.863525   \n",
       "3599    0.217773      -0.733670       0.343234    0.616603   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0              -1.101748        1.455813   -1.223091       2.072483   \n",
       "1               0.161638        0.032727   -1.721433       1.295657   \n",
       "2              -0.531799       -1.419891    1.165742      -1.287937   \n",
       "3              -0.151833       -0.199227    0.795841      -0.557479   \n",
       "4               0.228133       -1.455088    1.915819      -1.182528   \n",
       "...                  ...             ...         ...            ...   \n",
       "3595           -0.607792       -0.193510   -0.117082      -0.290259   \n",
       "3596           -0.294320       -0.451492    1.351945      -0.848897   \n",
       "3597           -0.047343       -0.227664    1.133765      -0.766024   \n",
       "3598           -0.484303        1.729776   -0.843437       1.177393   \n",
       "3599            0.275628       -0.473418    0.459635      -0.892503   \n",
       "\n",
       "      neutral_ratio  interaction_ratio  ...  user_bias  outlier_frac  \\\n",
       "user                                    ...                            \n",
       "0         -0.360042          -1.101748  ...  -1.729360      1.555813   \n",
       "1          1.452570           0.161638  ...  -1.019037     -0.879531   \n",
       "2         -0.487540          -0.531799  ...   1.452419     -0.846373   \n",
       "3         -0.721721          -0.151833  ...   1.193840      0.926231   \n",
       "4         -1.930129           0.228133  ...   0.954757     -1.284317   \n",
       "...             ...                ...  ...        ...           ...   \n",
       "3595       0.556078          -0.607792  ...  -0.081018     -0.339776   \n",
       "3596      -1.344620          -0.294320  ...   0.674505     -0.811832   \n",
       "3597      -1.062213          -0.047343  ...   1.187697      0.115373   \n",
       "3598       0.055999          -0.484303  ...  -1.137427      0.825328   \n",
       "3599       0.272674           0.275628  ...   0.496048     -0.202136   \n",
       "\n",
       "      mean_item_alignment  rating_entropy  extreme_ratio  user_mean_rank  \\\n",
       "user                                                                       \n",
       "0               -1.677011        0.549713       0.394985       -1.649779   \n",
       "1               -1.009237       -0.370275      -1.139736       -1.273539   \n",
       "2                1.420099       -2.008956      -0.871282        0.961288   \n",
       "3                1.191901        0.396310       0.542944        0.805404   \n",
       "4                1.013126       -2.067286       1.829722        1.628609   \n",
       "...                   ...             ...            ...             ...   \n",
       "3595            -0.094876        0.042066      -0.964181       -0.066876   \n",
       "3596             0.705402       -0.350036       1.283952        1.204257   \n",
       "3597             1.216609        0.012196       0.954901        1.038749   \n",
       "3598            -1.138540        1.882849       0.095666       -1.151333   \n",
       "3599             0.440186       -0.501901      -0.872557        0.480644   \n",
       "\n",
       "      avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                            \n",
       "0                1.003710         1.071897           0.976815   \n",
       "1                0.922383         0.553194          -0.490828   \n",
       "2                1.004850         0.901839          -1.773607   \n",
       "3                0.153503         0.826007           0.017939   \n",
       "4               -1.871693         0.767914          -1.269377   \n",
       "...                   ...              ...                ...   \n",
       "3595            -0.905806         0.919871          -0.117763   \n",
       "3596            -1.058761         0.590638          -0.239701   \n",
       "3597             0.212611        -1.827281           0.195116   \n",
       "3598             1.660008         0.610213           1.372524   \n",
       "3599            -1.296826        -0.319118          -0.496897   \n",
       "\n",
       "      rating_concentration  \n",
       "user                        \n",
       "0                -0.906606  \n",
       "1                 0.458200  \n",
       "2                 2.443201  \n",
       "3                -0.238795  \n",
       "4                 1.873478  \n",
       "...                    ...  \n",
       "3595              0.123364  \n",
       "3596              0.545020  \n",
       "3597              0.164125  \n",
       "3598             -1.553903  \n",
       "3599              0.798083  \n",
       "\n",
       "[3600 rows x 23 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we prepare the dataframe\n",
    "# We join X_lp with y_cat again so we can fill the unlabeled rows with -1\n",
    "\n",
    "X_lp = pd.DataFrame(\n",
    "    df_cla_2_scaled,\n",
    "    columns=df_cla_2.columns,\n",
    "    index=df_cla_2.index\n",
    ")\n",
    "\n",
    "X_lp = pd.merge(X_lp, y_cat, on=\"user\", how=\"left\")\n",
    "X_lp[\"anomtype\"] = X_lp[\"anomtype\"].fillna(-1)\n",
    "X_full = X_lp.drop([\"label\", \"anomtype\"], axis=1).set_index(\"user\")\n",
    "\n",
    "# Check X_full\n",
    "X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af4f7e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "3595    False\n",
       "3596    False\n",
       "3597    False\n",
       "3598    False\n",
       "3599    False\n",
       "Name: anomtype, Length: 3600, dtype: bool"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask where False = unlabeled, True = labeled\n",
    "\n",
    "y_full = X_lp[\"anomtype\"]\n",
    "mask_labeled = (y_full != -1)\n",
    "mask_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12cc30ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies per fold: [0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.5833333333333334]\n",
      "Mean CV accuracy on held-out labeled users: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Label Propagation\n",
    "# We test accuracy using Stratified K Fold\n",
    "\n",
    "# Here we mask the rows that already have labels\n",
    "labeled_idx = np.where(y_full != -1)[0]\n",
    "X_labeled = X_full.iloc[labeled_idx]\n",
    "y_labeled = y_full[labeled_idx]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=67)\n",
    "accs = []\n",
    "\n",
    "# print(labeled_idx)\n",
    "\n",
    "for train_idx, test_idx in skf.split(X_labeled, y_labeled):\n",
    "    # Create new partial labels (everything unlabeled)\n",
    "    y_partial = np.full_like(y_full, -1)\n",
    "\n",
    "    # SKF generates folds from index 0-59\n",
    "    # We need to convert them to the actual indices e.g. 26, 199, 202, ... in the full dataset\n",
    "    absolute_train_idx = labeled_idx[train_idx]\n",
    "    absolute_test_idx = labeled_idx[test_idx]\n",
    "\n",
    "    # Assign only the training fold labels\n",
    "    y_partial[absolute_train_idx] = y_labeled[absolute_train_idx]\n",
    "\n",
    "    # Fit the Label Spreading model\n",
    "    model = LabelSpreading(kernel='rbf', gamma=0.25, alpha=0.2, max_iter=50)\n",
    "    model.fit(X_full.values if isinstance(X_full, pd.DataFrame) else X_full, y_partial)\n",
    "\n",
    "    # Evaluate on held-out labeled users\n",
    "    y_pred = model.transduction_[absolute_test_idx]\n",
    "    acc = accuracy_score(y_labeled[absolute_test_idx], y_pred)\n",
    "    accs.append(acc)\n",
    "\n",
    "print(f\"Accuracies per fold: {accs}\")\n",
    "print(f\"Mean CV accuracy on held-out labeled users: {np.mean(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5d3ec",
   "metadata": {},
   "source": [
    "We get a more desirable accuracy of 76.7%, so we choose to use label propagation as our final classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1918e",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da5204",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e8f01f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>849</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3600</td>\n",
       "      <td>722</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3600</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3600</td>\n",
       "      <td>982</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600</td>\n",
       "      <td>749</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284949</th>\n",
       "      <td>4499</td>\n",
       "      <td>757</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284950</th>\n",
       "      <td>4499</td>\n",
       "      <td>752</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284951</th>\n",
       "      <td>4499</td>\n",
       "      <td>751</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284952</th>\n",
       "      <td>4499</td>\n",
       "      <td>778</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284953</th>\n",
       "      <td>4499</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284954 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  rating\n",
       "0       3600   849       5\n",
       "1       3600   722       5\n",
       "2       3600   462       4\n",
       "3       3600   982       4\n",
       "4       3600   749       4\n",
       "...      ...   ...     ...\n",
       "284949  4499   757       4\n",
       "284950  4499   752       4\n",
       "284951  4499   751       4\n",
       "284952  4499   778       4\n",
       "284953  4499     3       3\n",
       "\n",
       "[284954 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking what the test batch looks like again\n",
    "\n",
    "XX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea69a4c",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6aa0e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_bias = df_tmp.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:90: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  extreme_ratio = df_X.groupby(\"user\").apply(\n",
      "/var/folders/_1/6r1yt5795qz_qx57_g8gt4jc0000gn/T/ipykernel_49115/3112779125.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_rare_item_ratio = df_tmp.groupby(\"user\").apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-7.79989788e+00,  3.64651953e+01,  1.67627218e+01, ...,\n",
       "         2.55805825e+02,  9.95133820e-01,  3.21425205e-01],\n",
       "       [-1.24432240e+01,  1.66845320e+01,  9.28993270e+00, ...,\n",
       "         2.68673729e+02,  1.33191489e+00,  2.18435794e-01],\n",
       "       [-5.76038814e+00,  9.82102578e+00, -4.48504912e+00, ...,\n",
       "         2.44130699e+02,  6.37195122e-01,  4.77388420e-01],\n",
       "       ...,\n",
       "       [-3.38463312e+00,  2.14667896e+01,  1.12725913e+01, ...,\n",
       "         2.84154167e+02,  6.94560669e-01,  4.04618056e-01],\n",
       "       [ 1.45137212e+01,  1.43932120e+00,  1.49571291e-01, ...,\n",
       "         3.11236000e+02,  7.26907631e-01,  3.47040000e-01],\n",
       "       [ 1.88137033e+01, -4.68974389e+00,  1.61418788e+01, ...,\n",
       "         3.56677419e+02,  1.37037037e+00,  2.46575633e-01]],\n",
       "      shape=(900, 151))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 151)\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataframe for predictions\n",
    "\n",
    "XX_df = engineer_features(XX, n_svd_components=600)\n",
    "XX_df.columns = XX_df.columns.astype(str)\n",
    "\n",
    "XX_features_selected = selector_final_optimized.transform(XX_df.values)\n",
    "\n",
    "display(XX_features_selected)\n",
    "print(XX_features_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cb5a9",
   "metadata": {},
   "source": [
    "### Supervised learning prediction (LGBMRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56cb42f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (900,)\n",
      "Prediction range: [0.657, 0.995]\n",
      "(900,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using the trained LGBMRegressor model\n",
    "\n",
    "# XX_df_reg = scaler_full.transform(XX_df)\n",
    "\n",
    "yy_label_pred_optimized = lgbm_final.predict(XX_features_selected)\n",
    "print(f\"Predictions shape: {yy_label_pred_optimized.shape}\")\n",
    "print(f\"Prediction range: [{yy_label_pred_optimized.min():.3f}, {yy_label_pred_optimized.max():.3f}]\")\n",
    "print(yy_label_pred_optimized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7bacc",
   "metadata": {},
   "source": [
    "### Unsupervised learning prediction (Label propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "062f133f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>interaction_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.676375</td>\n",
       "      <td>-0.753724</td>\n",
       "      <td>-1.307564</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>1.455813</td>\n",
       "      <td>-1.223091</td>\n",
       "      <td>2.072483</td>\n",
       "      <td>-0.360042</td>\n",
       "      <td>-1.101748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.729360</td>\n",
       "      <td>1.555813</td>\n",
       "      <td>-1.677011</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>-1.649779</td>\n",
       "      <td>1.003710</td>\n",
       "      <td>1.071897</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>-0.906606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>1.174038</td>\n",
       "      <td>1.206414</td>\n",
       "      <td>-1.375877</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>-1.721433</td>\n",
       "      <td>1.295657</td>\n",
       "      <td>1.452570</td>\n",
       "      <td>0.161638</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019037</td>\n",
       "      <td>-0.879531</td>\n",
       "      <td>-1.009237</td>\n",
       "      <td>-0.370275</td>\n",
       "      <td>-1.139736</td>\n",
       "      <td>-1.273539</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>0.553194</td>\n",
       "      <td>-0.490828</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.098623</td>\n",
       "      <td>-0.591878</td>\n",
       "      <td>0.491361</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>-1.419891</td>\n",
       "      <td>1.165742</td>\n",
       "      <td>-1.287937</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>-0.531799</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452419</td>\n",
       "      <td>-0.846373</td>\n",
       "      <td>1.420099</td>\n",
       "      <td>-2.008956</td>\n",
       "      <td>-0.871282</td>\n",
       "      <td>0.961288</td>\n",
       "      <td>1.004850</td>\n",
       "      <td>0.901839</td>\n",
       "      <td>-1.773607</td>\n",
       "      <td>2.443201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.518016</td>\n",
       "      <td>-0.609861</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>-0.199227</td>\n",
       "      <td>0.795841</td>\n",
       "      <td>-0.557479</td>\n",
       "      <td>-0.721721</td>\n",
       "      <td>-0.151833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193840</td>\n",
       "      <td>0.926231</td>\n",
       "      <td>1.191901</td>\n",
       "      <td>0.396310</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>0.805404</td>\n",
       "      <td>0.153503</td>\n",
       "      <td>0.826007</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>-0.238795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.999091</td>\n",
       "      <td>-1.455058</td>\n",
       "      <td>1.880405</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>-1.455088</td>\n",
       "      <td>1.915819</td>\n",
       "      <td>-1.182528</td>\n",
       "      <td>-1.930129</td>\n",
       "      <td>0.228133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954757</td>\n",
       "      <td>-1.284317</td>\n",
       "      <td>1.013126</td>\n",
       "      <td>-2.067286</td>\n",
       "      <td>1.829722</td>\n",
       "      <td>1.628609</td>\n",
       "      <td>-1.871693</td>\n",
       "      <td>0.767914</td>\n",
       "      <td>-1.269377</td>\n",
       "      <td>1.873478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                         \n",
       "0       0.217773       0.676375      -0.753724   -1.307564   \n",
       "1       0.217773       1.174038       1.206414   -1.375877   \n",
       "2       0.217773      -1.098623      -0.591878    0.491361   \n",
       "3       0.217773      -0.518016      -0.609861    0.559675   \n",
       "4       0.217773      -0.999091      -1.455058    1.880405   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "0              -1.101748        1.455813   -1.223091       2.072483   \n",
       "1               0.161638        0.032727   -1.721433       1.295657   \n",
       "2              -0.531799       -1.419891    1.165742      -1.287937   \n",
       "3              -0.151833       -0.199227    0.795841      -0.557479   \n",
       "4               0.228133       -1.455088    1.915819      -1.182528   \n",
       "\n",
       "      neutral_ratio  interaction_ratio  ...  user_bias  outlier_frac  \\\n",
       "user                                    ...                            \n",
       "0         -0.360042          -1.101748  ...  -1.729360      1.555813   \n",
       "1          1.452570           0.161638  ...  -1.019037     -0.879531   \n",
       "2         -0.487540          -0.531799  ...   1.452419     -0.846373   \n",
       "3         -0.721721          -0.151833  ...   1.193840      0.926231   \n",
       "4         -1.930129           0.228133  ...   0.954757     -1.284317   \n",
       "\n",
       "      mean_item_alignment  rating_entropy  extreme_ratio  user_mean_rank  \\\n",
       "user                                                                       \n",
       "0               -1.677011        0.549713       0.394985       -1.649779   \n",
       "1               -1.009237       -0.370275      -1.139736       -1.273539   \n",
       "2                1.420099       -2.008956      -0.871282        0.961288   \n",
       "3                1.191901        0.396310       0.542944        0.805404   \n",
       "4                1.013126       -2.067286       1.829722        1.628609   \n",
       "\n",
       "      avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                            \n",
       "0                1.003710         1.071897           0.976815   \n",
       "1                0.922383         0.553194          -0.490828   \n",
       "2                1.004850         0.901839          -1.773607   \n",
       "3                0.153503         0.826007           0.017939   \n",
       "4               -1.871693         0.767914          -1.269377   \n",
       "\n",
       "      rating_concentration  \n",
       "user                        \n",
       "0                -0.906606  \n",
       "1                 0.458200  \n",
       "2                 2.443201  \n",
       "3                -0.238795  \n",
       "4                 1.873478  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_rating</th>\n",
       "      <th>count_dislike</th>\n",
       "      <th>count_neutral</th>\n",
       "      <th>count_like</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>normalized_std</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>neutral_ratio</th>\n",
       "      <th>interaction_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>user_bias</th>\n",
       "      <th>outlier_frac</th>\n",
       "      <th>mean_item_alignment</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extreme_ratio</th>\n",
       "      <th>user_mean_rank</th>\n",
       "      <th>avg_item_popularity</th>\n",
       "      <th>rare_item_ratio</th>\n",
       "      <th>rating_volatility</th>\n",
       "      <th>rating_concentration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.634138</td>\n",
       "      <td>0.145422</td>\n",
       "      <td>-0.909068</td>\n",
       "      <td>-1.044753</td>\n",
       "      <td>0.078842</td>\n",
       "      <td>-0.519438</td>\n",
       "      <td>-0.429492</td>\n",
       "      <td>1.429843</td>\n",
       "      <td>-1.044753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170875</td>\n",
       "      <td>-0.030588</td>\n",
       "      <td>0.151994</td>\n",
       "      <td>0.280634</td>\n",
       "      <td>-0.113755</td>\n",
       "      <td>-0.077461</td>\n",
       "      <td>-4.989630</td>\n",
       "      <td>-1.113025</td>\n",
       "      <td>0.640836</td>\n",
       "      <td>-0.129409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.683904</td>\n",
       "      <td>-1.239263</td>\n",
       "      <td>1.197269</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>-0.436394</td>\n",
       "      <td>1.468320</td>\n",
       "      <td>-0.786249</td>\n",
       "      <td>-1.624386</td>\n",
       "      <td>-0.047343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569302</td>\n",
       "      <td>-0.851080</td>\n",
       "      <td>0.594735</td>\n",
       "      <td>-0.539760</td>\n",
       "      <td>1.374624</td>\n",
       "      <td>1.319727</td>\n",
       "      <td>-5.428089</td>\n",
       "      <td>-10.453640</td>\n",
       "      <td>-0.259268</td>\n",
       "      <td>0.762769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-1.065446</td>\n",
       "      <td>-1.113383</td>\n",
       "      <td>0.548289</td>\n",
       "      <td>-0.740780</td>\n",
       "      <td>-1.284779</td>\n",
       "      <td>1.572636</td>\n",
       "      <td>-1.232968</td>\n",
       "      <td>-1.267427</td>\n",
       "      <td>-0.740780</td>\n",
       "      <td>...</td>\n",
       "      <td>1.548023</td>\n",
       "      <td>-0.588915</td>\n",
       "      <td>1.554745</td>\n",
       "      <td>-1.338287</td>\n",
       "      <td>1.305493</td>\n",
       "      <td>1.492932</td>\n",
       "      <td>-5.195337</td>\n",
       "      <td>-0.920714</td>\n",
       "      <td>-1.183546</td>\n",
       "      <td>1.106801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.119886</td>\n",
       "      <td>0.702892</td>\n",
       "      <td>-1.136780</td>\n",
       "      <td>-0.645789</td>\n",
       "      <td>-0.103666</td>\n",
       "      <td>-1.164513</td>\n",
       "      <td>0.207202</td>\n",
       "      <td>1.791473</td>\n",
       "      <td>-0.645789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.739144</td>\n",
       "      <td>-0.366387</td>\n",
       "      <td>-0.759867</td>\n",
       "      <td>-0.170710</td>\n",
       "      <td>-1.260152</td>\n",
       "      <td>-0.785678</td>\n",
       "      <td>-5.041143</td>\n",
       "      <td>-1.470724</td>\n",
       "      <td>-1.062868</td>\n",
       "      <td>0.360697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.742730</td>\n",
       "      <td>-0.717758</td>\n",
       "      <td>-1.205093</td>\n",
       "      <td>-0.959261</td>\n",
       "      <td>1.313556</td>\n",
       "      <td>-1.115291</td>\n",
       "      <td>1.955215</td>\n",
       "      <td>-0.407340</td>\n",
       "      <td>-0.959261</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.603562</td>\n",
       "      <td>1.215283</td>\n",
       "      <td>-1.564363</td>\n",
       "      <td>0.714649</td>\n",
       "      <td>-0.040648</td>\n",
       "      <td>-1.597817</td>\n",
       "      <td>-4.782417</td>\n",
       "      <td>-0.016319</td>\n",
       "      <td>1.337720</td>\n",
       "      <td>-0.941135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_rating  count_dislike  count_neutral  count_like  \\\n",
       "user                                                         \n",
       "4495    0.217773      -0.634138       0.145422   -0.909068   \n",
       "4496    0.217773      -0.683904      -1.239263    1.197269   \n",
       "4497    0.217773      -1.065446      -1.113383    0.548289   \n",
       "4498    0.217773      -0.119886       0.702892   -1.136780   \n",
       "4499    0.217773       0.742730      -0.717758   -1.205093   \n",
       "\n",
       "      total_interactions  normalized_std  like_ratio  dislike_ratio  \\\n",
       "user                                                                  \n",
       "4495           -1.044753        0.078842   -0.519438      -0.429492   \n",
       "4496           -0.047343       -0.436394    1.468320      -0.786249   \n",
       "4497           -0.740780       -1.284779    1.572636      -1.232968   \n",
       "4498           -0.645789       -0.103666   -1.164513       0.207202   \n",
       "4499           -0.959261        1.313556   -1.115291       1.955215   \n",
       "\n",
       "      neutral_ratio  interaction_ratio  ...  user_bias  outlier_frac  \\\n",
       "user                                    ...                            \n",
       "4495       1.429843          -1.044753  ...   0.170875     -0.030588   \n",
       "4496      -1.624386          -0.047343  ...   0.569302     -0.851080   \n",
       "4497      -1.267427          -0.740780  ...   1.548023     -0.588915   \n",
       "4498       1.791473          -0.645789  ...  -0.739144     -0.366387   \n",
       "4499      -0.407340          -0.959261  ...  -1.603562      1.215283   \n",
       "\n",
       "      mean_item_alignment  rating_entropy  extreme_ratio  user_mean_rank  \\\n",
       "user                                                                       \n",
       "4495             0.151994        0.280634      -0.113755       -0.077461   \n",
       "4496             0.594735       -0.539760       1.374624        1.319727   \n",
       "4497             1.554745       -1.338287       1.305493        1.492932   \n",
       "4498            -0.759867       -0.170710      -1.260152       -0.785678   \n",
       "4499            -1.564363        0.714649      -0.040648       -1.597817   \n",
       "\n",
       "      avg_item_popularity  rare_item_ratio  rating_volatility  \\\n",
       "user                                                            \n",
       "4495            -4.989630        -1.113025           0.640836   \n",
       "4496            -5.428089       -10.453640          -0.259268   \n",
       "4497            -5.195337        -0.920714          -1.183546   \n",
       "4498            -5.041143        -1.470724          -1.062868   \n",
       "4499            -4.782417        -0.016319           1.337720   \n",
       "\n",
       "      rating_concentration  \n",
       "user                        \n",
       "4495             -0.129409  \n",
       "4496              0.762769  \n",
       "4497              1.106801  \n",
       "4498              0.360697  \n",
       "4499             -0.941135  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for Label Propagation\n",
    "\n",
    "XX_lp_features = XX_df.iloc[:, -23:]\n",
    "# Use the same scaler the model was trained on \n",
    "XX_lp_features_scaled = scaler_features.transform(XX_lp_features)\n",
    "XX_lp_features = pd.DataFrame(\n",
    "    XX_lp_features_scaled,\n",
    "    columns=XX_lp_features.columns,\n",
    "    index=XX_lp_features.index\n",
    ")\n",
    "\n",
    "# Concatenate it with the previous training dataset\n",
    "lp_full = pd.concat([X_full, XX_lp_features], axis=0)\n",
    "\n",
    "display(lp_full.head(5))\n",
    "display(lp_full.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5a82358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4495   -1.0\n",
       "4496   -1.0\n",
       "4497   -1.0\n",
       "4498   -1.0\n",
       "4499   -1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 900 rows of -1 so we can predict on the test dataset\n",
    "y_full = pd.concat([y_full, pd.Series([-1] * 900)], ignore_index=True)\n",
    "y_full.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51a40ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2., 1., 2., 2., 2., 1., 2., 2., 1., 2., 2., 1., 1., 1., 0.,\n",
       "       2., 1., 2., 1., 2., 1., 2., 2., 1., 1., 1., 2., 1., 1., 2., 1., 1.,\n",
       "       1., 0., 0., 1., 2., 1., 2., 1., 0., 1., 1., 0., 2., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 2., 1., 1., 1., 2., 1., 2., 1., 2., 1.,\n",
       "       1., 2., 1., 0., 1., 1., 1., 0., 1., 1., 2., 2., 1., 2., 2., 2., 1.,\n",
       "       2., 0., 2., 1., 1., 2., 1., 1., 1., 1., 2., 1., 2., 1., 2., 0., 0.,\n",
       "       2., 2., 2., 1., 2., 1., 1., 2., 0., 2., 2., 1., 2., 2., 1., 2., 2.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 2., 2., 2., 0., 2., 1., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 2., 1., 2., 0., 1., 1., 2.,\n",
       "       2., 2., 2., 1., 0., 1., 2., 1., 1., 2., 1., 1., 2., 1., 1., 1., 2.,\n",
       "       1., 2., 1., 1., 2., 1., 1., 2., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 2., 1., 1., 1., 1., 0., 2., 2., 1., 1., 0., 2., 2.,\n",
       "       1., 1., 0., 2., 2., 0., 2., 1., 0., 1., 2., 2., 1., 2., 2., 1., 1.,\n",
       "       2., 1., 1., 2., 0., 1., 2., 2., 1., 0., 1., 2., 1., 1., 0., 1., 1.,\n",
       "       2., 1., 2., 2., 1., 2., 1., 0., 1., 2., 2., 2., 1., 1., 2., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 2., 1., 1., 1., 0., 2., 1., 1., 1., 0., 2., 2., 2.,\n",
       "       1., 1., 2., 2., 1., 2., 1., 1., 2., 2., 1., 1., 2., 1., 2., 0., 1.,\n",
       "       2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1.,\n",
       "       2., 1., 2., 1., 2., 2., 1., 2., 1., 1., 1., 1., 2., 2., 2., 0., 1.,\n",
       "       2., 2., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 2., 2., 0., 1., 1.,\n",
       "       1., 0., 2., 1., 1., 1., 0., 1., 1., 1., 2., 1., 1., 2., 1., 1., 2.,\n",
       "       1., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 1., 2., 1., 1., 2., 1.,\n",
       "       1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 1., 1., 2., 1., 1., 1., 1., 2., 1., 1., 2.,\n",
       "       2., 1., 1., 1., 2., 1., 1., 2., 2., 2., 1., 1., 2., 1., 1., 0., 1.,\n",
       "       2., 1., 1., 1., 2., 1., 0., 1., 1., 1., 0., 1., 1., 2., 1., 0., 1.,\n",
       "       1., 2., 0., 0., 1., 2., 2., 1., 2., 1., 1., 1., 1., 2., 0., 2., 1.,\n",
       "       1., 2., 0., 1., 2., 2., 2., 2., 2., 1., 0., 2., 0., 1., 1., 2., 1.,\n",
       "       2., 2., 0., 2., 2., 0., 1., 0., 2., 0., 2., 0., 2., 2., 2., 2., 1.,\n",
       "       1., 2., 1., 1., 0., 2., 2., 1., 1., 2., 1., 0., 2., 2., 1., 2., 1.,\n",
       "       1., 1., 2., 2., 1., 2., 1., 2., 2., 2., 0., 1., 1., 1., 2., 0., 2.,\n",
       "       1., 2., 1., 2., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 2., 1., 1., 0., 1., 2., 1., 2., 1., 2.,\n",
       "       2., 1., 2., 1., 0., 1., 1., 1., 0., 1., 2., 1., 2., 1., 2., 1., 1.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 1., 1., 2., 1., 1., 2., 1., 1., 0., 2.,\n",
       "       0., 2., 1., 2., 1., 1., 0., 1., 1., 0., 1., 1., 0., 2., 1., 1., 2.,\n",
       "       1., 2., 2., 1., 0., 1., 1., 1., 1., 2., 0., 2., 1., 2., 0., 2., 0.,\n",
       "       1., 0., 2., 0., 2., 2., 1., 2., 2., 2., 2., 1., 1., 1., 1., 2., 2.,\n",
       "       1., 2., 0., 2., 0., 1., 2., 1., 1., 1., 2., 2., 2., 1., 0., 1., 0.,\n",
       "       0., 2., 1., 1., 1., 2., 1., 1., 2., 2., 1., 1., 2., 1., 2., 2., 1.,\n",
       "       2., 0., 1., 1., 2., 2., 2., 1., 2., 1., 1., 0., 1., 0., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 2., 1., 1., 1., 1., 0., 2., 2., 1., 1., 0., 2., 1.,\n",
       "       0., 1., 1., 2., 1., 2., 0., 2., 2., 1., 1., 2., 2., 0., 2., 2., 1.,\n",
       "       1., 2., 1., 2., 1., 1., 1., 0., 1., 1., 2., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 2., 2., 1., 1., 2., 1., 1., 2., 1., 0., 1., 1., 0., 2., 2., 1.,\n",
       "       1., 1., 2., 0., 1., 2., 1., 1., 1., 2., 2., 2., 2., 1., 0., 0., 2.,\n",
       "       0., 2., 2., 2., 1., 2., 0., 2., 1., 2., 1., 2., 2., 0., 2., 2., 1.,\n",
       "       2., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 2., 1., 2., 1., 2.,\n",
       "       2., 2., 2., 0., 1., 2., 1., 1., 1., 1., 2., 1., 1., 2., 1., 2., 0.,\n",
       "       2., 2., 2., 1., 1., 2., 2., 1., 1., 1., 2., 1., 2., 2., 1., 0.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data to label propagation\n",
    "\n",
    "model = LabelSpreading(kernel='rbf', gamma=0.25, alpha=0.2, max_iter=50)\n",
    "model.fit(lp_full, y_full)\n",
    "\n",
    "yy_label_cluster = model.transduction_[-900:]\n",
    "yy_label_cluster\n",
    "\n",
    "# Debug: class probabilities\n",
    "# y_test_proba = model.label_distributions_[-900:]\n",
    "# y_test_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7684144",
   "metadata": {},
   "source": [
    "### Saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc822f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>0.833013</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3601</td>\n",
       "      <td>0.867981</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3602</td>\n",
       "      <td>0.919197</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3603</td>\n",
       "      <td>0.784252</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>0.887964</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3605</td>\n",
       "      <td>0.760322</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3606</td>\n",
       "      <td>0.727767</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3607</td>\n",
       "      <td>0.933284</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3608</td>\n",
       "      <td>0.963737</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3609</td>\n",
       "      <td>0.789779</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user     label  anomtype\n",
       "0  3600  0.833013       2.0\n",
       "1  3601  0.867981       1.0\n",
       "2  3602  0.919197       2.0\n",
       "3  3603  0.784252       1.0\n",
       "4  3604  0.887964       2.0\n",
       "5  3605  0.760322       2.0\n",
       "6  3606  0.727767       2.0\n",
       "7  3607  0.933284       1.0\n",
       "8  3608  0.963737       2.0\n",
       "9  3609  0.789779       2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine dataframe with predicted results\n",
    "\n",
    "result_df = XX_df.reset_index()\n",
    "result_df = result_df[[\"user\"]]\n",
    "result_df[\"label\"] = yy_label_pred_optimized # from LGBMRegressor\n",
    "result_df[\"anomtype\"] = yy_label_cluster # mapped from Label propagation\n",
    "display(result_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "037a6a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>anomtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600</td>\n",
       "      <td>0.833013</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3601</td>\n",
       "      <td>0.867981</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3602</td>\n",
       "      <td>0.919197</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3603</td>\n",
       "      <td>0.784252</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>0.887964</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>4495</td>\n",
       "      <td>0.722589</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>4496</td>\n",
       "      <td>0.911470</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>4497</td>\n",
       "      <td>0.854275</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>4498</td>\n",
       "      <td>0.792419</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>4499</td>\n",
       "      <td>0.898486</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user     label  anomtype\n",
       "0    3600  0.833013       2.0\n",
       "1    3601  0.867981       1.0\n",
       "2    3602  0.919197       2.0\n",
       "3    3603  0.784252       1.0\n",
       "4    3604  0.887964       2.0\n",
       "..    ...       ...       ...\n",
       "895  4495  0.722589       1.0\n",
       "896  4496  0.911470       2.0\n",
       "897  4497  0.854275       2.0\n",
       "898  4498  0.792419       1.0\n",
       "899  4499  0.898486       0.0\n",
       "\n",
       "[900 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up label predictions\n",
    "# We normalise label column as some predictions are <0 or >1\n",
    "\n",
    "result_df[\"label\"] = result_df[\"label\"].clip(lower=0, upper=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e379e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result successfully saved\n"
     ]
    }
   ],
   "source": [
    "# Save as csv\n",
    "\n",
    "result_df.to_csv('fifth_batch_output.csv',index=False)\n",
    "print(\"Result successfully saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ffb9e",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2984875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overfitting Analysis ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.0231\n",
      "Validation MAE: 0.0223\n",
      "Gap (Overfit indicator): -0.0008\n",
      "Relative Gap: -3.46%\n"
     ]
    }
   ],
   "source": [
    "# Add this analysis cell after training your final model\n",
    "print(\"=== Overfitting Analysis ===\")\n",
    "y_train_pred = lgbm_final.predict(X_train_opt_selected)\n",
    "y_val_pred = lgbm_final.predict(X_val_opt_selected)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_opt, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val_opt, y_val_pred)\n",
    "\n",
    "print(f\"Training MAE: {train_mae:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Gap (Overfit indicator): {val_mae - train_mae:.4f}\")\n",
    "print(f\"Relative Gap: {((val_mae - train_mae) / train_mae * 100):.2f}%\")\n",
    "\n",
    "# Rule of thumb:\n",
    "# - Gap < 0.005: Good generalization\n",
    "# - Gap 0.005-0.01: Acceptable\n",
    "# - Gap > 0.01: Likely overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83df07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 MAE: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 MAE: 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 MAE: 0.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 MAE: 0.0614\n",
      "Fold 5 MAE: 0.0664\n",
      "\n",
      "Mean CV MAE: 0.0642\n",
      "Std CV MAE: 0.0024\n",
      "95% Confidence Interval: [0.0593, 0.0690]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmmsia/Developer/cs421-proj/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "### Cross-Validation Check for Overfitting\n",
    "\n",
    "# Prepare full dataset\n",
    "X_full = df_reg_optimized.drop(columns=[\"label\"]).values\n",
    "y_full = df_reg_optimized[\"label\"].values\n",
    "X_full_selected = selector_final_optimized.transform(X_full)\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=67)\n",
    "cv_maes = []\n",
    "\n",
    "lgbm_cv = LGBMRegressor(\n",
    "    random_state=67,\n",
    "    max_depth=6,\n",
    "    num_leaves=64,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    verbose=-1,\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=0.001\n",
    ")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full_selected), 1):\n",
    "    X_train_fold = X_full_selected[train_idx]\n",
    "    X_val_fold = X_full_selected[val_idx]\n",
    "    y_train_fold = y_full[train_idx]\n",
    "    y_val_fold = y_full[val_idx]\n",
    "    \n",
    "    lgbm_cv.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = lgbm_cv.predict(X_val_fold)\n",
    "    \n",
    "    mae_fold = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    cv_maes.append(mae_fold)\n",
    "    print(f\"Fold {fold} MAE: {mae_fold:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV MAE: {np.mean(cv_maes):.4f}\")\n",
    "print(f\"Std CV MAE: {np.std(cv_maes):.4f}\")\n",
    "print(f\"95% Confidence Interval: [{np.mean(cv_maes) - 2*np.std(cv_maes):.4f}, {np.mean(cv_maes) + 2*np.std(cv_maes):.4f}]\")\n",
    "\n",
    "# Rule of thumb:\n",
    "# - Low std (< 0.005): Model is stable\n",
    "# - High std (> 0.01): Model is sensitive to data splits (potential overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fd719b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV of feature importances: 0.1870\n",
      "Max CV of feature importances: 0.4854\n"
     ]
    }
   ],
   "source": [
    "### Feature Importance Stability\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "feature_importance_folds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full_selected), 1):\n",
    "    lgbm_temp = LGBMRegressor(\n",
    "        random_state=67,\n",
    "        max_depth=6,\n",
    "        num_leaves=64,\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.01,\n",
    "        reg_lambda=0.1,\n",
    "        reg_alpha=1.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbose=-1,\n",
    "        min_child_samples=20,\n",
    "        min_child_weight=0.001\n",
    "    )\n",
    "    \n",
    "    lgbm_temp.fit(X_full_selected[train_idx], y_full[train_idx])\n",
    "    feature_importance_folds.append(lgbm_temp.feature_importances_)\n",
    "\n",
    "# Calculate stability (coefficient of variation)\n",
    "importance_array = np.array(feature_importance_folds)\n",
    "importance_mean = importance_array.mean(axis=0)\n",
    "importance_std = importance_array.std(axis=0)\n",
    "importance_cv = importance_std / (importance_mean + 1e-10)  # Coefficient of variation\n",
    "\n",
    "print(f\"Mean CV of feature importances: {importance_cv.mean():.4f}\")\n",
    "print(f\"Max CV of feature importances: {importance_cv.max():.4f}\")\n",
    "\n",
    "# Rule of thumb:\n",
    "# - Mean CV < 0.5: Stable features (good)\n",
    "# - Mean CV > 1.0: Unstable features (potential overfitting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
